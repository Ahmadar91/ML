{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torchsummary import summary\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import time\n",
    "import pathlib\n",
    "import os\n",
    "import scipy.io\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "import glob\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MRI Data set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class BrainTumorDataset(Dataset):\n",
    "    def __init__(self, mat_files, transform=None):\n",
    "        self.mat_files = mat_files\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.mat_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        with h5py.File(self.mat_files[idx], \"r\") as f:\n",
    "            image = f[\"cjdata\"][\"image\"][()]\n",
    "\n",
    "            label = int(\n",
    "                f[\"cjdata\"][\"label\"][0][0] - 1\n",
    "            )  # Convert labels from 1-3 to 0-2\n",
    "\n",
    "            # Convert to grayscale ('L' mode) before applying transformations\n",
    "            image = Image.fromarray(image).convert(\"L\")\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "\n",
    "# Sample use:\n",
    "# Assuming path to .mat files is './data/New folder/'\n",
    "mat_files = [\n",
    "    os.path.join(\"./data/New folder/\", f)\n",
    "    for f in os.listdir(\"./data/New folder/\")\n",
    "    if f.endswith(\".mat\")\n",
    "]\n",
    "\n",
    "# Split the data into train and test sets (e.g., 80% train, 20% test)\n",
    "indices = list(range(len(mat_files)))\n",
    "split = int(np.floor(0.7 * len(mat_files)))\n",
    "np.random.shuffle(indices)\n",
    "train_indices, test_indices = indices[:split], indices[split:]\n",
    "\n",
    "\n",
    "# Transforms\n",
    "transforming_img = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((227, 227)),\n",
    "        transforms.ToTensor(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "train_set_MRI = Subset(\n",
    "    BrainTumorDataset(mat_files, transform=transforming_img), train_indices\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 0.5666518807411194\n",
      "Standard Deviation: 0.4237934947013855\n"
     ]
    }
   ],
   "source": [
    "# Initializing variables to store sum and squared sum\n",
    "total_sum = 0\n",
    "total_squared_sum = 0\n",
    "num_pixels = 0\n",
    "\n",
    "# Calculating mean and standard deviation\n",
    "for data, _ in train_set_MRI:\n",
    "    total_sum += data.sum()\n",
    "    total_squared_sum += (data**2).sum()\n",
    "    num_pixels += data.numel()\n",
    "\n",
    "mean = total_sum / num_pixels\n",
    "std_dev = (total_squared_sum / num_pixels - mean**2) ** 0.5\n",
    "\n",
    "print(f\"Mean: {mean.item()}\")\n",
    "print(f\"Standard Deviation: {std_dev.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 227, 227])\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# Transforms\n",
    "transforming_img = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((227, 227)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((mean.item()), (std_dev.item())),\n",
    "    ]\n",
    ")\n",
    "\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "train_set_MRI = Subset(\n",
    "    BrainTumorDataset(mat_files, transform=transforming_img), train_indices\n",
    ")\n",
    "test_set_MRI = Subset(\n",
    "    BrainTumorDataset(mat_files, transform=transforming_img), test_indices\n",
    ")\n",
    "train_loader_MRI = DataLoader(train_set_MRI, batch_size=10, shuffle=True)\n",
    "test_loader_MRI = DataLoader(test_set_MRI, batch_size=10, shuffle=False)\n",
    "\n",
    "# Sample training loop:\n",
    "for image, label in train_set_MRI:\n",
    "    print(image.shape)\n",
    "    print(label)\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Architecture\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class MyCNN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size=28,\n",
    "        kernel_size=5,\n",
    "        num_filters1=32,\n",
    "        num_filters2=64,\n",
    "        dropout_rate=0.5,\n",
    "        in_channels=1,\n",
    "        classes=10,\n",
    "    ):\n",
    "        super(MyCNN, self).__init__()\n",
    "\n",
    "        self.kernel_size = kernel_size\n",
    "        self.input_size = input_size\n",
    "        self.pool_kernel_size = 3\n",
    "        self.pool_stride = 3\n",
    "\n",
    "        # Compute output sizes after conv and pool operations\n",
    "        conv1_out_size = self.input_size - self.kernel_size + 1\n",
    "        pool1_out_size = (\n",
    "            conv1_out_size - self.pool_kernel_size\n",
    "        ) // self.pool_stride + 1\n",
    "\n",
    "        conv2_input_size = pool1_out_size\n",
    "        conv2_out_size = conv2_input_size - self.kernel_size + 1\n",
    "        pool2_out_size = (\n",
    "            conv2_out_size - self.pool_kernel_size\n",
    "        ) // self.pool_stride + 1\n",
    "\n",
    "        fc_input_size = num_filters2 * pool2_out_size * pool2_out_size\n",
    "        print(fc_input_size)\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=in_channels,\n",
    "                out_channels=num_filters1,\n",
    "                kernel_size=self.kernel_size,\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(num_filters1),\n",
    "            nn.MaxPool2d(kernel_size=self.pool_kernel_size, stride=self.pool_stride),\n",
    "        )\n",
    "\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=num_filters1,\n",
    "                out_channels=num_filters2,\n",
    "                kernel_size=self.kernel_size,\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(num_filters2),\n",
    "            nn.MaxPool2d(kernel_size=self.pool_kernel_size, stride=self.pool_stride),\n",
    "        )\n",
    "        self.drop = nn.Dropout2d(dropout_rate)\n",
    "        self.fc1 = nn.Linear(fc_input_size, classes)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.drop(out)\n",
    "        out = self.fc1(out)\n",
    "        out = self.softmax(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "33856\n",
    "Input: torch.Size([10, 1, 227, 227])\n",
    "After layer1: torch.Size([10, 32, 74, 74])\n",
    "After layer2: torch.Size([10, 64, 23, 23])\n",
    "After reshaping: torch.Size([10, 33856])\n",
    "After dropout: torch.Size([10, 33856])\n",
    "After fc1: torch.Size([10, 3])\n",
    "After softmax: torch.Size([10, 3])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "64\n",
    "Input: torch.Size([10, 1, 28, 28])\n",
    "After layer1: torch.Size([10, 32, 8, 8])\n",
    "After layer2: torch.Size([10, 64, 1, 1])\n",
    "After reshaping: torch.Size([10, 64])\n",
    "After dropout: torch.Size([10, 64])\n",
    "After fc1: torch.Size([10, 10])\n",
    "After softmax: torch.Size([10, 10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train - Test Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Adjusted train function\n",
    "def train(model, data_loader, criterion, optimizer):\n",
    "    model.train()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for images, labels in data_loader:  # Unpacking tumor_border and tumor_mask\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    average_loss = total_loss / len(data_loader)\n",
    "    return accuracy, average_loss\n",
    "\n",
    "\n",
    "# Adjusted test function\n",
    "def test(model, data_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in data_loader:  # Unpacking tumor_border and tumor_mask\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    average_loss = total_loss / len(data_loader)\n",
    "    return accuracy, average_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PlotResults(\n",
    "    train_accuracies, test_accuracies, train_losses, test_losses, epochs, name\n",
    "):\n",
    "    epoch_range = list(\n",
    "        range(1, epochs + 1)\n",
    "    )  # List of epochs from 1 to the given number of epochs\n",
    "\n",
    "    # Visualizing the accuracy results\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(epoch_range, train_accuracies, \"k\", label=\"Training Accuracy\")\n",
    "    plt.plot(epoch_range, test_accuracies, \"r\", label=\"Testing Accuracy\")\n",
    "    plt.title(\"Training and Testing Accuracy over Epochs\", size=20)\n",
    "    plt.xlabel(\"Epochs\", size=15)\n",
    "    plt.ylabel(\"Accuracy (%)\", size=15)\n",
    "    plt.legend(loc=\"lower right\", fontsize=12)\n",
    "    plt.grid(True)\n",
    "    plt.xticks(epoch_range)\n",
    "    plt.tight_layout()  # Ensure that elements don't overlap when saving as PDF\n",
    "    plt.savefig(f\"Training_and_Testing_Accuracy_{name}.pdf\")\n",
    "    plt.show()\n",
    "\n",
    "    # Visualizing the loss results\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(epoch_range, train_losses, \"k\", label=\"Training Loss\")\n",
    "    plt.plot(epoch_range, test_losses, \"r\", label=\"Testing Loss\")\n",
    "    plt.title(\"Training and Test Loss over Epochs\", size=20)\n",
    "    plt.xlabel(\"Epochs\", size=15)\n",
    "    plt.ylabel(\"Loss\", size=15)\n",
    "    plt.legend(loc=\"upper right\", fontsize=12)\n",
    "    plt.grid(True)\n",
    "    plt.xticks(epoch_range)\n",
    "    plt.tight_layout()  # Ensure that elements don't overlap when saving as PDF\n",
    "    plt.savefig(f\"Training_and_Testing_Loss_{name}.pdf\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_model(\n",
    "    model, train_loader, test_loader, criterion, optimizer, num_epochs=30, name=\"Model\"\n",
    "):\n",
    "    train_accuracies, test_accuracies, train_losses, test_losses = [], [], [], []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        train_accuracy, train_loss = train(model, train_loader, criterion, optimizer)\n",
    "        test_accuracy, test_loss = test(model, test_loader)\n",
    "\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        test_accuracies.append(test_accuracy)\n",
    "        train_losses.append(train_loss)\n",
    "        test_losses.append(test_loss)\n",
    "\n",
    "        print(\n",
    "            f\"Epoch [{epoch+1}/{num_epochs}], Train Accuracy: {train_accuracy:.2f}%, Test Accuracy: {test_accuracy:.2f}%, Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}\"\n",
    "        )\n",
    "\n",
    "    print(\"Training and testing completed!\")\n",
    "\n",
    "    PlotResults(\n",
    "        train_accuracies,\n",
    "        test_accuracies,\n",
    "        train_losses,\n",
    "        test_losses,\n",
    "        num_epochs,\n",
    "        name,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MRI Initial using same hyper parameters as the paper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33856\n",
      "Input:  torch.Size([10, 1, 227, 227])\n",
      "After layer1:  torch.Size([10, 32, 74, 74])\n",
      "After layer2:  torch.Size([10, 64, 23, 23])\n",
      "After reshaping:  torch.Size([10, 33856])\n",
      "After dropout:  torch.Size([10, 33856])\n",
      "After fc1:  torch.Size([10, 3])\n",
      "After softmax:  torch.Size([10, 3])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/functional.py:1331: UserWarning: dropout2d: Received a 2-D input to dropout2d, which is deprecated and will result in an error in a future release. To retain the behavior and silence this warning, please use dropout instead. Note that dropout2d exists to provide channel-wise dropout on inputs with 2 spatial dimensions, a channel dimension, and an optional batch dimension (i.e. 3D or 4D inputs).\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:  torch.Size([10, 1, 227, 227])\n",
      "After layer1:  torch.Size([10, 32, 74, 74])\n",
      "After layer2:  torch.Size([10, 64, 23, 23])\n",
      "After reshaping:  torch.Size([10, 33856])\n",
      "After dropout:  torch.Size([10, 33856])\n",
      "After fc1:  torch.Size([10, 3])\n",
      "After softmax:  torch.Size([10, 3])\n",
      "Input:  torch.Size([10, 1, 227, 227])\n",
      "After layer1:  torch.Size([10, 32, 74, 74])\n",
      "After layer2:  torch.Size([10, 64, 23, 23])\n",
      "After reshaping:  torch.Size([10, 33856])\n",
      "After dropout:  torch.Size([10, 33856])\n",
      "After fc1:  torch.Size([10, 3])\n",
      "After softmax:  torch.Size([10, 3])\n",
      "Input:  torch.Size([10, 1, 227, 227])\n",
      "After layer1:  torch.Size([10, 32, 74, 74])\n",
      "After layer2:  torch.Size([10, 64, 23, 23])\n",
      "After reshaping:  torch.Size([10, 33856])\n",
      "After dropout:  torch.Size([10, 33856])\n",
      "After fc1:  torch.Size([10, 3])\n",
      "After softmax:  torch.Size([10, 3])\n",
      "Input:  torch.Size([10, 1, 227, 227])\n",
      "After layer1:  torch.Size([10, 32, 74, 74])\n",
      "After layer2:  torch.Size([10, 64, 23, 23])\n",
      "After reshaping:  torch.Size([10, 33856])\n",
      "After dropout:  torch.Size([10, 33856])\n",
      "After fc1:  torch.Size([10, 3])\n",
      "After softmax:  torch.Size([10, 3])\n",
      "Input:  torch.Size([10, 1, 227, 227])\n",
      "After layer1:  torch.Size([10, 32, 74, 74])\n",
      "After layer2:  torch.Size([10, 64, 23, 23])\n",
      "After reshaping:  torch.Size([10, 33856])\n",
      "After dropout:  torch.Size([10, 33856])\n",
      "After fc1:  torch.Size([10, 3])\n",
      "After softmax:  torch.Size([10, 3])\n",
      "Input:  torch.Size([10, 1, 227, 227])\n",
      "After layer1:  torch.Size([10, 32, 74, 74])\n",
      "After layer2:  torch.Size([10, 64, 23, 23])\n",
      "After reshaping:  torch.Size([10, 33856])\n",
      "After dropout:  torch.Size([10, 33856])\n",
      "After fc1:  torch.Size([10, 3])\n",
      "After softmax:  torch.Size([10, 3])\n",
      "Input:  torch.Size([10, 1, 227, 227])\n",
      "After layer1:  torch.Size([10, 32, 74, 74])\n",
      "After layer2:  torch.Size([10, 64, 23, 23])\n",
      "After reshaping:  torch.Size([10, 33856])\n",
      "After dropout:  torch.Size([10, 33856])\n",
      "After fc1:  torch.Size([10, 3])\n",
      "After softmax:  torch.Size([10, 3])\n",
      "Input:  torch.Size([10, 1, 227, 227])\n",
      "After layer1:  torch.Size([10, 32, 74, 74])\n",
      "After layer2:  torch.Size([10, 64, 23, 23])\n",
      "After reshaping:  torch.Size([10, 33856])\n",
      "After dropout:  torch.Size([10, 33856])\n",
      "After fc1:  torch.Size([10, 3])\n",
      "After softmax:  torch.Size([10, 3])\n",
      "Input:  torch.Size([10, 1, 227, 227])\n",
      "After layer1:  torch.Size([10, 32, 74, 74])\n",
      "After layer2:  torch.Size([10, 64, 23, 23])\n",
      "After reshaping:  torch.Size([10, 33856])\n",
      "After dropout:  torch.Size([10, 33856])\n",
      "After fc1:  torch.Size([10, 3])\n",
      "After softmax:  torch.Size([10, 3])\n",
      "Input:  torch.Size([10, 1, 227, 227])\n",
      "After layer1:  torch.Size([10, 32, 74, 74])\n",
      "After layer2:  torch.Size([10, 64, 23, 23])\n",
      "After reshaping:  torch.Size([10, 33856])\n",
      "After dropout:  torch.Size([10, 33856])\n",
      "After fc1:  torch.Size([10, 3])\n",
      "After softmax:  torch.Size([10, 3])\n",
      "Input:  torch.Size([10, 1, 227, 227])\n",
      "After layer1:  torch.Size([10, 32, 74, 74])\n",
      "After layer2:  torch.Size([10, 64, 23, 23])\n",
      "After reshaping:  torch.Size([10, 33856])\n",
      "After dropout:  torch.Size([10, 33856])\n",
      "After fc1:  torch.Size([10, 3])\n",
      "After softmax:  torch.Size([10, 3])\n",
      "Input:  torch.Size([10, 1, 227, 227])\n",
      "After layer1:  torch.Size([10, 32, 74, 74])\n",
      "After layer2:  torch.Size([10, 64, 23, 23])\n",
      "After reshaping:  torch.Size([10, 33856])\n",
      "After dropout:  torch.Size([10, 33856])\n",
      "After fc1:  torch.Size([10, 3])\n",
      "After softmax:  torch.Size([10, 3])\n",
      "Input:  torch.Size([10, 1, 227, 227])\n",
      "After layer1:  torch.Size([10, 32, 74, 74])\n",
      "After layer2:  torch.Size([10, 64, 23, 23])\n",
      "After reshaping:  torch.Size([10, 33856])\n",
      "After dropout:  torch.Size([10, 33856])\n",
      "After fc1:  torch.Size([10, 3])\n",
      "After softmax:  torch.Size([10, 3])\n",
      "Input:  torch.Size([10, 1, 227, 227])\n",
      "After layer1:  torch.Size([10, 32, 74, 74])\n",
      "After layer2:  torch.Size([10, 64, 23, 23])\n",
      "After reshaping:  torch.Size([10, 33856])\n",
      "After dropout:  torch.Size([10, 33856])\n",
      "After fc1:  torch.Size([10, 3])\n",
      "After softmax:  torch.Size([10, 3])\n",
      "Input:  torch.Size([10, 1, 227, 227])\n",
      "After layer1:  torch.Size([10, 32, 74, 74])\n",
      "After layer2:  torch.Size([10, 64, 23, 23])\n",
      "After reshaping:  torch.Size([10, 33856])\n",
      "After dropout:  torch.Size([10, 33856])\n",
      "After fc1:  torch.Size([10, 3])\n",
      "After softmax:  torch.Size([10, 3])\n",
      "Input:  torch.Size([10, 1, 227, 227])\n",
      "After layer1:  torch.Size([10, 32, 74, 74])\n",
      "After layer2:  torch.Size([10, 64, 23, 23])\n",
      "After reshaping:  torch.Size([10, 33856])\n",
      "After dropout:  torch.Size([10, 33856])\n",
      "After fc1:  torch.Size([10, 3])\n",
      "After softmax:  torch.Size([10, 3])\n",
      "Input:  torch.Size([10, 1, 227, 227])\n",
      "After layer1:  torch.Size([10, 32, 74, 74])\n",
      "After layer2:  torch.Size([10, 64, 23, 23])\n",
      "After reshaping:  torch.Size([10, 33856])\n",
      "After dropout:  torch.Size([10, 33856])\n",
      "After fc1:  torch.Size([10, 3])\n",
      "After softmax:  torch.Size([10, 3])\n",
      "Input:  torch.Size([10, 1, 227, 227])\n",
      "After layer1:  torch.Size([10, 32, 74, 74])\n",
      "After layer2:  torch.Size([10, 64, 23, 23])\n",
      "After reshaping:  torch.Size([10, 33856])\n",
      "After dropout:  torch.Size([10, 33856])\n",
      "After fc1:  torch.Size([10, 3])\n",
      "After softmax:  torch.Size([10, 3])\n",
      "Input:  torch.Size([10, 1, 227, 227])\n",
      "After layer1:  torch.Size([10, 32, 74, 74])\n",
      "After layer2:  torch.Size([10, 64, 23, 23])\n",
      "After reshaping:  torch.Size([10, 33856])\n",
      "After dropout:  torch.Size([10, 33856])\n",
      "After fc1:  torch.Size([10, 3])\n",
      "After softmax:  torch.Size([10, 3])\n",
      "Input:  torch.Size([10, 1, 227, 227])\n",
      "After layer1:  torch.Size([10, 32, 74, 74])\n",
      "After layer2:  torch.Size([10, 64, 23, 23])\n",
      "After reshaping:  torch.Size([10, 33856])\n",
      "After dropout:  torch.Size([10, 33856])\n",
      "After fc1:  torch.Size([10, 3])\n",
      "After softmax:  torch.Size([10, 3])\n",
      "Input:  torch.Size([10, 1, 227, 227])\n",
      "After layer1:  torch.Size([10, 32, 74, 74])\n",
      "After layer2:  torch.Size([10, 64, 23, 23])\n",
      "After reshaping:  torch.Size([10, 33856])\n",
      "After dropout:  torch.Size([10, 33856])\n",
      "After fc1:  torch.Size([10, 3])\n",
      "After softmax:  torch.Size([10, 3])\n",
      "Input:  torch.Size([10, 1, 227, 227])\n",
      "After layer1:  torch.Size([10, 32, 74, 74])\n",
      "After layer2:  torch.Size([10, 64, 23, 23])\n",
      "After reshaping:  torch.Size([10, 33856])\n",
      "After dropout:  torch.Size([10, 33856])\n",
      "After fc1:  torch.Size([10, 3])\n",
      "After softmax:  torch.Size([10, 3])\n",
      "Input:  torch.Size([10, 1, 227, 227])\n",
      "After layer1:  torch.Size([10, 32, 74, 74])\n",
      "After layer2:  torch.Size([10, 64, 23, 23])\n",
      "After reshaping:  torch.Size([10, 33856])\n",
      "After dropout:  torch.Size([10, 33856])\n",
      "After fc1:  torch.Size([10, 3])\n",
      "After softmax:  torch.Size([10, 3])\n",
      "Input:  torch.Size([10, 1, 227, 227])\n",
      "After layer1:  torch.Size([10, 32, 74, 74])\n",
      "After layer2:  torch.Size([10, 64, 23, 23])\n",
      "After reshaping:  torch.Size([10, 33856])\n",
      "After dropout:  torch.Size([10, 33856])\n",
      "After fc1:  torch.Size([10, 3])\n",
      "After softmax:  torch.Size([10, 3])\n",
      "Input:  torch.Size([10, 1, 227, 227])\n",
      "After layer1:  torch.Size([10, 32, 74, 74])\n",
      "After layer2:  torch.Size([10, 64, 23, 23])\n",
      "After reshaping:  torch.Size([10, 33856])\n",
      "After dropout:  torch.Size([10, 33856])\n",
      "After fc1:  torch.Size([10, 3])\n",
      "After softmax:  torch.Size([10, 3])\n",
      "Input:  torch.Size([10, 1, 227, 227])\n",
      "After layer1:  torch.Size([10, 32, 74, 74])\n",
      "After layer2:  torch.Size([10, 64, 23, 23])\n",
      "After reshaping:  torch.Size([10, 33856])\n",
      "After dropout:  torch.Size([10, 33856])\n",
      "After fc1:  torch.Size([10, 3])\n",
      "After softmax:  torch.Size([10, 3])\n",
      "Input:  torch.Size([10, 1, 227, 227])\n",
      "After layer1:  torch.Size([10, 32, 74, 74])\n",
      "After layer2:  torch.Size([10, 64, 23, 23])\n",
      "After reshaping:  torch.Size([10, 33856])\n",
      "After dropout:  torch.Size([10, 33856])\n",
      "After fc1:  torch.Size([10, 3])\n",
      "After softmax:  torch.Size([10, 3])\n",
      "Input:  torch.Size([10, 1, 227, 227])\n",
      "After layer1:  torch.Size([10, 32, 74, 74])\n",
      "After layer2:  torch.Size([10, 64, 23, 23])\n",
      "After reshaping:  torch.Size([10, 33856])\n",
      "After dropout:  torch.Size([10, 33856])\n",
      "After fc1:  torch.Size([10, 3])\n",
      "After softmax:  torch.Size([10, 3])\n",
      "Input:  torch.Size([10, 1, 227, 227])\n",
      "After layer1:  torch.Size([10, 32, 74, 74])\n",
      "After layer2:  torch.Size([10, 64, 23, 23])\n",
      "After reshaping:  torch.Size([10, 33856])\n",
      "After dropout:  torch.Size([10, 33856])\n",
      "After fc1:  torch.Size([10, 3])\n",
      "After softmax:  torch.Size([10, 3])\n",
      "Input:  torch.Size([10, 1, 227, 227])\n",
      "After layer1:  torch.Size([10, 32, 74, 74])\n",
      "After layer2:  torch.Size([10, 64, 23, 23])\n",
      "After reshaping:  torch.Size([10, 33856])\n",
      "After dropout:  torch.Size([10, 33856])\n",
      "After fc1:  torch.Size([10, 3])\n",
      "After softmax:  torch.Size([10, 3])\n",
      "Input:  torch.Size([10, 1, 227, 227])\n",
      "After layer1:  torch.Size([10, 32, 74, 74])\n",
      "After layer2:  torch.Size([10, 64, 23, 23])\n",
      "After reshaping:  torch.Size([10, 33856])\n",
      "After dropout:  torch.Size([10, 33856])\n",
      "After fc1:  torch.Size([10, 3])\n",
      "After softmax:  torch.Size([10, 3])\n",
      "Input:  torch.Size([10, 1, 227, 227])\n",
      "After layer1:  torch.Size([10, 32, 74, 74])\n",
      "After layer2:  torch.Size([10, 64, 23, 23])\n",
      "After reshaping:  torch.Size([10, 33856])\n",
      "After dropout:  torch.Size([10, 33856])\n",
      "After fc1:  torch.Size([10, 3])\n",
      "After softmax:  torch.Size([10, 3])\n",
      "Input:  torch.Size([10, 1, 227, 227])\n",
      "After layer1:  torch.Size([10, 32, 74, 74])\n",
      "After layer2:  torch.Size([10, 64, 23, 23])\n",
      "After reshaping:  torch.Size([10, 33856])\n",
      "After dropout:  torch.Size([10, 33856])\n",
      "After fc1:  torch.Size([10, 3])\n",
      "After softmax:  torch.Size([10, 3])\n",
      "Input:  torch.Size([10, 1, 227, 227])\n",
      "After layer1:  torch.Size([10, 32, 74, 74])\n",
      "After layer2:  torch.Size([10, 64, 23, 23])\n",
      "After reshaping:  torch.Size([10, 33856])\n",
      "After dropout:  torch.Size([10, 33856])\n",
      "After fc1:  torch.Size([10, 3])\n",
      "After softmax:  torch.Size([10, 3])\n",
      "Input:  torch.Size([10, 1, 227, 227])\n",
      "After layer1:  torch.Size([10, 32, 74, 74])\n",
      "After layer2:  torch.Size([10, 64, 23, 23])\n",
      "After reshaping:  torch.Size([10, 33856])\n",
      "After dropout:  torch.Size([10, 33856])\n",
      "After fc1:  torch.Size([10, 3])\n",
      "After softmax:  torch.Size([10, 3])\n",
      "Input:  torch.Size([10, 1, 227, 227])\n",
      "After layer1:  torch.Size([10, 32, 74, 74])\n",
      "After layer2:  torch.Size([10, 64, 23, 23])\n",
      "After reshaping:  torch.Size([10, 33856])\n",
      "After dropout:  torch.Size([10, 33856])\n",
      "After fc1:  torch.Size([10, 3])\n",
      "After softmax:  torch.Size([10, 3])\n",
      "Input:  torch.Size([10, 1, 227, 227])\n",
      "After layer1:  torch.Size([10, 32, 74, 74])\n",
      "After layer2:  torch.Size([10, 64, 23, 23])\n",
      "After reshaping:  torch.Size([10, 33856])\n",
      "After dropout:  torch.Size([10, 33856])\n",
      "After fc1:  torch.Size([10, 3])\n",
      "After softmax:  torch.Size([10, 3])\n",
      "Input:  torch.Size([10, 1, 227, 227])\n",
      "After layer1:  torch.Size([10, 32, 74, 74])\n",
      "After layer2:  torch.Size([10, 64, 23, 23])\n",
      "After reshaping:  torch.Size([10, 33856])\n",
      "After dropout:  torch.Size([10, 33856])\n",
      "After fc1:  torch.Size([10, 3])\n",
      "After softmax:  torch.Size([10, 3])\n",
      "Input:  torch.Size([10, 1, 227, 227])\n",
      "After layer1:  torch.Size([10, 32, 74, 74])\n",
      "After layer2:  torch.Size([10, 64, 23, 23])\n",
      "After reshaping:  torch.Size([10, 33856])\n",
      "After dropout:  torch.Size([10, 33856])\n",
      "After fc1:  torch.Size([10, 3])\n",
      "After softmax:  torch.Size([10, 3])\n",
      "Input:  torch.Size([10, 1, 227, 227])\n",
      "After layer1:  torch.Size([10, 32, 74, 74])\n",
      "After layer2:  torch.Size([10, 64, 23, 23])\n",
      "After reshaping:  torch.Size([10, 33856])\n",
      "After dropout:  torch.Size([10, 33856])\n",
      "After fc1:  torch.Size([10, 3])\n",
      "After softmax:  torch.Size([10, 3])\n",
      "Input:  torch.Size([10, 1, 227, 227])\n",
      "After layer1:  torch.Size([10, 32, 74, 74])\n",
      "After layer2:  torch.Size([10, 64, 23, 23])\n",
      "After reshaping:  torch.Size([10, 33856])\n",
      "After dropout:  torch.Size([10, 33856])\n",
      "After fc1:  torch.Size([10, 3])\n",
      "After softmax:  torch.Size([10, 3])\n",
      "Input:  torch.Size([10, 1, 227, 227])\n",
      "After layer1:  torch.Size([10, 32, 74, 74])\n",
      "After layer2:  torch.Size([10, 64, 23, 23])\n",
      "After reshaping:  torch.Size([10, 33856])\n",
      "After dropout:  torch.Size([10, 33856])\n",
      "After fc1:  torch.Size([10, 3])\n",
      "After softmax:  torch.Size([10, 3])\n",
      "Input:  torch.Size([10, 1, 227, 227])\n",
      "After layer1:  torch.Size([10, 32, 74, 74])\n",
      "After layer2:  torch.Size([10, 64, 23, 23])\n",
      "After reshaping:  torch.Size([10, 33856])\n",
      "After dropout:  torch.Size([10, 33856])\n",
      "After fc1:  torch.Size([10, 3])\n",
      "After softmax:  torch.Size([10, 3])\n",
      "Input:  torch.Size([10, 1, 227, 227])\n",
      "After layer1:  torch.Size([10, 32, 74, 74])\n",
      "After layer2:  torch.Size([10, 64, 23, 23])\n",
      "After reshaping:  torch.Size([10, 33856])\n",
      "After dropout:  torch.Size([10, 33856])\n",
      "After fc1:  torch.Size([10, 3])\n",
      "After softmax:  torch.Size([10, 3])\n",
      "Input:  torch.Size([10, 1, 227, 227])\n",
      "After layer1:  torch.Size([10, 32, 74, 74])\n",
      "After layer2:  torch.Size([10, 64, 23, 23])\n",
      "After reshaping:  torch.Size([10, 33856])\n",
      "After dropout:  torch.Size([10, 33856])\n",
      "After fc1:  torch.Size([10, 3])\n",
      "After softmax:  torch.Size([10, 3])\n",
      "Input:  torch.Size([10, 1, 227, 227])\n",
      "After layer1:  torch.Size([10, 32, 74, 74])\n",
      "After layer2:  torch.Size([10, 64, 23, 23])\n",
      "After reshaping:  torch.Size([10, 33856])\n",
      "After dropout:  torch.Size([10, 33856])\n",
      "After fc1:  torch.Size([10, 3])\n",
      "After softmax:  torch.Size([10, 3])\n",
      "Input:  torch.Size([10, 1, 227, 227])\n",
      "After layer1:  torch.Size([10, 32, 74, 74])\n",
      "After layer2:  torch.Size([10, 64, 23, 23])\n",
      "After reshaping:  torch.Size([10, 33856])\n",
      "After dropout:  torch.Size([10, 33856])\n",
      "After fc1:  torch.Size([10, 3])\n",
      "After softmax:  torch.Size([10, 3])\n",
      "Input:  torch.Size([10, 1, 227, 227])\n",
      "After layer1:  torch.Size([10, 32, 74, 74])\n",
      "After layer2:  torch.Size([10, 64, 23, 23])\n",
      "After reshaping:  torch.Size([10, 33856])\n",
      "After dropout:  torch.Size([10, 33856])\n",
      "After fc1:  torch.Size([10, 3])\n",
      "After softmax:  torch.Size([10, 3])\n",
      "Input:  torch.Size([10, 1, 227, 227])\n",
      "After layer1:  torch.Size([10, 32, 74, 74])\n",
      "After layer2:  torch.Size([10, 64, 23, 23])\n",
      "After reshaping:  torch.Size([10, 33856])\n",
      "After dropout:  torch.Size([10, 33856])\n",
      "After fc1:  torch.Size([10, 3])\n",
      "After softmax:  torch.Size([10, 3])\n",
      "Input:  torch.Size([10, 1, 227, 227])\n",
      "After layer1:  torch.Size([10, 32, 74, 74])\n",
      "After layer2:  torch.Size([10, 64, 23, 23])\n",
      "After reshaping:  torch.Size([10, 33856])\n",
      "After dropout:  torch.Size([10, 33856])\n",
      "After fc1:  torch.Size([10, 3])\n",
      "After softmax:  torch.Size([10, 3])\n",
      "Input:  torch.Size([10, 1, 227, 227])\n",
      "After layer1:  torch.Size([10, 32, 74, 74])\n",
      "After layer2:  torch.Size([10, 64, 23, 23])\n",
      "After reshaping:  torch.Size([10, 33856])\n",
      "After dropout:  torch.Size([10, 33856])\n",
      "After fc1:  torch.Size([10, 3])\n",
      "After softmax:  torch.Size([10, 3])\n",
      "Input:  torch.Size([10, 1, 227, 227])\n",
      "After layer1:  torch.Size([10, 32, 74, 74])\n",
      "After layer2:  torch.Size([10, 64, 23, 23])\n",
      "After reshaping:  torch.Size([10, 33856])\n",
      "After dropout:  torch.Size([10, 33856])\n",
      "After fc1:  torch.Size([10, 3])\n",
      "After softmax:  torch.Size([10, 3])\n",
      "Input:  torch.Size([10, 1, 227, 227])\n",
      "After layer1:  torch.Size([10, 32, 74, 74])\n",
      "After layer2:  torch.Size([10, 64, 23, 23])\n",
      "After reshaping:  torch.Size([10, 33856])\n",
      "After dropout:  torch.Size([10, 33856])\n",
      "After fc1:  torch.Size([10, 3])\n",
      "After softmax:  torch.Size([10, 3])\n",
      "Input:  torch.Size([10, 1, 227, 227])\n",
      "After layer1:  torch.Size([10, 32, 74, 74])\n",
      "After layer2:  torch.Size([10, 64, 23, 23])\n",
      "After reshaping:  torch.Size([10, 33856])\n",
      "After dropout:  torch.Size([10, 33856])\n",
      "After fc1:  torch.Size([10, 3])\n",
      "After softmax:  torch.Size([10, 3])\n",
      "Input:  torch.Size([10, 1, 227, 227])\n",
      "After layer1:  torch.Size([10, 32, 74, 74])\n",
      "After layer2:  torch.Size([10, 64, 23, 23])\n",
      "After reshaping:  torch.Size([10, 33856])\n",
      "After dropout:  torch.Size([10, 33856])\n",
      "After fc1:  torch.Size([10, 3])\n",
      "After softmax:  torch.Size([10, 3])\n",
      "Input:  torch.Size([10, 1, 227, 227])\n",
      "After layer1:  torch.Size([10, 32, 74, 74])\n",
      "After layer2:  torch.Size([10, 64, 23, 23])\n",
      "After reshaping:  torch.Size([10, 33856])\n",
      "After dropout:  torch.Size([10, 33856])\n",
      "After fc1:  torch.Size([10, 3])\n",
      "After softmax:  torch.Size([10, 3])\n",
      "Input:  torch.Size([10, 1, 227, 227])\n",
      "After layer1:  torch.Size([10, 32, 74, 74])\n",
      "After layer2:  torch.Size([10, 64, 23, 23])\n",
      "After reshaping:  torch.Size([10, 33856])\n",
      "After dropout:  torch.Size([10, 33856])\n",
      "After fc1:  torch.Size([10, 3])\n",
      "After softmax:  torch.Size([10, 3])\n",
      "Input:  torch.Size([10, 1, 227, 227])\n",
      "After layer1:  torch.Size([10, 32, 74, 74])\n",
      "After layer2:  torch.Size([10, 64, 23, 23])\n",
      "After reshaping:  torch.Size([10, 33856])\n",
      "After dropout:  torch.Size([10, 33856])\n",
      "After fc1:  torch.Size([10, 3])\n",
      "After softmax:  torch.Size([10, 3])\n",
      "Input:  torch.Size([10, 1, 227, 227])\n",
      "After layer1:  torch.Size([10, 32, 74, 74])\n",
      "After layer2:  torch.Size([10, 64, 23, 23])\n",
      "After reshaping:  torch.Size([10, 33856])\n",
      "After dropout:  torch.Size([10, 33856])\n",
      "After fc1:  torch.Size([10, 3])\n",
      "After softmax:  torch.Size([10, 3])\n",
      "Input:  torch.Size([10, 1, 227, 227])\n",
      "After layer1:  torch.Size([10, 32, 74, 74])\n",
      "After layer2:  torch.Size([10, 64, 23, 23])\n",
      "After reshaping:  torch.Size([10, 33856])\n",
      "After dropout:  torch.Size([10, 33856])\n",
      "After fc1:  torch.Size([10, 3])\n",
      "After softmax:  torch.Size([10, 3])\n",
      "Input:  torch.Size([10, 1, 227, 227])\n",
      "After layer1:  torch.Size([10, 32, 74, 74])\n",
      "After layer2:  torch.Size([10, 64, 23, 23])\n",
      "After reshaping:  torch.Size([10, 33856])\n",
      "After dropout:  torch.Size([10, 33856])\n",
      "After fc1:  torch.Size([10, 3])\n",
      "After softmax:  torch.Size([10, 3])\n",
      "Input:  torch.Size([10, 1, 227, 227])\n",
      "After layer1:  torch.Size([10, 32, 74, 74])\n",
      "After layer2:  torch.Size([10, 64, 23, 23])\n",
      "After reshaping:  torch.Size([10, 33856])\n",
      "After dropout:  torch.Size([10, 33856])\n",
      "After fc1:  torch.Size([10, 3])\n",
      "After softmax:  torch.Size([10, 3])\n",
      "Input:  torch.Size([10, 1, 227, 227])\n",
      "After layer1:  torch.Size([10, 32, 74, 74])\n",
      "After layer2:  torch.Size([10, 64, 23, 23])\n",
      "After reshaping:  torch.Size([10, 33856])\n",
      "After dropout:  torch.Size([10, 33856])\n",
      "After fc1:  torch.Size([10, 3])\n",
      "After softmax:  torch.Size([10, 3])\n",
      "Input:  torch.Size([10, 1, 227, 227])\n",
      "After layer1:  torch.Size([10, 32, 74, 74])\n",
      "After layer2:  torch.Size([10, 64, 23, 23])\n",
      "After reshaping:  torch.Size([10, 33856])\n",
      "After dropout:  torch.Size([10, 33856])\n",
      "After fc1:  torch.Size([10, 3])\n",
      "After softmax:  torch.Size([10, 3])\n",
      "Input:  torch.Size([10, 1, 227, 227])\n",
      "After layer1:  torch.Size([10, 32, 74, 74])\n",
      "After layer2:  torch.Size([10, 64, 23, 23])\n",
      "After reshaping:  torch.Size([10, 33856])\n",
      "After dropout:  torch.Size([10, 33856])\n",
      "After fc1:  torch.Size([10, 3])\n",
      "After softmax:  torch.Size([10, 3])\n",
      "Input:  torch.Size([10, 1, 227, 227])\n",
      "After layer1:  torch.Size([10, 32, 74, 74])\n",
      "After layer2:  torch.Size([10, 64, 23, 23])\n",
      "After reshaping:  torch.Size([10, 33856])\n",
      "After dropout:  torch.Size([10, 33856])\n",
      "After fc1:  torch.Size([10, 3])\n",
      "After softmax:  torch.Size([10, 3])\n",
      "Input:  torch.Size([10, 1, 227, 227])\n",
      "After layer1:  torch.Size([10, 32, 74, 74])\n",
      "After layer2:  torch.Size([10, 64, 23, 23])\n",
      "After reshaping:  torch.Size([10, 33856])\n",
      "After dropout:  torch.Size([10, 33856])\n",
      "After fc1:  torch.Size([10, 3])\n",
      "After softmax:  torch.Size([10, 3])\n",
      "Input:  torch.Size([10, 1, 227, 227])\n",
      "After layer1:  torch.Size([10, 32, 74, 74])\n",
      "After layer2:  torch.Size([10, 64, 23, 23])\n",
      "After reshaping:  torch.Size([10, 33856])\n",
      "After dropout:  torch.Size([10, 33856])\n",
      "After fc1:  torch.Size([10, 3])\n",
      "After softmax:  torch.Size([10, 3])\n",
      "Input:  torch.Size([10, 1, 227, 227])\n",
      "After layer1:  torch.Size([10, 32, 74, 74])\n",
      "After layer2:  torch.Size([10, 64, 23, 23])\n",
      "After reshaping:  torch.Size([10, 33856])\n",
      "After dropout:  torch.Size([10, 33856])\n",
      "After fc1:  torch.Size([10, 3])\n",
      "After softmax:  torch.Size([10, 3])\n",
      "Input:  torch.Size([10, 1, 227, 227])\n",
      "After layer1:  torch.Size([10, 32, 74, 74])\n",
      "After layer2:  torch.Size([10, 64, 23, 23])\n",
      "After reshaping:  torch.Size([10, 33856])\n",
      "After dropout:  torch.Size([10, 33856])\n",
      "After fc1:  torch.Size([10, 3])\n",
      "After softmax:  torch.Size([10, 3])\n",
      "Input:  torch.Size([10, 1, 227, 227])\n",
      "After layer1:  torch.Size([10, 32, 74, 74])\n",
      "After layer2:  torch.Size([10, 64, 23, 23])\n",
      "After reshaping:  torch.Size([10, 33856])\n",
      "After dropout:  torch.Size([10, 33856])\n",
      "After fc1:  torch.Size([10, 3])\n",
      "After softmax:  torch.Size([10, 3])\n",
      "Input:  torch.Size([10, 1, 227, 227])\n",
      "After layer1:  torch.Size([10, 32, 74, 74])\n",
      "After layer2:  torch.Size([10, 64, 23, 23])\n",
      "After reshaping:  torch.Size([10, 33856])\n",
      "After dropout:  torch.Size([10, 33856])\n",
      "After fc1:  torch.Size([10, 3])\n",
      "After softmax:  torch.Size([10, 3])\n",
      "Input:  torch.Size([10, 1, 227, 227])\n",
      "After layer1:  torch.Size([10, 32, 74, 74])\n",
      "After layer2:  torch.Size([10, 64, 23, 23])\n",
      "After reshaping:  torch.Size([10, 33856])\n",
      "After dropout:  torch.Size([10, 33856])\n",
      "After fc1:  torch.Size([10, 3])\n",
      "After softmax:  torch.Size([10, 3])\n",
      "Input:  torch.Size([10, 1, 227, 227])\n",
      "After layer1:  torch.Size([10, 32, 74, 74])\n",
      "After layer2:  torch.Size([10, 64, 23, 23])\n",
      "After reshaping:  torch.Size([10, 33856])\n",
      "After dropout:  torch.Size([10, 33856])\n",
      "After fc1:  torch.Size([10, 3])\n",
      "After softmax:  torch.Size([10, 3])\n",
      "Input:  torch.Size([10, 1, 227, 227])\n",
      "After layer1:  torch.Size([10, 32, 74, 74])\n",
      "After layer2:  torch.Size([10, 64, 23, 23])\n",
      "After reshaping:  torch.Size([10, 33856])\n",
      "After dropout:  torch.Size([10, 33856])\n",
      "After fc1:  torch.Size([10, 3])\n",
      "After softmax:  torch.Size([10, 3])\n",
      "Input:  torch.Size([10, 1, 227, 227])\n",
      "After layer1:  torch.Size([10, 32, 74, 74])\n",
      "After layer2:  torch.Size([10, 64, 23, 23])\n",
      "After reshaping:  torch.Size([10, 33856])\n",
      "After dropout:  torch.Size([10, 33856])\n",
      "After fc1:  torch.Size([10, 3])\n",
      "After softmax:  torch.Size([10, 3])\n",
      "Input:  torch.Size([10, 1, 227, 227])\n",
      "After layer1:  torch.Size([10, 32, 74, 74])\n",
      "After layer2:  torch.Size([10, 64, 23, 23])\n",
      "After reshaping:  torch.Size([10, 33856])\n",
      "After dropout:  torch.Size([10, 33856])\n",
      "After fc1:  torch.Size([10, 3])\n",
      "After softmax:  torch.Size([10, 3])\n",
      "Input:  torch.Size([10, 1, 227, 227])\n",
      "After layer1:  torch.Size([10, 32, 74, 74])\n",
      "After layer2:  torch.Size([10, 64, 23, 23])\n",
      "After reshaping:  torch.Size([10, 33856])\n",
      "After dropout:  torch.Size([10, 33856])\n",
      "After fc1:  torch.Size([10, 3])\n",
      "After softmax:  torch.Size([10, 3])\n",
      "Input:  torch.Size([10, 1, 227, 227])\n",
      "After layer1:  torch.Size([10, 32, 74, 74])\n",
      "After layer2:  torch.Size([10, 64, 23, 23])\n",
      "After reshaping:  torch.Size([10, 33856])\n",
      "After dropout:  torch.Size([10, 33856])\n",
      "After fc1:  torch.Size([10, 3])\n",
      "After softmax:  torch.Size([10, 3])\n",
      "Input:  torch.Size([10, 1, 227, 227])\n",
      "After layer1:  torch.Size([10, 32, 74, 74])\n",
      "After layer2:  torch.Size([10, 64, 23, 23])\n",
      "After reshaping:  torch.Size([10, 33856])\n",
      "After dropout:  torch.Size([10, 33856])\n",
      "After fc1:  torch.Size([10, 3])\n",
      "After softmax:  torch.Size([10, 3])\n",
      "Input:  torch.Size([10, 1, 227, 227])\n",
      "After layer1:  torch.Size([10, 32, 74, 74])\n",
      "After layer2:  torch.Size([10, 64, 23, 23])\n",
      "After reshaping:  torch.Size([10, 33856])\n",
      "After dropout:  torch.Size([10, 33856])\n",
      "After fc1:  torch.Size([10, 3])\n",
      "After softmax:  torch.Size([10, 3])\n",
      "Input:  torch.Size([10, 1, 227, 227])\n",
      "After layer1:  torch.Size([10, 32, 74, 74])\n",
      "After layer2:  torch.Size([10, 64, 23, 23])\n",
      "After reshaping:  torch.Size([10, 33856])\n",
      "After dropout:  torch.Size([10, 33856])\n",
      "After fc1:  torch.Size([10, 3])\n",
      "After softmax:  torch.Size([10, 3])\n",
      "Input:  torch.Size([10, 1, 227, 227])\n",
      "After layer1:  torch.Size([10, 32, 74, 74])\n",
      "After layer2:  torch.Size([10, 64, 23, 23])\n",
      "After reshaping:  torch.Size([10, 33856])\n",
      "After dropout:  torch.Size([10, 33856])\n",
      "After fc1:  torch.Size([10, 3])\n",
      "After softmax:  torch.Size([10, 3])\n",
      "Input:  torch.Size([10, 1, 227, 227])\n",
      "After layer1:  torch.Size([10, 32, 74, 74])\n",
      "After layer2:  torch.Size([10, 64, 23, 23])\n",
      "After reshaping:  torch.Size([10, 33856])\n",
      "After dropout:  torch.Size([10, 33856])\n",
      "After fc1:  torch.Size([10, 3])\n",
      "After softmax:  torch.Size([10, 3])\n",
      "Input:  torch.Size([10, 1, 227, 227])\n",
      "After layer1:  torch.Size([10, 32, 74, 74])\n",
      "After layer2:  torch.Size([10, 64, 23, 23])\n",
      "After reshaping:  torch.Size([10, 33856])\n",
      "After dropout:  torch.Size([10, 33856])\n",
      "After fc1:  torch.Size([10, 3])\n",
      "After softmax:  torch.Size([10, 3])\n",
      "Input:  torch.Size([10, 1, 227, 227])\n",
      "After layer1:  torch.Size([10, 32, 74, 74])\n",
      "After layer2:  torch.Size([10, 64, 23, 23])\n",
      "After reshaping:  torch.Size([10, 33856])\n",
      "After dropout:  torch.Size([10, 33856])\n",
      "After fc1:  torch.Size([10, 3])\n",
      "After softmax:  torch.Size([10, 3])\n",
      "Input:  torch.Size([10, 1, 227, 227])\n",
      "After layer1:  torch.Size([10, 32, 74, 74])\n",
      "After layer2:  torch.Size([10, 64, 23, 23])\n",
      "After reshaping:  torch.Size([10, 33856])\n",
      "After dropout:  torch.Size([10, 33856])\n",
      "After fc1:  torch.Size([10, 3])\n",
      "After softmax:  torch.Size([10, 3])\n",
      "Input:  torch.Size([10, 1, 227, 227])\n",
      "After layer1:  torch.Size([10, 32, 74, 74])\n",
      "After layer2:  torch.Size([10, 64, 23, 23])\n",
      "After reshaping:  torch.Size([10, 33856])\n",
      "After dropout:  torch.Size([10, 33856])\n",
      "After fc1:  torch.Size([10, 3])\n",
      "After softmax:  torch.Size([10, 3])\n",
      "Input:  torch.Size([10, 1, 227, 227])\n",
      "After layer1:  torch.Size([10, 32, 74, 74])\n",
      "After layer2:  torch.Size([10, 64, 23, 23])\n",
      "After reshaping:  torch.Size([10, 33856])\n",
      "After dropout:  torch.Size([10, 33856])\n",
      "After fc1:  torch.Size([10, 3])\n",
      "After softmax:  torch.Size([10, 3])\n",
      "Input:  torch.Size([10, 1, 227, 227])\n",
      "After layer1:  torch.Size([10, 32, 74, 74])\n",
      "After layer2:  torch.Size([10, 64, 23, 23])\n",
      "After reshaping:  torch.Size([10, 33856])\n",
      "After dropout:  torch.Size([10, 33856])\n",
      "After fc1:  torch.Size([10, 3])\n",
      "After softmax:  torch.Size([10, 3])\n",
      "Input:  torch.Size([10, 1, 227, 227])\n",
      "After layer1:  torch.Size([10, 32, 74, 74])\n",
      "After layer2:  torch.Size([10, 64, 23, 23])\n",
      "After reshaping:  torch.Size([10, 33856])\n",
      "After dropout:  torch.Size([10, 33856])\n",
      "After fc1:  torch.Size([10, 3])\n",
      "After softmax:  torch.Size([10, 3])\n",
      "Input:  torch.Size([10, 1, 227, 227])\n",
      "After layer1:  torch.Size([10, 32, 74, 74])\n",
      "After layer2:  torch.Size([10, 64, 23, 23])\n",
      "After reshaping:  torch.Size([10, 33856])\n",
      "After dropout:  torch.Size([10, 33856])\n",
      "After fc1:  torch.Size([10, 3])\n",
      "After softmax:  torch.Size([10, 3])\n",
      "Input:  torch.Size([10, 1, 227, 227])\n",
      "After layer1:  torch.Size([10, 32, 74, 74])\n",
      "After layer2:  torch.Size([10, 64, 23, 23])\n",
      "After reshaping:  torch.Size([10, 33856])\n",
      "After dropout:  torch.Size([10, 33856])\n",
      "After fc1:  torch.Size([10, 3])\n",
      "After softmax:  torch.Size([10, 3])\n",
      "Input:  torch.Size([10, 1, 227, 227])\n",
      "After layer1:  torch.Size([10, 32, 74, 74])\n",
      "After layer2:  torch.Size([10, 64, 23, 23])\n",
      "After reshaping:  torch.Size([10, 33856])\n",
      "After dropout:  torch.Size([10, 33856])\n",
      "After fc1:  torch.Size([10, 3])\n",
      "After softmax:  torch.Size([10, 3])\n",
      "Input:  torch.Size([10, 1, 227, 227])\n",
      "After layer1:  torch.Size([10, 32, 74, 74])\n",
      "After layer2:  torch.Size([10, 64, 23, 23])\n",
      "After reshaping:  torch.Size([10, 33856])\n",
      "After dropout:  torch.Size([10, 33856])\n",
      "After fc1:  torch.Size([10, 3])\n",
      "After softmax:  torch.Size([10, 3])\n",
      "Input:  torch.Size([10, 1, 227, 227])\n",
      "After layer1:  torch.Size([10, 32, 74, 74])\n",
      "After layer2:  torch.Size([10, 64, 23, 23])\n",
      "After reshaping:  torch.Size([10, 33856])\n",
      "After dropout:  torch.Size([10, 33856])\n",
      "After fc1:  torch.Size([10, 3])\n",
      "After softmax:  torch.Size([10, 3])\n",
      "Input:  torch.Size([10, 1, 227, 227])\n",
      "After layer1:  torch.Size([10, 32, 74, 74])\n",
      "After layer2:  torch.Size([10, 64, 23, 23])\n",
      "After reshaping:  torch.Size([10, 33856])\n",
      "After dropout:  torch.Size([10, 33856])\n",
      "After fc1:  torch.Size([10, 3])\n",
      "After softmax:  torch.Size([10, 3])\n",
      "Input:  torch.Size([10, 1, 227, 227])\n",
      "After layer1:  torch.Size([10, 32, 74, 74])\n",
      "After layer2:  torch.Size([10, 64, 23, 23])\n",
      "After reshaping:  torch.Size([10, 33856])\n",
      "After dropout:  torch.Size([10, 33856])\n",
      "After fc1:  torch.Size([10, 3])\n",
      "After softmax:  torch.Size([10, 3])\n",
      "Input:  torch.Size([10, 1, 227, 227])\n",
      "After layer1:  torch.Size([10, 32, 74, 74])\n",
      "After layer2:  torch.Size([10, 64, 23, 23])\n",
      "After reshaping:  torch.Size([10, 33856])\n",
      "After dropout:  torch.Size([10, 33856])\n",
      "After fc1:  torch.Size([10, 3])\n",
      "After softmax:  torch.Size([10, 3])\n",
      "Input:  torch.Size([10, 1, 227, 227])\n",
      "After layer1:  torch.Size([10, 32, 74, 74])\n",
      "After layer2:  torch.Size([10, 64, 23, 23])\n",
      "After reshaping:  torch.Size([10, 33856])\n",
      "After dropout:  torch.Size([10, 33856])\n",
      "After fc1:  torch.Size([10, 3])\n",
      "After softmax:  torch.Size([10, 3])\n",
      "Input:  torch.Size([10, 1, 227, 227])\n",
      "After layer1:  torch.Size([10, 32, 74, 74])\n",
      "After layer2:  torch.Size([10, 64, 23, 23])\n",
      "After reshaping:  torch.Size([10, 33856])\n",
      "After dropout:  torch.Size([10, 33856])\n",
      "After fc1:  torch.Size([10, 3])\n",
      "After softmax:  torch.Size([10, 3])\n",
      "Input:  torch.Size([10, 1, 227, 227])\n",
      "After layer1:  torch.Size([10, 32, 74, 74])\n",
      "After layer2:  torch.Size([10, 64, 23, 23])\n",
      "After reshaping:  torch.Size([10, 33856])\n",
      "After dropout:  torch.Size([10, 33856])\n",
      "After fc1:  torch.Size([10, 3])\n",
      "After softmax:  torch.Size([10, 3])\n",
      "Input:  torch.Size([10, 1, 227, 227])\n",
      "After layer1:  torch.Size([10, 32, 74, 74])\n",
      "After layer2:  torch.Size([10, 64, 23, 23])\n",
      "After reshaping:  torch.Size([10, 33856])\n",
      "After dropout:  torch.Size([10, 33856])\n",
      "After fc1:  torch.Size([10, 3])\n",
      "After softmax:  torch.Size([10, 3])\n",
      "Input:  torch.Size([10, 1, 227, 227])\n",
      "After layer1:  torch.Size([10, 32, 74, 74])\n",
      "After layer2:  torch.Size([10, 64, 23, 23])\n",
      "After reshaping:  torch.Size([10, 33856])\n",
      "After dropout:  torch.Size([10, 33856])\n",
      "After fc1:  torch.Size([10, 3])\n",
      "After softmax:  torch.Size([10, 3])\n",
      "Input:  torch.Size([10, 1, 227, 227])\n",
      "After layer1:  torch.Size([10, 32, 74, 74])\n",
      "After layer2:  torch.Size([10, 64, 23, 23])\n",
      "After reshaping:  torch.Size([10, 33856])\n",
      "After dropout:  torch.Size([10, 33856])\n",
      "After fc1:  torch.Size([10, 3])\n",
      "After softmax:  torch.Size([10, 3])\n",
      "Input:  torch.Size([10, 1, 227, 227])\n",
      "After layer1:  torch.Size([10, 32, 74, 74])\n",
      "After layer2:  torch.Size([10, 64, 23, 23])\n",
      "After reshaping:  torch.Size([10, 33856])\n",
      "After dropout:  torch.Size([10, 33856])\n",
      "After fc1:  torch.Size([10, 3])\n",
      "After softmax:  torch.Size([10, 3])\n",
      "Input:  torch.Size([10, 1, 227, 227])\n",
      "After layer1:  torch.Size([10, 32, 74, 74])\n",
      "After layer2:  torch.Size([10, 64, 23, 23])\n",
      "After reshaping:  torch.Size([10, 33856])\n",
      "After dropout:  torch.Size([10, 33856])\n",
      "After fc1:  torch.Size([10, 3])\n",
      "After softmax:  torch.Size([10, 3])\n",
      "Input:  torch.Size([10, 1, 227, 227])\n",
      "After layer1:  torch.Size([10, 32, 74, 74])\n",
      "After layer2:  torch.Size([10, 64, 23, 23])\n",
      "After reshaping:  torch.Size([10, 33856])\n",
      "After dropout:  torch.Size([10, 33856])\n",
      "After fc1:  torch.Size([10, 3])\n",
      "After softmax:  torch.Size([10, 3])\n",
      "Input:  torch.Size([10, 1, 227, 227])\n",
      "After layer1:  torch.Size([10, 32, 74, 74])\n",
      "After layer2:  torch.Size([10, 64, 23, 23])\n",
      "After reshaping:  torch.Size([10, 33856])\n",
      "After dropout:  torch.Size([10, 33856])\n",
      "After fc1:  torch.Size([10, 3])\n",
      "After softmax:  torch.Size([10, 3])\n",
      "Input:  torch.Size([10, 1, 227, 227])\n",
      "After layer1:  torch.Size([10, 32, 74, 74])\n",
      "After layer2:  torch.Size([10, 64, 23, 23])\n",
      "After reshaping:  torch.Size([10, 33856])\n",
      "After dropout:  torch.Size([10, 33856])\n",
      "After fc1:  torch.Size([10, 3])\n",
      "After softmax:  torch.Size([10, 3])\n",
      "Input:  torch.Size([10, 1, 227, 227])\n",
      "After layer1:  torch.Size([10, 32, 74, 74])\n",
      "After layer2:  torch.Size([10, 64, 23, 23])\n",
      "After reshaping:  torch.Size([10, 33856])\n",
      "After dropout:  torch.Size([10, 33856])\n",
      "After fc1:  torch.Size([10, 3])\n",
      "After softmax:  torch.Size([10, 3])\n",
      "Input:  torch.Size([10, 1, 227, 227])\n",
      "After layer1:  torch.Size([10, 32, 74, 74])\n",
      "After layer2:  torch.Size([10, 64, 23, 23])\n",
      "After reshaping:  torch.Size([10, 33856])\n",
      "After dropout:  torch.Size([10, 33856])\n",
      "After fc1:  torch.Size([10, 3])\n",
      "After softmax:  torch.Size([10, 3])\n",
      "Input:  torch.Size([10, 1, 227, 227])\n",
      "After layer1:  torch.Size([10, 32, 74, 74])\n",
      "After layer2:  torch.Size([10, 64, 23, 23])\n",
      "After reshaping:  torch.Size([10, 33856])\n",
      "After dropout:  torch.Size([10, 33856])\n",
      "After fc1:  torch.Size([10, 3])\n",
      "After softmax:  torch.Size([10, 3])\n",
      "Input:  torch.Size([10, 1, 227, 227])\n",
      "After layer1:  torch.Size([10, 32, 74, 74])\n",
      "After layer2:  torch.Size([10, 64, 23, 23])\n",
      "After reshaping:  torch.Size([10, 33856])\n",
      "After dropout:  torch.Size([10, 33856])\n",
      "After fc1:  torch.Size([10, 3])\n",
      "After softmax:  torch.Size([10, 3])\n",
      "Input:  torch.Size([10, 1, 227, 227])\n",
      "After layer1:  torch.Size([10, 32, 74, 74])\n",
      "After layer2:  torch.Size([10, 64, 23, 23])\n",
      "After reshaping:  torch.Size([10, 33856])\n",
      "After dropout:  torch.Size([10, 33856])\n",
      "After fc1:  torch.Size([10, 3])\n",
      "After softmax:  torch.Size([10, 3])\n",
      "Input:  torch.Size([10, 1, 227, 227])\n",
      "After layer1:  torch.Size([10, 32, 74, 74])\n",
      "After layer2:  torch.Size([10, 64, 23, 23])\n",
      "After reshaping:  torch.Size([10, 33856])\n",
      "After dropout:  torch.Size([10, 33856])\n",
      "After fc1:  torch.Size([10, 3])\n",
      "After softmax:  torch.Size([10, 3])\n",
      "Input:  torch.Size([10, 1, 227, 227])\n",
      "After layer1:  torch.Size([10, 32, 74, 74])\n",
      "After layer2:  torch.Size([10, 64, 23, 23])\n",
      "After reshaping:  torch.Size([10, 33856])\n",
      "After dropout:  torch.Size([10, 33856])\n",
      "After fc1:  torch.Size([10, 3])\n",
      "After softmax:  torch.Size([10, 3])\n",
      "Input:  torch.Size([10, 1, 227, 227])\n",
      "After layer1:  torch.Size([10, 32, 74, 74])\n",
      "After layer2:  torch.Size([10, 64, 23, 23])\n",
      "After reshaping:  torch.Size([10, 33856])\n",
      "After dropout:  torch.Size([10, 33856])\n",
      "After fc1:  torch.Size([10, 3])\n",
      "After softmax:  torch.Size([10, 3])\n",
      "Input:  torch.Size([10, 1, 227, 227])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/ahmadar/Downloads/ML/ML-Project.ipynb Cell 16\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ahmadar/Downloads/ML/ML-Project.ipynb#Y202sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m criterion \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mCrossEntropyLoss()\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ahmadar/Downloads/ML/ML-Project.ipynb#Y202sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m optimizer \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mAdam(model\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39m\u001b[39m0.0003\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/ahmadar/Downloads/ML/ML-Project.ipynb#Y202sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m train_and_evaluate_model(\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ahmadar/Downloads/ML/ML-Project.ipynb#Y202sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     model,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ahmadar/Downloads/ML/ML-Project.ipynb#Y202sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     train_loader_MRI,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ahmadar/Downloads/ML/ML-Project.ipynb#Y202sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     test_loader_MRI,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ahmadar/Downloads/ML/ML-Project.ipynb#Y202sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     criterion,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ahmadar/Downloads/ML/ML-Project.ipynb#Y202sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     optimizer,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ahmadar/Downloads/ML/ML-Project.ipynb#Y202sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     num_epochs\u001b[39m=\u001b[39;49m\u001b[39m30\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ahmadar/Downloads/ML/ML-Project.ipynb#Y202sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mMRI_Initial_Paper\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ahmadar/Downloads/ML/ML-Project.ipynb#Y202sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m )\n",
      "\u001b[1;32m/Users/ahmadar/Downloads/ML/ML-Project.ipynb Cell 16\u001b[0m line \u001b[0;36m7\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ahmadar/Downloads/ML/ML-Project.ipynb#Y202sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m train_accuracies, test_accuracies, train_losses, test_losses \u001b[39m=\u001b[39m [], [], [], []\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ahmadar/Downloads/ML/ML-Project.ipynb#Y202sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_epochs):\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/ahmadar/Downloads/ML/ML-Project.ipynb#Y202sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     train_accuracy, train_loss \u001b[39m=\u001b[39m train(model, train_loader, criterion, optimizer)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ahmadar/Downloads/ML/ML-Project.ipynb#Y202sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     test_accuracy, test_loss \u001b[39m=\u001b[39m test(model, test_loader)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ahmadar/Downloads/ML/ML-Project.ipynb#Y202sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     train_accuracies\u001b[39m.\u001b[39mappend(train_accuracy)\n",
      "\u001b[1;32m/Users/ahmadar/Downloads/ML/ML-Project.ipynb Cell 16\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ahmadar/Downloads/ML/ML-Project.ipynb#Y202sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m images, labels \u001b[39m=\u001b[39m images\u001b[39m.\u001b[39mto(device), labels\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ahmadar/Downloads/ML/ML-Project.ipynb#Y202sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/ahmadar/Downloads/ML/ML-Project.ipynb#Y202sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m outputs \u001b[39m=\u001b[39m model(images)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ahmadar/Downloads/ML/ML-Project.ipynb#Y202sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ahmadar/Downloads/ML/ML-Project.ipynb#Y202sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/Users/ahmadar/Downloads/ML/ML-Project.ipynb Cell 16\u001b[0m line \u001b[0;36m6\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ahmadar/Downloads/ML/ML-Project.ipynb#Y202sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ahmadar/Downloads/ML/ML-Project.ipynb#Y202sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mInput: \u001b[39m\u001b[39m\"\u001b[39m, x\u001b[39m.\u001b[39mshape)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/ahmadar/Downloads/ML/ML-Project.ipynb#Y202sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlayer1(x)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ahmadar/Downloads/ML/ML-Project.ipynb#Y202sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mAfter layer1: \u001b[39m\u001b[39m\"\u001b[39m, out\u001b[39m.\u001b[39mshape)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ahmadar/Downloads/ML/ML-Project.ipynb#Y202sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer2(out)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 463\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    456\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    457\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    458\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 459\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    460\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# MAT SETUP\n",
    "model = MyCNN(input_size=227, classes=3, in_channels=1).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0003)\n",
    "train_and_evaluate_model(\n",
    "    model,\n",
    "    train_loader_MRI,\n",
    "    test_loader_MRI,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    num_epochs=30,\n",
    "    name=\"MRI_Initial_Paper\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FMnist Data set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_set = torchvision.datasets.FashionMNIST(\n",
    "    \"./data\", download=True, transform=transforms.Compose([transforms.ToTensor()])\n",
    ")\n",
    "\n",
    "test_set = torchvision.datasets.FashionMNIST(\n",
    "    \"./data\",\n",
    "    download=True,\n",
    "    train=False,\n",
    "    transform=transforms.Compose([transforms.ToTensor()]),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 0.2860410809516907\n",
      "Standard Deviation: 0.3530237078666687\n"
     ]
    }
   ],
   "source": [
    "# Initializing variables to store sum and squared sum\n",
    "total_sum = 0\n",
    "total_squared_sum = 0\n",
    "num_pixels = 0\n",
    "\n",
    "# Calculating mean and standard deviation\n",
    "for data, _ in train_set:\n",
    "    total_sum += data.sum()\n",
    "    total_squared_sum += (data**2).sum()\n",
    "    num_pixels += data.numel()\n",
    "\n",
    "mean = total_sum / num_pixels\n",
    "std_dev = (total_squared_sum / num_pixels - mean**2) ** 0.5\n",
    "\n",
    "print(f\"Mean: {mean.item()}\")\n",
    "print(f\"Standard Deviation: {std_dev.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Assuming these are the calculated mean and standard deviation values\n",
    "mean_value = mean.item()  # replace with the calculated mean value\n",
    "std_value = std_dev.item()  # replace with the calculated std deviation value\n",
    "\n",
    "# Updated transformations with normalization\n",
    "transforming_img = transforms.Compose(\n",
    "    [transforms.ToTensor(), transforms.Normalize((mean_value,), (std_value,))]\n",
    ")\n",
    "\n",
    "# Loading the FashionMNIST datasets with the updated transformations\n",
    "train_set = torchvision.datasets.FashionMNIST(\n",
    "    \"./data\", download=True, transform=transforming_img\n",
    ")\n",
    "test_set = torchvision.datasets.FashionMNIST(\n",
    "    \"./data\", download=True, train=False, transform=transforming_img\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=10, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def output_label(label):\n",
    "    output_mapping = {\n",
    "        0: \"T-shirt/Top\",\n",
    "        1: \"Trouser\",\n",
    "        2: \"Pullover\",\n",
    "        3: \"Dress\",\n",
    "        4: \"Coat\",\n",
    "        5: \"Sandal\",\n",
    "        6: \"Shirt\",\n",
    "        7: \"Sneaker\",\n",
    "        8: \"Bag\",\n",
    "        9: \"Ankle Boot\",\n",
    "    }\n",
    "    input = label.item() if type(label) == torch.Tensor else label\n",
    "    return output_mapping[input]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 1, 28, 28])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = next(iter(train_loader))\n",
    "a[0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image, label = next(iter(train_set))\n",
    "plt.imshow(image.squeeze(), cmap=\"gray\")\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "torch.Size([10, 1, 28, 28]) torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "demo_loader = torch.utils.data.DataLoader(train_set, batch_size=10)\n",
    "\n",
    "batch = next(iter(demo_loader))\n",
    "images, labels = batch\n",
    "print(type(images), type(labels))\n",
    "print(images.shape, labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "torch.Size([10, 1, 28, 28]) torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "demo_loader = torch.utils.data.DataLoader(train_set, batch_size=10)\n",
    "\n",
    "batch = next(iter(demo_loader))\n",
    "images, labels = batch\n",
    "print(type(images), type(labels))\n",
    "print(images.shape, labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FMnist Initial using same parameter as paper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n",
      "Input:  torch.Size([10, 1, 28, 28])\n",
      "After layer1:  torch.Size([10, 32, 8, 8])\n",
      "After layer2:  torch.Size([10, 64, 1, 1])\n",
      "After reshaping:  torch.Size([10, 64])\n",
      "After dropout:  torch.Size([10, 64])\n",
      "After fc1:  torch.Size([10, 10])\n",
      "After softmax:  torch.Size([10, 10])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/ahmadar/Downloads/ML/ML-Project.ipynb Cell 30\u001b[0m line \u001b[0;36m6\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ahmadar/Downloads/ML/ML-Project.ipynb#Y221sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m criterion \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mCrossEntropyLoss()\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ahmadar/Downloads/ML/ML-Project.ipynb#Y221sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m optimizer \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mAdam(model\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39m\u001b[39m0.0003\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/ahmadar/Downloads/ML/ML-Project.ipynb#Y221sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m train_and_evaluate_model(\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ahmadar/Downloads/ML/ML-Project.ipynb#Y221sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     model,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ahmadar/Downloads/ML/ML-Project.ipynb#Y221sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     train_loader,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ahmadar/Downloads/ML/ML-Project.ipynb#Y221sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     test_loader,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ahmadar/Downloads/ML/ML-Project.ipynb#Y221sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     criterion,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ahmadar/Downloads/ML/ML-Project.ipynb#Y221sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     optimizer,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ahmadar/Downloads/ML/ML-Project.ipynb#Y221sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     num_epochs\u001b[39m=\u001b[39;49m\u001b[39m30\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ahmadar/Downloads/ML/ML-Project.ipynb#Y221sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mFMnist_Initial\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ahmadar/Downloads/ML/ML-Project.ipynb#Y221sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m )\n",
      "\u001b[1;32m/Users/ahmadar/Downloads/ML/ML-Project.ipynb Cell 30\u001b[0m line \u001b[0;36m7\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ahmadar/Downloads/ML/ML-Project.ipynb#Y221sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m train_accuracies, test_accuracies, train_losses, test_losses \u001b[39m=\u001b[39m [], [], [], []\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ahmadar/Downloads/ML/ML-Project.ipynb#Y221sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_epochs):\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/ahmadar/Downloads/ML/ML-Project.ipynb#Y221sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     train_accuracy, train_loss \u001b[39m=\u001b[39m train(model, train_loader, criterion, optimizer)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ahmadar/Downloads/ML/ML-Project.ipynb#Y221sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     test_accuracy, test_loss \u001b[39m=\u001b[39m test(model, test_loader)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ahmadar/Downloads/ML/ML-Project.ipynb#Y221sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     train_accuracies\u001b[39m.\u001b[39mappend(train_accuracy)\n",
      "\u001b[1;32m/Users/ahmadar/Downloads/ML/ML-Project.ipynb Cell 30\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ahmadar/Downloads/ML/ML-Project.ipynb#Y221sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m outputs \u001b[39m=\u001b[39m model(images)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ahmadar/Downloads/ML/ML-Project.ipynb#Y221sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/ahmadar/Downloads/ML/ML-Project.ipynb#Y221sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ahmadar/Downloads/ML/ML-Project.ipynb#Y221sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ahmadar/Downloads/ML/ML-Project.ipynb#Y221sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m total_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Model, loss function, optimizer initialization\n",
    "model = MyCNN().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0003)\n",
    "\n",
    "train_and_evaluate_model(\n",
    "    model,\n",
    "    train_loader,\n",
    "    test_loader,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    num_epochs=30,\n",
    "    name=\"FMnist_Initial\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x700 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Function to predict and plot\n",
    "def predict_and_plot(model, data_loader, num_images=5):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    images_so_far = 0\n",
    "    fig = plt.figure(figsize=(15, 7))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (images, labels) in enumerate(data_loader):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            for j in range(images.size()[0]):\n",
    "                images_so_far += 1\n",
    "                ax = plt.subplot(num_images // 5, 5, images_so_far)\n",
    "                ax.axis(\"off\")\n",
    "                ax.set_title(f\"Predicted: {preds[j]}, True: {labels[j]}\")\n",
    "                img = images.cpu().data[j].numpy().transpose((1, 2, 0))\n",
    "                plt.imshow(img.squeeze(), cmap=\"gray\")\n",
    "\n",
    "                if images_so_far == num_images:\n",
    "                    return\n",
    "\n",
    "\n",
    "# Predict and plot using the test data\n",
    "predict_and_plot(model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper Tuning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration 1/162:\n",
      "LR: 0.0001, BS: 10, Optimizer: SGD, Dropout: 0.3, Kernel: 3\n",
      "256\n",
      "Train accuracy is less than 70%. Skipping this configuration.\n",
      "------------------------------------------------------\n",
      "Configuration 2/162:\n",
      "LR: 0.0001, BS: 10, Optimizer: SGD, Dropout: 0.3, Kernel: 4\n",
      "64\n",
      "Train accuracy is less than 70%. Skipping this configuration.\n",
      "------------------------------------------------------\n",
      "Configuration 3/162:\n",
      "LR: 0.0001, BS: 10, Optimizer: SGD, Dropout: 0.3, Kernel: 5\n",
      "64\n",
      "Train accuracy is less than 70%. Skipping this configuration.\n",
      "------------------------------------------------------\n",
      "Configuration 4/162:\n",
      "LR: 0.0001, BS: 10, Optimizer: SGD, Dropout: 0.5, Kernel: 3\n",
      "256\n",
      "Train accuracy is less than 70%. Skipping this configuration.\n",
      "------------------------------------------------------\n",
      "Configuration 5/162:\n",
      "LR: 0.0001, BS: 10, Optimizer: SGD, Dropout: 0.5, Kernel: 4\n",
      "64\n",
      "Train accuracy is less than 70%. Skipping this configuration.\n",
      "------------------------------------------------------\n",
      "Configuration 6/162:\n",
      "LR: 0.0001, BS: 10, Optimizer: SGD, Dropout: 0.5, Kernel: 5\n",
      "64\n",
      "Train accuracy is less than 70%. Skipping this configuration.\n",
      "------------------------------------------------------\n",
      "Configuration 7/162:\n",
      "LR: 0.0001, BS: 10, Optimizer: SGD, Dropout: 0.6, Kernel: 3\n",
      "256\n",
      "Train accuracy is less than 70%. Skipping this configuration.\n",
      "------------------------------------------------------\n",
      "Configuration 8/162:\n",
      "LR: 0.0001, BS: 10, Optimizer: SGD, Dropout: 0.6, Kernel: 4\n",
      "64\n",
      "Train accuracy is less than 70%. Skipping this configuration.\n",
      "------------------------------------------------------\n",
      "Configuration 9/162:\n",
      "LR: 0.0001, BS: 10, Optimizer: SGD, Dropout: 0.6, Kernel: 5\n",
      "64\n",
      "Train accuracy is less than 70%. Skipping this configuration.\n",
      "------------------------------------------------------\n",
      "Configuration 10/162:\n",
      "LR: 0.0001, BS: 10, Optimizer: Adam, Dropout: 0.3, Kernel: 3\n",
      "256\n",
      "Epoch 1/10:\n",
      "Train Accuracy: 77.57% - Train Loss: 1.7098\n",
      "Test Accuracy: 83.94% - Test Loss: 1.6350\n",
      "----------------------------------------\n",
      "Epoch 2/10:\n",
      "Train Accuracy: 84.34% - Train Loss: 1.6296\n",
      "Test Accuracy: 85.87% - Test Loss: 1.6115\n",
      "----------------------------------------\n",
      "Epoch 3/10:\n",
      "Train Accuracy: 85.74% - Train Loss: 1.6126\n",
      "Test Accuracy: 86.66% - Test Loss: 1.6011\n",
      "----------------------------------------\n",
      "Epoch 4/10:\n",
      "Train Accuracy: 86.61% - Train Loss: 1.6017\n",
      "Test Accuracy: 87.29% - Test Loss: 1.5929\n",
      "----------------------------------------\n",
      "Epoch 5/10:\n",
      "Train Accuracy: 87.25% - Train Loss: 1.5948\n",
      "Test Accuracy: 87.59% - Test Loss: 1.5894\n",
      "----------------------------------------\n",
      "Epoch 6/10:\n",
      "Train Accuracy: 87.76% - Train Loss: 1.5885\n",
      "Test Accuracy: 87.76% - Test Loss: 1.5875\n",
      "----------------------------------------\n",
      "Epoch 7/10:\n",
      "Train Accuracy: 88.02% - Train Loss: 1.5851\n",
      "Test Accuracy: 88.24% - Test Loss: 1.5817\n",
      "----------------------------------------\n",
      "Epoch 8/10:\n",
      "Train Accuracy: 88.31% - Train Loss: 1.5816\n",
      "Test Accuracy: 88.54% - Test Loss: 1.5792\n",
      "----------------------------------------\n",
      "Epoch 9/10:\n",
      "Train Accuracy: 88.63% - Train Loss: 1.5785\n",
      "Test Accuracy: 88.51% - Test Loss: 1.5791\n",
      "----------------------------------------\n",
      "Epoch 10/10:\n",
      "Train Accuracy: 88.79% - Train Loss: 1.5767\n",
      "Test Accuracy: 88.91% - Test Loss: 1.5758\n",
      "----------------------------------------\n",
      "------------------------------------------------------\n",
      "Configuration 11/162:\n",
      "LR: 0.0001, BS: 10, Optimizer: Adam, Dropout: 0.3, Kernel: 4\n",
      "64\n",
      "Epoch 1/10:\n",
      "Train Accuracy: 75.69% - Train Loss: 1.7294\n",
      "Test Accuracy: 82.04% - Test Loss: 1.6515\n",
      "----------------------------------------\n",
      "Epoch 2/10:\n",
      "Train Accuracy: 81.68% - Train Loss: 1.6562\n",
      "Test Accuracy: 83.39% - Test Loss: 1.6337\n",
      "----------------------------------------\n",
      "Epoch 3/10:\n",
      "Train Accuracy: 83.17% - Train Loss: 1.6379\n",
      "Test Accuracy: 84.33% - Test Loss: 1.6228\n",
      "----------------------------------------\n",
      "Epoch 4/10:\n",
      "Train Accuracy: 84.11% - Train Loss: 1.6267\n",
      "Test Accuracy: 85.06% - Test Loss: 1.6143\n",
      "----------------------------------------\n",
      "Epoch 5/10:\n",
      "Train Accuracy: 84.67% - Train Loss: 1.6203\n",
      "Test Accuracy: 85.08% - Test Loss: 1.6132\n",
      "----------------------------------------\n",
      "Epoch 6/10:\n",
      "Train Accuracy: 85.34% - Train Loss: 1.6131\n",
      "Test Accuracy: 86.03% - Test Loss: 1.6060\n",
      "----------------------------------------\n",
      "Epoch 7/10:\n",
      "Train Accuracy: 85.85% - Train Loss: 1.6077\n",
      "Test Accuracy: 86.39% - Test Loss: 1.6007\n",
      "----------------------------------------\n",
      "Epoch 8/10:\n",
      "Train Accuracy: 86.32% - Train Loss: 1.6030\n",
      "Test Accuracy: 86.20% - Test Loss: 1.6024\n",
      "----------------------------------------\n",
      "Epoch 9/10:\n",
      "Train Accuracy: 86.44% - Train Loss: 1.6007\n",
      "Test Accuracy: 85.88% - Test Loss: 1.6038\n",
      "----------------------------------------\n",
      "Epoch 10/10:\n",
      "Train Accuracy: 86.95% - Train Loss: 1.5955\n",
      "Test Accuracy: 86.44% - Test Loss: 1.5987\n",
      "----------------------------------------\n",
      "------------------------------------------------------\n",
      "Configuration 12/162:\n",
      "LR: 0.0001, BS: 10, Optimizer: Adam, Dropout: 0.3, Kernel: 5\n",
      "64\n",
      "Epoch 1/10:\n",
      "Train Accuracy: 76.64% - Train Loss: 1.7184\n",
      "Test Accuracy: 80.82% - Test Loss: 1.6613\n",
      "----------------------------------------\n",
      "Epoch 2/10:\n",
      "Train Accuracy: 80.78% - Train Loss: 1.6629\n",
      "Test Accuracy: 82.23% - Test Loss: 1.6442\n",
      "----------------------------------------\n",
      "Epoch 3/10:\n",
      "Train Accuracy: 81.89% - Train Loss: 1.6487\n",
      "Test Accuracy: 82.63% - Test Loss: 1.6385\n",
      "----------------------------------------\n",
      "Epoch 4/10:\n",
      "Train Accuracy: 82.89% - Train Loss: 1.6386\n",
      "Test Accuracy: 83.65% - Test Loss: 1.6285\n",
      "----------------------------------------\n",
      "Epoch 5/10:\n",
      "Train Accuracy: 85.05% - Train Loss: 1.6172\n",
      "Test Accuracy: 86.08% - Test Loss: 1.6058\n",
      "----------------------------------------\n",
      "Epoch 6/10:\n",
      "Train Accuracy: 86.55% - Train Loss: 1.6020\n",
      "Test Accuracy: 87.10% - Test Loss: 1.5955\n",
      "----------------------------------------\n",
      "Epoch 7/10:\n",
      "Train Accuracy: 87.09% - Train Loss: 1.5957\n",
      "Test Accuracy: 87.26% - Test Loss: 1.5915\n",
      "----------------------------------------\n",
      "Epoch 8/10:\n",
      "Train Accuracy: 87.70% - Train Loss: 1.5890\n",
      "Test Accuracy: 87.47% - Test Loss: 1.5913\n",
      "----------------------------------------\n",
      "Epoch 9/10:\n",
      "Train Accuracy: 88.22% - Train Loss: 1.5842\n",
      "Test Accuracy: 88.00% - Test Loss: 1.5851\n",
      "----------------------------------------\n",
      "Epoch 10/10:\n",
      "Train Accuracy: 88.56% - Train Loss: 1.5804\n",
      "Test Accuracy: 88.02% - Test Loss: 1.5832\n",
      "----------------------------------------\n",
      "------------------------------------------------------\n",
      "Configuration 13/162:\n",
      "LR: 0.0001, BS: 10, Optimizer: Adam, Dropout: 0.5, Kernel: 3\n",
      "256\n",
      "Epoch 1/10:\n",
      "Train Accuracy: 71.97% - Train Loss: 1.7624\n",
      "Test Accuracy: 82.34% - Test Loss: 1.6489\n",
      "----------------------------------------\n",
      "Epoch 2/10:\n",
      "Train Accuracy: 82.01% - Train Loss: 1.6519\n",
      "Test Accuracy: 85.18% - Test Loss: 1.6185\n",
      "----------------------------------------\n",
      "Epoch 3/10:\n",
      "Train Accuracy: 83.98% - Train Loss: 1.6300\n",
      "Test Accuracy: 85.76% - Test Loss: 1.6082\n",
      "----------------------------------------\n",
      "Epoch 4/10:\n",
      "Train Accuracy: 84.83% - Train Loss: 1.6201\n",
      "Test Accuracy: 86.29% - Test Loss: 1.6025\n",
      "----------------------------------------\n",
      "Epoch 5/10:\n",
      "Train Accuracy: 85.49% - Train Loss: 1.6118\n",
      "Test Accuracy: 86.73% - Test Loss: 1.5970\n",
      "----------------------------------------\n",
      "Epoch 6/10:\n",
      "Train Accuracy: 85.96% - Train Loss: 1.6064\n",
      "Test Accuracy: 87.04% - Test Loss: 1.5941\n",
      "----------------------------------------\n",
      "Epoch 7/10:\n",
      "Train Accuracy: 86.26% - Train Loss: 1.6025\n",
      "Test Accuracy: 87.53% - Test Loss: 1.5899\n",
      "----------------------------------------\n",
      "Epoch 8/10:\n",
      "Train Accuracy: 86.41% - Train Loss: 1.5996\n",
      "Test Accuracy: 87.55% - Test Loss: 1.5871\n",
      "----------------------------------------\n",
      "Epoch 9/10:\n",
      "Train Accuracy: 86.88% - Train Loss: 1.5957\n",
      "Test Accuracy: 87.58% - Test Loss: 1.5869\n",
      "----------------------------------------\n",
      "Epoch 10/10:\n",
      "Train Accuracy: 87.18% - Train Loss: 1.5930\n",
      "Test Accuracy: 87.86% - Test Loss: 1.5832\n",
      "----------------------------------------\n",
      "------------------------------------------------------\n",
      "Configuration 14/162:\n",
      "LR: 0.0001, BS: 10, Optimizer: Adam, Dropout: 0.5, Kernel: 4\n",
      "64\n",
      "Epoch 1/10:\n",
      "Train Accuracy: 71.93% - Train Loss: 1.7696\n",
      "Test Accuracy: 80.01% - Test Loss: 1.6684\n",
      "----------------------------------------\n",
      "Epoch 2/10:\n",
      "Train Accuracy: 79.44% - Train Loss: 1.6808\n",
      "Test Accuracy: 82.32% - Test Loss: 1.6457\n",
      "----------------------------------------\n",
      "Epoch 3/10:\n",
      "Train Accuracy: 80.79% - Train Loss: 1.6628\n",
      "Test Accuracy: 82.86% - Test Loss: 1.6376\n",
      "----------------------------------------\n",
      "Epoch 4/10:\n",
      "Train Accuracy: 81.79% - Train Loss: 1.6515\n",
      "Test Accuracy: 83.73% - Test Loss: 1.6273\n",
      "----------------------------------------\n",
      "Epoch 5/10:\n",
      "Train Accuracy: 82.55% - Train Loss: 1.6423\n",
      "Test Accuracy: 83.82% - Test Loss: 1.6250\n",
      "----------------------------------------\n",
      "Epoch 6/10:\n",
      "Train Accuracy: 83.24% - Train Loss: 1.6350\n",
      "Test Accuracy: 84.45% - Test Loss: 1.6190\n",
      "----------------------------------------\n",
      "Epoch 7/10:\n",
      "Train Accuracy: 83.72% - Train Loss: 1.6299\n",
      "Test Accuracy: 85.10% - Test Loss: 1.6131\n",
      "----------------------------------------\n",
      "Epoch 8/10:\n",
      "Train Accuracy: 83.96% - Train Loss: 1.6260\n",
      "Test Accuracy: 84.85% - Test Loss: 1.6147\n",
      "----------------------------------------\n",
      "Epoch 9/10:\n",
      "Train Accuracy: 84.29% - Train Loss: 1.6228\n",
      "Test Accuracy: 85.61% - Test Loss: 1.6076\n",
      "----------------------------------------\n",
      "Epoch 10/10:\n",
      "Train Accuracy: 84.57% - Train Loss: 1.6195\n",
      "Test Accuracy: 85.71% - Test Loss: 1.6066\n",
      "----------------------------------------\n",
      "------------------------------------------------------\n",
      "Configuration 15/162:\n",
      "LR: 0.0001, BS: 10, Optimizer: Adam, Dropout: 0.5, Kernel: 5\n",
      "64\n",
      "Epoch 1/10:\n",
      "Train Accuracy: 74.44% - Train Loss: 1.7431\n",
      "Test Accuracy: 80.68% - Test Loss: 1.6631\n",
      "----------------------------------------\n",
      "Epoch 2/10:\n",
      "Train Accuracy: 80.25% - Train Loss: 1.6713\n",
      "Test Accuracy: 83.34% - Test Loss: 1.6345\n",
      "----------------------------------------\n",
      "Epoch 3/10:\n",
      "Train Accuracy: 82.33% - Train Loss: 1.6485\n",
      "Test Accuracy: 84.51% - Test Loss: 1.6226\n",
      "----------------------------------------\n",
      "Epoch 4/10:\n",
      "Train Accuracy: 83.42% - Train Loss: 1.6344\n",
      "Test Accuracy: 85.14% - Test Loss: 1.6145\n",
      "----------------------------------------\n",
      "Epoch 5/10:\n",
      "Train Accuracy: 84.28% - Train Loss: 1.6246\n",
      "Test Accuracy: 85.58% - Test Loss: 1.6080\n",
      "----------------------------------------\n",
      "Epoch 6/10:\n",
      "Train Accuracy: 84.93% - Train Loss: 1.6169\n",
      "Test Accuracy: 86.14% - Test Loss: 1.6039\n",
      "----------------------------------------\n",
      "Epoch 7/10:\n",
      "Train Accuracy: 85.52% - Train Loss: 1.6112\n",
      "Test Accuracy: 86.36% - Test Loss: 1.5989\n",
      "----------------------------------------\n",
      "Epoch 8/10:\n",
      "Train Accuracy: 86.11% - Train Loss: 1.6055\n",
      "Test Accuracy: 86.64% - Test Loss: 1.5963\n",
      "----------------------------------------\n",
      "Epoch 9/10:\n",
      "Train Accuracy: 86.24% - Train Loss: 1.6032\n",
      "Test Accuracy: 87.01% - Test Loss: 1.5945\n",
      "----------------------------------------\n",
      "Epoch 10/10:\n",
      "Train Accuracy: 86.66% - Train Loss: 1.5989\n",
      "Test Accuracy: 87.08% - Test Loss: 1.5915\n",
      "----------------------------------------\n",
      "------------------------------------------------------\n",
      "Configuration 16/162:\n",
      "LR: 0.0001, BS: 10, Optimizer: Adam, Dropout: 0.6, Kernel: 3\n",
      "256\n",
      "Epoch 1/10:\n",
      "Train Accuracy: 72.29% - Train Loss: 1.7604\n",
      "Test Accuracy: 81.68% - Test Loss: 1.6528\n",
      "----------------------------------------\n",
      "Epoch 2/10:\n",
      "Train Accuracy: 80.78% - Train Loss: 1.6645\n",
      "Test Accuracy: 84.47% - Test Loss: 1.6248\n",
      "----------------------------------------\n",
      "Epoch 3/10:\n",
      "Train Accuracy: 82.61% - Train Loss: 1.6426\n",
      "Test Accuracy: 85.00% - Test Loss: 1.6163\n",
      "----------------------------------------\n",
      "Epoch 4/10:\n",
      "Train Accuracy: 83.50% - Train Loss: 1.6330\n",
      "Test Accuracy: 85.83% - Test Loss: 1.6071\n",
      "----------------------------------------\n",
      "Epoch 5/10:\n",
      "Train Accuracy: 84.25% - Train Loss: 1.6250\n",
      "Test Accuracy: 86.30% - Test Loss: 1.6019\n",
      "----------------------------------------\n",
      "Epoch 6/10:\n",
      "Train Accuracy: 84.57% - Train Loss: 1.6204\n",
      "Test Accuracy: 86.56% - Test Loss: 1.5994\n",
      "----------------------------------------\n",
      "Epoch 7/10:\n",
      "Train Accuracy: 85.05% - Train Loss: 1.6155\n",
      "Test Accuracy: 86.67% - Test Loss: 1.5975\n",
      "----------------------------------------\n",
      "Epoch 8/10:\n",
      "Train Accuracy: 85.40% - Train Loss: 1.6109\n",
      "Test Accuracy: 87.09% - Test Loss: 1.5933\n",
      "----------------------------------------\n",
      "Epoch 9/10:\n",
      "Train Accuracy: 85.60% - Train Loss: 1.6085\n",
      "Test Accuracy: 87.42% - Test Loss: 1.5901\n",
      "----------------------------------------\n",
      "Epoch 10/10:\n",
      "Train Accuracy: 85.77% - Train Loss: 1.6057\n",
      "Test Accuracy: 87.70% - Test Loss: 1.5880\n",
      "----------------------------------------\n",
      "------------------------------------------------------\n",
      "Configuration 17/162:\n",
      "LR: 0.0001, BS: 10, Optimizer: Adam, Dropout: 0.6, Kernel: 4\n",
      "64\n",
      "Train accuracy is less than 70%. Skipping this configuration.\n",
      "------------------------------------------------------\n",
      "Configuration 18/162:\n",
      "LR: 0.0001, BS: 10, Optimizer: Adam, Dropout: 0.6, Kernel: 5\n",
      "64\n",
      "Epoch 1/10:\n",
      "Train Accuracy: 71.42% - Train Loss: 1.7759\n",
      "Test Accuracy: 80.28% - Test Loss: 1.6670\n",
      "----------------------------------------\n",
      "Epoch 2/10:\n",
      "Train Accuracy: 79.04% - Train Loss: 1.6859\n",
      "Test Accuracy: 82.43% - Test Loss: 1.6411\n",
      "----------------------------------------\n",
      "Epoch 3/10:\n",
      "Train Accuracy: 80.89% - Train Loss: 1.6626\n",
      "Test Accuracy: 83.74% - Test Loss: 1.6268\n",
      "----------------------------------------\n",
      "Epoch 4/10:\n",
      "Train Accuracy: 82.31% - Train Loss: 1.6474\n",
      "Test Accuracy: 84.63% - Test Loss: 1.6189\n",
      "----------------------------------------\n",
      "Epoch 5/10:\n",
      "Train Accuracy: 83.23% - Train Loss: 1.6363\n",
      "Test Accuracy: 85.04% - Test Loss: 1.6136\n",
      "----------------------------------------\n",
      "Epoch 6/10:\n",
      "Train Accuracy: 83.79% - Train Loss: 1.6298\n",
      "Test Accuracy: 85.34% - Test Loss: 1.6086\n",
      "----------------------------------------\n",
      "Epoch 7/10:\n",
      "Train Accuracy: 84.57% - Train Loss: 1.6217\n",
      "Test Accuracy: 85.80% - Test Loss: 1.6040\n",
      "----------------------------------------\n",
      "Epoch 8/10:\n",
      "Train Accuracy: 85.03% - Train Loss: 1.6165\n",
      "Test Accuracy: 86.76% - Test Loss: 1.5971\n",
      "----------------------------------------\n",
      "Epoch 9/10:\n",
      "Train Accuracy: 85.43% - Train Loss: 1.6130\n",
      "Test Accuracy: 86.52% - Test Loss: 1.5980\n",
      "----------------------------------------\n",
      "Epoch 10/10:\n",
      "Train Accuracy: 85.75% - Train Loss: 1.6094\n",
      "Test Accuracy: 86.75% - Test Loss: 1.5942\n",
      "----------------------------------------\n",
      "------------------------------------------------------\n",
      "Configuration 19/162:\n",
      "LR: 0.0001, BS: 32, Optimizer: SGD, Dropout: 0.3, Kernel: 3\n",
      "256\n",
      "Train accuracy is less than 70%. Skipping this configuration.\n",
      "------------------------------------------------------\n",
      "Configuration 20/162:\n",
      "LR: 0.0001, BS: 32, Optimizer: SGD, Dropout: 0.3, Kernel: 4\n",
      "64\n",
      "Train accuracy is less than 70%. Skipping this configuration.\n",
      "------------------------------------------------------\n",
      "Configuration 21/162:\n",
      "LR: 0.0001, BS: 32, Optimizer: SGD, Dropout: 0.3, Kernel: 5\n",
      "64\n",
      "Train accuracy is less than 70%. Skipping this configuration.\n",
      "------------------------------------------------------\n",
      "Configuration 22/162:\n",
      "LR: 0.0001, BS: 32, Optimizer: SGD, Dropout: 0.5, Kernel: 3\n",
      "256\n",
      "Train accuracy is less than 70%. Skipping this configuration.\n",
      "------------------------------------------------------\n",
      "Configuration 23/162:\n",
      "LR: 0.0001, BS: 32, Optimizer: SGD, Dropout: 0.5, Kernel: 4\n",
      "64\n",
      "Train accuracy is less than 70%. Skipping this configuration.\n",
      "------------------------------------------------------\n",
      "Configuration 24/162:\n",
      "LR: 0.0001, BS: 32, Optimizer: SGD, Dropout: 0.5, Kernel: 5\n",
      "64\n",
      "Train accuracy is less than 70%. Skipping this configuration.\n",
      "------------------------------------------------------\n",
      "Configuration 25/162:\n",
      "LR: 0.0001, BS: 32, Optimizer: SGD, Dropout: 0.6, Kernel: 3\n",
      "256\n",
      "Train accuracy is less than 70%. Skipping this configuration.\n",
      "------------------------------------------------------\n",
      "Configuration 26/162:\n",
      "LR: 0.0001, BS: 32, Optimizer: SGD, Dropout: 0.6, Kernel: 4\n",
      "64\n",
      "Train accuracy is less than 70%. Skipping this configuration.\n",
      "------------------------------------------------------\n",
      "Configuration 27/162:\n",
      "LR: 0.0001, BS: 32, Optimizer: SGD, Dropout: 0.6, Kernel: 5\n",
      "64\n",
      "Train accuracy is less than 70%. Skipping this configuration.\n",
      "------------------------------------------------------\n",
      "Configuration 28/162:\n",
      "LR: 0.0001, BS: 32, Optimizer: Adam, Dropout: 0.3, Kernel: 3\n",
      "256\n",
      "Epoch 1/10:\n",
      "Train Accuracy: 72.30% - Train Loss: 1.7661\n",
      "Test Accuracy: 80.41% - Test Loss: 1.6723\n",
      "----------------------------------------\n",
      "Epoch 2/10:\n",
      "Train Accuracy: 81.90% - Train Loss: 1.6577\n",
      "Test Accuracy: 84.42% - Test Loss: 1.6277\n",
      "----------------------------------------\n",
      "Epoch 3/10:\n",
      "Train Accuracy: 84.83% - Train Loss: 1.6252\n",
      "Test Accuracy: 85.84% - Test Loss: 1.6114\n",
      "----------------------------------------\n",
      "Epoch 4/10:\n",
      "Train Accuracy: 85.97% - Train Loss: 1.6112\n",
      "Test Accuracy: 86.89% - Test Loss: 1.6011\n",
      "----------------------------------------\n",
      "Epoch 5/10:\n",
      "Train Accuracy: 86.73% - Train Loss: 1.6025\n",
      "Test Accuracy: 87.24% - Test Loss: 1.5952\n",
      "----------------------------------------\n",
      "Epoch 6/10:\n",
      "Train Accuracy: 87.30% - Train Loss: 1.5950\n",
      "Test Accuracy: 87.65% - Test Loss: 1.5911\n",
      "----------------------------------------\n",
      "Epoch 7/10:\n",
      "Train Accuracy: 87.69% - Train Loss: 1.5908\n",
      "Test Accuracy: 87.96% - Test Loss: 1.5877\n",
      "----------------------------------------\n",
      "Epoch 8/10:\n",
      "Train Accuracy: 88.15% - Train Loss: 1.5860\n",
      "Test Accuracy: 87.90% - Test Loss: 1.5880\n",
      "----------------------------------------\n",
      "Epoch 9/10:\n",
      "Train Accuracy: 88.44% - Train Loss: 1.5830\n",
      "Test Accuracy: 87.92% - Test Loss: 1.5851\n",
      "----------------------------------------\n",
      "Epoch 10/10:\n",
      "Train Accuracy: 88.51% - Train Loss: 1.5804\n",
      "Test Accuracy: 87.99% - Test Loss: 1.5838\n",
      "----------------------------------------\n",
      "------------------------------------------------------\n",
      "Configuration 29/162:\n",
      "LR: 0.0001, BS: 32, Optimizer: Adam, Dropout: 0.3, Kernel: 4\n",
      "64\n",
      "Epoch 1/10:\n",
      "Train Accuracy: 74.08% - Train Loss: 1.7513\n",
      "Test Accuracy: 80.73% - Test Loss: 1.6696\n",
      "----------------------------------------\n",
      "Epoch 2/10:\n",
      "Train Accuracy: 80.41% - Train Loss: 1.6709\n",
      "Test Accuracy: 82.04% - Test Loss: 1.6483\n",
      "----------------------------------------\n",
      "Epoch 3/10:\n",
      "Train Accuracy: 82.19% - Train Loss: 1.6506\n",
      "Test Accuracy: 83.33% - Test Loss: 1.6358\n",
      "----------------------------------------\n",
      "Epoch 4/10:\n",
      "Train Accuracy: 83.03% - Train Loss: 1.6402\n",
      "Test Accuracy: 83.96% - Test Loss: 1.6294\n",
      "----------------------------------------\n",
      "Epoch 5/10:\n",
      "Train Accuracy: 83.93% - Train Loss: 1.6303\n",
      "Test Accuracy: 84.39% - Test Loss: 1.6213\n",
      "----------------------------------------\n",
      "Epoch 6/10:\n",
      "Train Accuracy: 84.38% - Train Loss: 1.6246\n",
      "Test Accuracy: 84.60% - Test Loss: 1.6196\n",
      "----------------------------------------\n",
      "Epoch 7/10:\n",
      "Train Accuracy: 84.98% - Train Loss: 1.6182\n",
      "Test Accuracy: 85.23% - Test Loss: 1.6145\n",
      "----------------------------------------\n",
      "Epoch 8/10:\n",
      "Train Accuracy: 85.37% - Train Loss: 1.6143\n",
      "Test Accuracy: 85.15% - Test Loss: 1.6116\n",
      "----------------------------------------\n",
      "Epoch 9/10:\n",
      "Train Accuracy: 85.82% - Train Loss: 1.6093\n",
      "Test Accuracy: 85.67% - Test Loss: 1.6090\n",
      "----------------------------------------\n",
      "Epoch 10/10:\n",
      "Train Accuracy: 86.17% - Train Loss: 1.6056\n",
      "Test Accuracy: 85.61% - Test Loss: 1.6076\n",
      "----------------------------------------\n",
      "------------------------------------------------------\n",
      "Configuration 30/162:\n",
      "LR: 0.0001, BS: 32, Optimizer: Adam, Dropout: 0.3, Kernel: 5\n",
      "64\n",
      "Epoch 1/10:\n",
      "Train Accuracy: 76.90% - Train Loss: 1.7247\n",
      "Test Accuracy: 83.17% - Test Loss: 1.6457\n",
      "----------------------------------------\n",
      "Epoch 2/10:\n",
      "Train Accuracy: 83.23% - Train Loss: 1.6452\n",
      "Test Accuracy: 84.96% - Test Loss: 1.6236\n",
      "----------------------------------------\n",
      "Epoch 3/10:\n",
      "Train Accuracy: 85.06% - Train Loss: 1.6225\n",
      "Test Accuracy: 86.11% - Test Loss: 1.6072\n",
      "----------------------------------------\n",
      "Epoch 4/10:\n",
      "Train Accuracy: 86.10% - Train Loss: 1.6110\n",
      "Test Accuracy: 86.51% - Test Loss: 1.6013\n",
      "----------------------------------------\n",
      "Epoch 5/10:\n",
      "Train Accuracy: 86.85% - Train Loss: 1.6012\n",
      "Test Accuracy: 87.15% - Test Loss: 1.5965\n",
      "----------------------------------------\n",
      "Epoch 6/10:\n",
      "Train Accuracy: 87.55% - Train Loss: 1.5943\n",
      "Test Accuracy: 87.41% - Test Loss: 1.5927\n",
      "----------------------------------------\n",
      "Epoch 7/10:\n",
      "Train Accuracy: 88.00% - Train Loss: 1.5892\n",
      "Test Accuracy: 87.70% - Test Loss: 1.5884\n",
      "----------------------------------------\n",
      "Epoch 8/10:\n",
      "Train Accuracy: 88.17% - Train Loss: 1.5857\n",
      "Test Accuracy: 88.01% - Test Loss: 1.5863\n",
      "----------------------------------------\n",
      "Epoch 9/10:\n",
      "Train Accuracy: 88.74% - Train Loss: 1.5804\n",
      "Test Accuracy: 88.13% - Test Loss: 1.5834\n",
      "----------------------------------------\n",
      "Epoch 10/10:\n",
      "Train Accuracy: 89.02% - Train Loss: 1.5766\n",
      "Test Accuracy: 88.04% - Test Loss: 1.5830\n",
      "----------------------------------------\n",
      "------------------------------------------------------\n",
      "Configuration 31/162:\n",
      "LR: 0.0001, BS: 32, Optimizer: Adam, Dropout: 0.5, Kernel: 3\n",
      "256\n",
      "Epoch 1/10:\n",
      "Train Accuracy: 70.32% - Train Loss: 1.7866\n",
      "Test Accuracy: 80.11% - Test Loss: 1.6740\n",
      "----------------------------------------\n",
      "Epoch 2/10:\n",
      "Train Accuracy: 80.09% - Train Loss: 1.6743\n",
      "Test Accuracy: 83.61% - Test Loss: 1.6373\n",
      "----------------------------------------\n",
      "Epoch 3/10:\n",
      "Train Accuracy: 82.43% - Train Loss: 1.6480\n",
      "Test Accuracy: 84.84% - Test Loss: 1.6210\n",
      "----------------------------------------\n",
      "Epoch 4/10:\n",
      "Train Accuracy: 83.73% - Train Loss: 1.6331\n",
      "Test Accuracy: 85.41% - Test Loss: 1.6138\n",
      "----------------------------------------\n",
      "Epoch 5/10:\n",
      "Train Accuracy: 84.53% - Train Loss: 1.6234\n",
      "Test Accuracy: 85.95% - Test Loss: 1.6071\n",
      "----------------------------------------\n",
      "Epoch 6/10:\n",
      "Train Accuracy: 85.12% - Train Loss: 1.6177\n",
      "Test Accuracy: 86.29% - Test Loss: 1.6019\n",
      "----------------------------------------\n",
      "Epoch 7/10:\n",
      "Train Accuracy: 85.52% - Train Loss: 1.6127\n",
      "Test Accuracy: 86.54% - Test Loss: 1.6006\n",
      "----------------------------------------\n",
      "Epoch 8/10:\n",
      "Train Accuracy: 85.89% - Train Loss: 1.6073\n",
      "Test Accuracy: 87.03% - Test Loss: 1.5946\n",
      "----------------------------------------\n",
      "Epoch 9/10:\n",
      "Train Accuracy: 86.37% - Train Loss: 1.6037\n",
      "Test Accuracy: 87.48% - Test Loss: 1.5907\n",
      "----------------------------------------\n",
      "Epoch 10/10:\n",
      "Train Accuracy: 86.71% - Train Loss: 1.5994\n",
      "Test Accuracy: 87.67% - Test Loss: 1.5885\n",
      "----------------------------------------\n",
      "------------------------------------------------------\n",
      "Configuration 32/162:\n",
      "LR: 0.0001, BS: 32, Optimizer: Adam, Dropout: 0.5, Kernel: 4\n",
      "64\n",
      "Epoch 1/10:\n",
      "Train Accuracy: 71.37% - Train Loss: 1.7812\n",
      "Test Accuracy: 79.86% - Test Loss: 1.6754\n",
      "----------------------------------------\n",
      "Epoch 2/10:\n",
      "Train Accuracy: 78.75% - Train Loss: 1.6908\n",
      "Test Accuracy: 81.56% - Test Loss: 1.6533\n",
      "----------------------------------------\n",
      "Epoch 3/10:\n",
      "Train Accuracy: 80.39% - Train Loss: 1.6702\n",
      "Test Accuracy: 82.38% - Test Loss: 1.6419\n",
      "----------------------------------------\n",
      "Epoch 4/10:\n",
      "Train Accuracy: 81.45% - Train Loss: 1.6572\n",
      "Test Accuracy: 83.20% - Test Loss: 1.6340\n",
      "----------------------------------------\n",
      "Epoch 5/10:\n",
      "Train Accuracy: 82.02% - Train Loss: 1.6494\n",
      "Test Accuracy: 83.92% - Test Loss: 1.6269\n",
      "----------------------------------------\n",
      "Epoch 6/10:\n",
      "Train Accuracy: 82.77% - Train Loss: 1.6419\n",
      "Test Accuracy: 84.04% - Test Loss: 1.6235\n",
      "----------------------------------------\n",
      "Epoch 7/10:\n",
      "Train Accuracy: 83.19% - Train Loss: 1.6367\n",
      "Test Accuracy: 84.51% - Test Loss: 1.6192\n",
      "----------------------------------------\n",
      "Epoch 8/10:\n",
      "Train Accuracy: 83.70% - Train Loss: 1.6322\n",
      "Test Accuracy: 84.72% - Test Loss: 1.6182\n",
      "----------------------------------------\n",
      "Epoch 9/10:\n",
      "Train Accuracy: 84.08% - Train Loss: 1.6275\n",
      "Test Accuracy: 84.93% - Test Loss: 1.6150\n",
      "----------------------------------------\n",
      "Epoch 10/10:\n",
      "Train Accuracy: 84.36% - Train Loss: 1.6236\n",
      "Test Accuracy: 85.05% - Test Loss: 1.6136\n",
      "----------------------------------------\n",
      "------------------------------------------------------\n",
      "Configuration 33/162:\n",
      "LR: 0.0001, BS: 32, Optimizer: Adam, Dropout: 0.5, Kernel: 5\n",
      "64\n",
      "Epoch 1/10:\n",
      "Train Accuracy: 73.42% - Train Loss: 1.7589\n",
      "Test Accuracy: 81.04% - Test Loss: 1.6650\n",
      "----------------------------------------\n",
      "Epoch 2/10:\n",
      "Train Accuracy: 80.51% - Train Loss: 1.6749\n",
      "Test Accuracy: 83.08% - Test Loss: 1.6387\n",
      "----------------------------------------\n",
      "Epoch 3/10:\n",
      "Train Accuracy: 82.45% - Train Loss: 1.6505\n",
      "Test Accuracy: 84.47% - Test Loss: 1.6240\n",
      "----------------------------------------\n",
      "Epoch 4/10:\n",
      "Train Accuracy: 83.46% - Train Loss: 1.6376\n",
      "Test Accuracy: 85.11% - Test Loss: 1.6158\n",
      "----------------------------------------\n",
      "Epoch 5/10:\n",
      "Train Accuracy: 84.29% - Train Loss: 1.6276\n",
      "Test Accuracy: 85.53% - Test Loss: 1.6109\n",
      "----------------------------------------\n",
      "Epoch 6/10:\n",
      "Train Accuracy: 85.11% - Train Loss: 1.6191\n",
      "Test Accuracy: 85.94% - Test Loss: 1.6049\n",
      "----------------------------------------\n",
      "Epoch 7/10:\n",
      "Train Accuracy: 85.51% - Train Loss: 1.6140\n",
      "Test Accuracy: 86.40% - Test Loss: 1.6005\n",
      "----------------------------------------\n",
      "Epoch 8/10:\n",
      "Train Accuracy: 86.08% - Train Loss: 1.6079\n",
      "Test Accuracy: 86.72% - Test Loss: 1.5976\n",
      "----------------------------------------\n",
      "Epoch 9/10:\n",
      "Train Accuracy: 86.45% - Train Loss: 1.6036\n",
      "Test Accuracy: 87.03% - Test Loss: 1.5944\n",
      "----------------------------------------\n",
      "Epoch 10/10:\n",
      "Train Accuracy: 86.83% - Train Loss: 1.5994\n",
      "Test Accuracy: 87.35% - Test Loss: 1.5910\n",
      "----------------------------------------\n",
      "------------------------------------------------------\n",
      "Configuration 34/162:\n",
      "LR: 0.0001, BS: 32, Optimizer: Adam, Dropout: 0.6, Kernel: 3\n",
      "256\n",
      "Train accuracy is less than 70%. Skipping this configuration.\n",
      "------------------------------------------------------\n",
      "Configuration 35/162:\n",
      "LR: 0.0001, BS: 32, Optimizer: Adam, Dropout: 0.6, Kernel: 4\n",
      "64\n",
      "Train accuracy is less than 70%. Skipping this configuration.\n",
      "------------------------------------------------------\n",
      "Configuration 36/162:\n",
      "LR: 0.0001, BS: 32, Optimizer: Adam, Dropout: 0.6, Kernel: 5\n",
      "64\n",
      "Epoch 1/10:\n",
      "Train Accuracy: 70.22% - Train Loss: 1.7921\n",
      "Test Accuracy: 79.90% - Test Loss: 1.6759\n",
      "----------------------------------------\n",
      "Epoch 2/10:\n",
      "Train Accuracy: 78.30% - Train Loss: 1.6990\n",
      "Test Accuracy: 81.81% - Test Loss: 1.6521\n",
      "----------------------------------------\n",
      "Epoch 3/10:\n",
      "Train Accuracy: 80.27% - Train Loss: 1.6732\n",
      "Test Accuracy: 83.63% - Test Loss: 1.6319\n",
      "----------------------------------------\n",
      "Epoch 4/10:\n",
      "Train Accuracy: 81.44% - Train Loss: 1.6585\n",
      "Test Accuracy: 84.00% - Test Loss: 1.6254\n",
      "----------------------------------------\n",
      "Epoch 5/10:\n",
      "Train Accuracy: 82.39% - Train Loss: 1.6483\n",
      "Test Accuracy: 84.77% - Test Loss: 1.6184\n",
      "----------------------------------------\n",
      "Epoch 6/10:\n",
      "Train Accuracy: 83.13% - Train Loss: 1.6398\n",
      "Test Accuracy: 84.99% - Test Loss: 1.6145\n",
      "----------------------------------------\n",
      "Epoch 7/10:\n",
      "Train Accuracy: 83.94% - Train Loss: 1.6311\n",
      "Test Accuracy: 85.58% - Test Loss: 1.6086\n",
      "----------------------------------------\n",
      "Epoch 8/10:\n",
      "Train Accuracy: 84.39% - Train Loss: 1.6263\n",
      "Test Accuracy: 86.21% - Test Loss: 1.6023\n",
      "----------------------------------------\n",
      "Epoch 9/10:\n",
      "Train Accuracy: 84.91% - Train Loss: 1.6205\n",
      "Test Accuracy: 86.18% - Test Loss: 1.6012\n",
      "----------------------------------------\n",
      "Epoch 10/10:\n",
      "Train Accuracy: 85.15% - Train Loss: 1.6175\n",
      "Test Accuracy: 86.45% - Test Loss: 1.5992\n",
      "----------------------------------------\n",
      "------------------------------------------------------\n",
      "Configuration 37/162:\n",
      "LR: 0.0001, BS: 64, Optimizer: SGD, Dropout: 0.3, Kernel: 3\n",
      "256\n",
      "Train accuracy is less than 70%. Skipping this configuration.\n",
      "------------------------------------------------------\n",
      "Configuration 38/162:\n",
      "LR: 0.0001, BS: 64, Optimizer: SGD, Dropout: 0.3, Kernel: 4\n",
      "64\n",
      "Train accuracy is less than 70%. Skipping this configuration.\n",
      "------------------------------------------------------\n",
      "Configuration 39/162:\n",
      "LR: 0.0001, BS: 64, Optimizer: SGD, Dropout: 0.3, Kernel: 5\n",
      "64\n",
      "Train accuracy is less than 70%. Skipping this configuration.\n",
      "------------------------------------------------------\n",
      "Configuration 40/162:\n",
      "LR: 0.0001, BS: 64, Optimizer: SGD, Dropout: 0.5, Kernel: 3\n",
      "256\n",
      "Train accuracy is less than 70%. Skipping this configuration.\n",
      "------------------------------------------------------\n",
      "Configuration 41/162:\n",
      "LR: 0.0001, BS: 64, Optimizer: SGD, Dropout: 0.5, Kernel: 4\n",
      "64\n",
      "Train accuracy is less than 70%. Skipping this configuration.\n",
      "------------------------------------------------------\n",
      "Configuration 42/162:\n",
      "LR: 0.0001, BS: 64, Optimizer: SGD, Dropout: 0.5, Kernel: 5\n",
      "64\n",
      "Train accuracy is less than 70%. Skipping this configuration.\n",
      "------------------------------------------------------\n",
      "Configuration 43/162:\n",
      "LR: 0.0001, BS: 64, Optimizer: SGD, Dropout: 0.6, Kernel: 3\n",
      "256\n",
      "Train accuracy is less than 70%. Skipping this configuration.\n",
      "------------------------------------------------------\n",
      "Configuration 44/162:\n",
      "LR: 0.0001, BS: 64, Optimizer: SGD, Dropout: 0.6, Kernel: 4\n",
      "64\n",
      "Train accuracy is less than 70%. Skipping this configuration.\n",
      "------------------------------------------------------\n",
      "Configuration 45/162:\n",
      "LR: 0.0001, BS: 64, Optimizer: SGD, Dropout: 0.6, Kernel: 5\n",
      "64\n",
      "Train accuracy is less than 70%. Skipping this configuration.\n",
      "------------------------------------------------------\n",
      "Configuration 46/162:\n",
      "LR: 0.0001, BS: 64, Optimizer: Adam, Dropout: 0.3, Kernel: 3\n",
      "256\n",
      "Epoch 1/10:\n",
      "Train Accuracy: 71.38% - Train Loss: 1.7813\n",
      "Test Accuracy: 81.07% - Test Loss: 1.6700\n",
      "----------------------------------------\n",
      "Epoch 2/10:\n",
      "Train Accuracy: 82.22% - Train Loss: 1.6594\n",
      "Test Accuracy: 84.14% - Test Loss: 1.6352\n",
      "----------------------------------------\n",
      "Epoch 3/10:\n",
      "Train Accuracy: 84.36% - Train Loss: 1.6325\n",
      "Test Accuracy: 85.50% - Test Loss: 1.6186\n",
      "----------------------------------------\n",
      "Epoch 4/10:\n",
      "Train Accuracy: 85.42% - Train Loss: 1.6188\n",
      "Test Accuracy: 85.83% - Test Loss: 1.6115\n",
      "----------------------------------------\n",
      "Epoch 5/10:\n",
      "Train Accuracy: 86.27% - Train Loss: 1.6094\n",
      "Test Accuracy: 86.57% - Test Loss: 1.6044\n",
      "----------------------------------------\n",
      "Epoch 6/10:\n",
      "Train Accuracy: 86.73% - Train Loss: 1.6030\n",
      "Test Accuracy: 86.79% - Test Loss: 1.5999\n",
      "----------------------------------------\n",
      "Epoch 7/10:\n",
      "Train Accuracy: 87.14% - Train Loss: 1.5980\n",
      "Test Accuracy: 87.37% - Test Loss: 1.5944\n",
      "----------------------------------------\n",
      "Epoch 8/10:\n",
      "Train Accuracy: 87.29% - Train Loss: 1.5950\n",
      "Test Accuracy: 87.53% - Test Loss: 1.5909\n",
      "----------------------------------------\n",
      "Epoch 9/10:\n",
      "Train Accuracy: 87.83% - Train Loss: 1.5904\n",
      "Test Accuracy: 87.55% - Test Loss: 1.5900\n",
      "----------------------------------------\n",
      "Epoch 10/10:\n",
      "Train Accuracy: 88.08% - Train Loss: 1.5868\n",
      "Test Accuracy: 87.84% - Test Loss: 1.5873\n",
      "----------------------------------------\n",
      "------------------------------------------------------\n",
      "Configuration 47/162:\n",
      "LR: 0.0001, BS: 64, Optimizer: Adam, Dropout: 0.3, Kernel: 4\n",
      "64\n",
      "Epoch 1/10:\n",
      "Train Accuracy: 71.20% - Train Loss: 1.7819\n",
      "Test Accuracy: 77.84% - Test Loss: 1.6963\n",
      "----------------------------------------\n",
      "Epoch 2/10:\n",
      "Train Accuracy: 78.19% - Train Loss: 1.6942\n",
      "Test Accuracy: 79.66% - Test Loss: 1.6730\n",
      "----------------------------------------\n",
      "Epoch 3/10:\n",
      "Train Accuracy: 80.45% - Train Loss: 1.6693\n",
      "Test Accuracy: 81.76% - Test Loss: 1.6520\n",
      "----------------------------------------\n",
      "Epoch 4/10:\n",
      "Train Accuracy: 82.08% - Train Loss: 1.6511\n",
      "Test Accuracy: 82.77% - Test Loss: 1.6405\n",
      "----------------------------------------\n",
      "Epoch 5/10:\n",
      "Train Accuracy: 83.05% - Train Loss: 1.6403\n",
      "Test Accuracy: 83.47% - Test Loss: 1.6322\n",
      "----------------------------------------\n",
      "Epoch 6/10:\n",
      "Train Accuracy: 83.76% - Train Loss: 1.6319\n",
      "Test Accuracy: 84.28% - Test Loss: 1.6250\n",
      "----------------------------------------\n",
      "Epoch 7/10:\n",
      "Train Accuracy: 84.57% - Train Loss: 1.6235\n",
      "Test Accuracy: 84.68% - Test Loss: 1.6200\n",
      "----------------------------------------\n",
      "Epoch 8/10:\n",
      "Train Accuracy: 84.89% - Train Loss: 1.6195\n",
      "Test Accuracy: 85.15% - Test Loss: 1.6152\n",
      "----------------------------------------\n",
      "Epoch 9/10:\n",
      "Train Accuracy: 85.39% - Train Loss: 1.6150\n",
      "Test Accuracy: 85.44% - Test Loss: 1.6130\n",
      "----------------------------------------\n",
      "Epoch 10/10:\n",
      "Train Accuracy: 85.73% - Train Loss: 1.6105\n",
      "Test Accuracy: 85.35% - Test Loss: 1.6113\n",
      "----------------------------------------\n",
      "------------------------------------------------------\n",
      "Configuration 48/162:\n",
      "LR: 0.0001, BS: 64, Optimizer: Adam, Dropout: 0.3, Kernel: 5\n",
      "64\n",
      "Epoch 1/10:\n",
      "Train Accuracy: 74.62% - Train Loss: 1.7491\n",
      "Test Accuracy: 81.43% - Test Loss: 1.6638\n",
      "----------------------------------------\n",
      "Epoch 2/10:\n",
      "Train Accuracy: 81.86% - Train Loss: 1.6597\n",
      "Test Accuracy: 83.54% - Test Loss: 1.6368\n",
      "----------------------------------------\n",
      "Epoch 3/10:\n",
      "Train Accuracy: 83.90% - Train Loss: 1.6368\n",
      "Test Accuracy: 84.79% - Test Loss: 1.6225\n",
      "----------------------------------------\n",
      "Epoch 4/10:\n",
      "Train Accuracy: 85.13% - Train Loss: 1.6216\n",
      "Test Accuracy: 85.66% - Test Loss: 1.6129\n",
      "----------------------------------------\n",
      "Epoch 5/10:\n",
      "Train Accuracy: 86.12% - Train Loss: 1.6111\n",
      "Test Accuracy: 85.93% - Test Loss: 1.6074\n",
      "----------------------------------------\n",
      "Epoch 6/10:\n",
      "Train Accuracy: 86.86% - Train Loss: 1.6025\n",
      "Test Accuracy: 86.74% - Test Loss: 1.6000\n",
      "----------------------------------------\n",
      "Epoch 7/10:\n",
      "Train Accuracy: 87.53% - Train Loss: 1.5955\n",
      "Test Accuracy: 86.97% - Test Loss: 1.5973\n",
      "----------------------------------------\n",
      "Epoch 8/10:\n",
      "Train Accuracy: 88.02% - Train Loss: 1.5897\n",
      "Test Accuracy: 87.38% - Test Loss: 1.5924\n",
      "----------------------------------------\n",
      "Epoch 9/10:\n",
      "Train Accuracy: 88.40% - Train Loss: 1.5850\n",
      "Test Accuracy: 87.59% - Test Loss: 1.5898\n",
      "----------------------------------------\n",
      "Epoch 10/10:\n",
      "Train Accuracy: 88.59% - Train Loss: 1.5827\n",
      "Test Accuracy: 87.82% - Test Loss: 1.5879\n",
      "----------------------------------------\n",
      "------------------------------------------------------\n",
      "Configuration 49/162:\n",
      "LR: 0.0001, BS: 64, Optimizer: Adam, Dropout: 0.5, Kernel: 3\n",
      "256\n",
      "Train accuracy is less than 70%. Skipping this configuration.\n",
      "------------------------------------------------------\n",
      "Configuration 50/162:\n",
      "LR: 0.0001, BS: 64, Optimizer: Adam, Dropout: 0.5, Kernel: 4\n",
      "64\n",
      "Train accuracy is less than 70%. Skipping this configuration.\n",
      "------------------------------------------------------\n",
      "Configuration 51/162:\n",
      "LR: 0.0001, BS: 64, Optimizer: Adam, Dropout: 0.5, Kernel: 5\n",
      "64\n",
      "Epoch 1/10:\n",
      "Train Accuracy: 71.38% - Train Loss: 1.7848\n",
      "Test Accuracy: 80.55% - Test Loss: 1.6744\n",
      "----------------------------------------\n",
      "Epoch 2/10:\n",
      "Train Accuracy: 80.14% - Train Loss: 1.6831\n",
      "Test Accuracy: 82.76% - Test Loss: 1.6432\n",
      "----------------------------------------\n",
      "Epoch 3/10:\n",
      "Train Accuracy: 81.94% - Train Loss: 1.6581\n",
      "Test Accuracy: 84.03% - Test Loss: 1.6288\n",
      "----------------------------------------\n",
      "Epoch 4/10:\n",
      "Train Accuracy: 83.44% - Train Loss: 1.6419\n",
      "Test Accuracy: 84.91% - Test Loss: 1.6195\n",
      "----------------------------------------\n",
      "Epoch 5/10:\n",
      "Train Accuracy: 84.10% - Train Loss: 1.6325\n",
      "Test Accuracy: 85.33% - Test Loss: 1.6134\n",
      "----------------------------------------\n",
      "Epoch 6/10:\n",
      "Train Accuracy: 84.82% - Train Loss: 1.6246\n",
      "Test Accuracy: 86.07% - Test Loss: 1.6079\n",
      "----------------------------------------\n",
      "Epoch 7/10:\n",
      "Train Accuracy: 85.42% - Train Loss: 1.6178\n",
      "Test Accuracy: 86.23% - Test Loss: 1.6049\n",
      "----------------------------------------\n",
      "Epoch 8/10:\n",
      "Train Accuracy: 85.86% - Train Loss: 1.6117\n",
      "Test Accuracy: 86.76% - Test Loss: 1.5991\n",
      "----------------------------------------\n",
      "Epoch 9/10:\n",
      "Train Accuracy: 86.21% - Train Loss: 1.6080\n",
      "Test Accuracy: 86.62% - Test Loss: 1.5983\n",
      "----------------------------------------\n",
      "Epoch 10/10:\n",
      "Train Accuracy: 86.52% - Train Loss: 1.6035\n",
      "Test Accuracy: 86.64% - Test Loss: 1.5970\n",
      "----------------------------------------\n",
      "------------------------------------------------------\n",
      "Configuration 52/162:\n",
      "LR: 0.0001, BS: 64, Optimizer: Adam, Dropout: 0.6, Kernel: 3\n",
      "256\n",
      "Train accuracy is less than 70%. Skipping this configuration.\n",
      "------------------------------------------------------\n",
      "Configuration 53/162:\n",
      "LR: 0.0001, BS: 64, Optimizer: Adam, Dropout: 0.6, Kernel: 4\n",
      "64\n",
      "Train accuracy is less than 70%. Skipping this configuration.\n",
      "------------------------------------------------------\n",
      "Configuration 54/162:\n",
      "LR: 0.0001, BS: 64, Optimizer: Adam, Dropout: 0.6, Kernel: 5\n",
      "64\n",
      "Train accuracy is less than 70%. Skipping this configuration.\n",
      "------------------------------------------------------\n",
      "Configuration 55/162:\n",
      "LR: 1e-05, BS: 10, Optimizer: SGD, Dropout: 0.3, Kernel: 3\n",
      "256\n",
      "Train accuracy is less than 70%. Skipping this configuration.\n",
      "------------------------------------------------------\n",
      "Configuration 56/162:\n",
      "LR: 1e-05, BS: 10, Optimizer: SGD, Dropout: 0.3, Kernel: 4\n",
      "64\n",
      "Train accuracy is less than 70%. Skipping this configuration.\n",
      "------------------------------------------------------\n",
      "Configuration 57/162:\n",
      "LR: 1e-05, BS: 10, Optimizer: SGD, Dropout: 0.3, Kernel: 5\n",
      "64\n",
      "Train accuracy is less than 70%. Skipping this configuration.\n",
      "------------------------------------------------------\n",
      "Configuration 58/162:\n",
      "LR: 1e-05, BS: 10, Optimizer: SGD, Dropout: 0.5, Kernel: 3\n",
      "256\n",
      "Train accuracy is less than 70%. Skipping this configuration.\n",
      "------------------------------------------------------\n",
      "Configuration 59/162:\n",
      "LR: 1e-05, BS: 10, Optimizer: SGD, Dropout: 0.5, Kernel: 4\n",
      "64\n",
      "Train accuracy is less than 70%. Skipping this configuration.\n",
      "------------------------------------------------------\n",
      "Configuration 60/162:\n",
      "LR: 1e-05, BS: 10, Optimizer: SGD, Dropout: 0.5, Kernel: 5\n",
      "64\n",
      "Train accuracy is less than 70%. Skipping this configuration.\n",
      "------------------------------------------------------\n",
      "Configuration 61/162:\n",
      "LR: 1e-05, BS: 10, Optimizer: SGD, Dropout: 0.6, Kernel: 3\n",
      "256\n",
      "Train accuracy is less than 70%. Skipping this configuration.\n",
      "------------------------------------------------------\n",
      "Configuration 62/162:\n",
      "LR: 1e-05, BS: 10, Optimizer: SGD, Dropout: 0.6, Kernel: 4\n",
      "64\n",
      "Train accuracy is less than 70%. Skipping this configuration.\n",
      "------------------------------------------------------\n",
      "Configuration 63/162:\n",
      "LR: 1e-05, BS: 10, Optimizer: SGD, Dropout: 0.6, Kernel: 5\n",
      "64\n",
      "Train accuracy is less than 70%. Skipping this configuration.\n",
      "------------------------------------------------------\n",
      "Configuration 64/162:\n",
      "LR: 1e-05, BS: 10, Optimizer: Adam, Dropout: 0.3, Kernel: 3\n",
      "256\n",
      "Train accuracy is less than 70%. Skipping this configuration.\n",
      "------------------------------------------------------\n",
      "Configuration 65/162:\n",
      "LR: 1e-05, BS: 10, Optimizer: Adam, Dropout: 0.3, Kernel: 4\n",
      "64\n",
      "Train accuracy is less than 70%. Skipping this configuration.\n",
      "------------------------------------------------------\n",
      "Configuration 66/162:\n",
      "LR: 1e-05, BS: 10, Optimizer: Adam, Dropout: 0.3, Kernel: 5\n",
      "64\n",
      "Train accuracy is less than 70%. Skipping this configuration.\n",
      "------------------------------------------------------\n",
      "Configuration 67/162:\n",
      "LR: 1e-05, BS: 10, Optimizer: Adam, Dropout: 0.5, Kernel: 3\n",
      "256\n",
      "Train accuracy is less than 70%. Skipping this configuration.\n",
      "------------------------------------------------------\n",
      "Configuration 68/162:\n",
      "LR: 1e-05, BS: 10, Optimizer: Adam, Dropout: 0.5, Kernel: 4\n",
      "64\n",
      "Train accuracy is less than 70%. Skipping this configuration.\n",
      "------------------------------------------------------\n",
      "Configuration 69/162:\n",
      "LR: 1e-05, BS: 10, Optimizer: Adam, Dropout: 0.5, Kernel: 5\n",
      "64\n",
      "Train accuracy is less than 70%. Skipping this configuration.\n",
      "------------------------------------------------------\n",
      "Configuration 70/162:\n",
      "LR: 1e-05, BS: 10, Optimizer: Adam, Dropout: 0.6, Kernel: 3\n",
      "256\n",
      "Train accuracy is less than 70%. Skipping this configuration.\n",
      "------------------------------------------------------\n",
      "Configuration 71/162:\n",
      "LR: 1e-05, BS: 10, Optimizer: Adam, Dropout: 0.6, Kernel: 4\n",
      "64\n",
      "Train accuracy is less than 70%. Skipping this configuration.\n",
      "------------------------------------------------------\n",
      "Configuration 72/162:\n",
      "LR: 1e-05, BS: 10, Optimizer: Adam, Dropout: 0.6, Kernel: 5\n",
      "64\n",
      "Train accuracy is less than 70%. Skipping this configuration.\n",
      "------------------------------------------------------\n",
      "Configuration 73/162:\n",
      "LR: 1e-05, BS: 32, Optimizer: SGD, Dropout: 0.3, Kernel: 3\n",
      "256\n",
      "Train accuracy is less than 70%. Skipping this configuration.\n",
      "------------------------------------------------------\n",
      "Configuration 74/162:\n",
      "LR: 1e-05, BS: 32, Optimizer: SGD, Dropout: 0.3, Kernel: 4\n",
      "64\n",
      "Train accuracy is less than 70%. Skipping this configuration.\n",
      "------------------------------------------------------\n",
      "Configuration 75/162:\n",
      "LR: 1e-05, BS: 32, Optimizer: SGD, Dropout: 0.3, Kernel: 5\n",
      "64\n",
      "Train accuracy is less than 70%. Skipping this configuration.\n",
      "------------------------------------------------------\n",
      "Configuration 76/162:\n",
      "LR: 1e-05, BS: 32, Optimizer: SGD, Dropout: 0.5, Kernel: 3\n",
      "256\n",
      "Train accuracy is less than 70%. Skipping this configuration.\n",
      "------------------------------------------------------\n",
      "Configuration 77/162:\n",
      "LR: 1e-05, BS: 32, Optimizer: SGD, Dropout: 0.5, Kernel: 4\n",
      "64\n",
      "Train accuracy is less than 70%. Skipping this configuration.\n",
      "------------------------------------------------------\n",
      "Configuration 78/162:\n",
      "LR: 1e-05, BS: 32, Optimizer: SGD, Dropout: 0.5, Kernel: 5\n",
      "64\n",
      "Train accuracy is less than 70%. Skipping this configuration.\n",
      "------------------------------------------------------\n",
      "Configuration 79/162:\n",
      "LR: 1e-05, BS: 32, Optimizer: SGD, Dropout: 0.6, Kernel: 3\n",
      "256\n",
      "Train accuracy is less than 70%. Skipping this configuration.\n",
      "------------------------------------------------------\n",
      "Configuration 80/162:\n",
      "LR: 1e-05, BS: 32, Optimizer: SGD, Dropout: 0.6, Kernel: 4\n",
      "64\n",
      "Train accuracy is less than 70%. Skipping this configuration.\n",
      "------------------------------------------------------\n",
      "Configuration 81/162:\n",
      "LR: 1e-05, BS: 32, Optimizer: SGD, Dropout: 0.6, Kernel: 5\n",
      "64\n",
      "Train accuracy is less than 70%. Skipping this configuration.\n",
      "------------------------------------------------------\n",
      "Configuration 82/162:\n",
      "LR: 1e-05, BS: 32, Optimizer: Adam, Dropout: 0.3, Kernel: 3\n",
      "256\n",
      "Train accuracy is less than 70%. Skipping this configuration.\n",
      "------------------------------------------------------\n",
      "Configuration 83/162:\n",
      "LR: 1e-05, BS: 32, Optimizer: Adam, Dropout: 0.3, Kernel: 4\n",
      "64\n",
      "Train accuracy is less than 70%. Skipping this configuration.\n",
      "------------------------------------------------------\n",
      "Configuration 84/162:\n",
      "LR: 1e-05, BS: 32, Optimizer: Adam, Dropout: 0.3, Kernel: 5\n",
      "64\n",
      "Train accuracy is less than 70%. Skipping this configuration.\n",
      "------------------------------------------------------\n",
      "Configuration 85/162:\n",
      "LR: 1e-05, BS: 32, Optimizer: Adam, Dropout: 0.5, Kernel: 3\n",
      "256\n",
      "Train accuracy is less than 70%. Skipping this configuration.\n",
      "------------------------------------------------------\n",
      "Configuration 86/162:\n",
      "LR: 1e-05, BS: 32, Optimizer: Adam, Dropout: 0.5, Kernel: 4\n",
      "64\n",
      "Train accuracy is less than 70%. Skipping this configuration.\n",
      "------------------------------------------------------\n",
      "Configuration 87/162:\n",
      "LR: 1e-05, BS: 32, Optimizer: Adam, Dropout: 0.5, Kernel: 5\n",
      "64\n",
      "Train accuracy is less than 70%. Skipping this configuration.\n",
      "------------------------------------------------------\n",
      "Configuration 88/162:\n",
      "LR: 1e-05, BS: 32, Optimizer: Adam, Dropout: 0.6, Kernel: 3\n",
      "256\n",
      "Train accuracy is less than 70%. Skipping this configuration.\n",
      "------------------------------------------------------\n",
      "Configuration 89/162:\n",
      "LR: 1e-05, BS: 32, Optimizer: Adam, Dropout: 0.6, Kernel: 4\n",
      "64\n",
      "Train accuracy is less than 70%. Skipping this configuration.\n",
      "------------------------------------------------------\n",
      "Configuration 90/162:\n",
      "LR: 1e-05, BS: 32, Optimizer: Adam, Dropout: 0.6, Kernel: 5\n",
      "64\n",
      "Train accuracy is less than 70%. Skipping this configuration.\n",
      "------------------------------------------------------\n",
      "Configuration 91/162:\n",
      "LR: 1e-05, BS: 64, Optimizer: SGD, Dropout: 0.3, Kernel: 3\n",
      "256\n",
      "Train accuracy is less than 70%. Skipping this configuration.\n",
      "------------------------------------------------------\n",
      "Configuration 92/162:\n",
      "LR: 1e-05, BS: 64, Optimizer: SGD, Dropout: 0.3, Kernel: 4\n",
      "64\n",
      "Train accuracy is less than 70%. Skipping this configuration.\n",
      "------------------------------------------------------\n",
      "Configuration 93/162:\n",
      "LR: 1e-05, BS: 64, Optimizer: SGD, Dropout: 0.3, Kernel: 5\n",
      "64\n",
      "Train accuracy is less than 70%. Skipping this configuration.\n",
      "------------------------------------------------------\n",
      "Configuration 94/162:\n",
      "LR: 1e-05, BS: 64, Optimizer: SGD, Dropout: 0.5, Kernel: 3\n",
      "256\n",
      "Train accuracy is less than 70%. Skipping this configuration.\n",
      "------------------------------------------------------\n",
      "Configuration 95/162:\n",
      "LR: 1e-05, BS: 64, Optimizer: SGD, Dropout: 0.5, Kernel: 4\n",
      "64\n",
      "Train accuracy is less than 70%. Skipping this configuration.\n",
      "------------------------------------------------------\n",
      "Configuration 96/162:\n",
      "LR: 1e-05, BS: 64, Optimizer: SGD, Dropout: 0.5, Kernel: 5\n",
      "64\n",
      "Train accuracy is less than 70%. Skipping this configuration.\n",
      "------------------------------------------------------\n",
      "Configuration 97/162:\n",
      "LR: 1e-05, BS: 64, Optimizer: SGD, Dropout: 0.6, Kernel: 3\n",
      "256\n",
      "Train accuracy is less than 70%. Skipping this configuration.\n",
      "------------------------------------------------------\n",
      "Configuration 98/162:\n",
      "LR: 1e-05, BS: 64, Optimizer: SGD, Dropout: 0.6, Kernel: 4\n",
      "64\n",
      "Train accuracy is less than 70%. Skipping this configuration.\n",
      "------------------------------------------------------\n",
      "Configuration 99/162:\n",
      "LR: 1e-05, BS: 64, Optimizer: SGD, Dropout: 0.6, Kernel: 5\n",
      "64\n",
      "Train accuracy is less than 70%. Skipping this configuration.\n",
      "------------------------------------------------------\n",
      "Configuration 100/162:\n",
      "LR: 1e-05, BS: 64, Optimizer: Adam, Dropout: 0.3, Kernel: 3\n",
      "256\n",
      "Train accuracy is less than 70%. Skipping this configuration.\n",
      "------------------------------------------------------\n",
      "Configuration 101/162:\n",
      "LR: 1e-05, BS: 64, Optimizer: Adam, Dropout: 0.3, Kernel: 4\n",
      "64\n",
      "Train accuracy is less than 70%. Skipping this configuration.\n",
      "------------------------------------------------------\n",
      "Configuration 102/162:\n",
      "LR: 1e-05, BS: 64, Optimizer: Adam, Dropout: 0.3, Kernel: 5\n",
      "64\n",
      "Train accuracy is less than 70%. Skipping this configuration.\n",
      "------------------------------------------------------\n",
      "Configuration 103/162:\n",
      "LR: 1e-05, BS: 64, Optimizer: Adam, Dropout: 0.5, Kernel: 3\n",
      "256\n",
      "Train accuracy is less than 70%. Skipping this configuration.\n",
      "------------------------------------------------------\n",
      "Configuration 104/162:\n",
      "LR: 1e-05, BS: 64, Optimizer: Adam, Dropout: 0.5, Kernel: 4\n",
      "64\n",
      "Train accuracy is less than 70%. Skipping this configuration.\n",
      "------------------------------------------------------\n",
      "Configuration 105/162:\n",
      "LR: 1e-05, BS: 64, Optimizer: Adam, Dropout: 0.5, Kernel: 5\n",
      "64\n",
      "Train accuracy is less than 70%. Skipping this configuration.\n",
      "------------------------------------------------------\n",
      "Configuration 106/162:\n",
      "LR: 1e-05, BS: 64, Optimizer: Adam, Dropout: 0.6, Kernel: 3\n",
      "256\n",
      "Train accuracy is less than 70%. Skipping this configuration.\n",
      "------------------------------------------------------\n",
      "Configuration 107/162:\n",
      "LR: 1e-05, BS: 64, Optimizer: Adam, Dropout: 0.6, Kernel: 4\n",
      "64\n",
      "Train accuracy is less than 70%. Skipping this configuration.\n",
      "------------------------------------------------------\n",
      "Configuration 108/162:\n",
      "LR: 1e-05, BS: 64, Optimizer: Adam, Dropout: 0.6, Kernel: 5\n",
      "64\n",
      "Train accuracy is less than 70%. Skipping this configuration.\n",
      "------------------------------------------------------\n",
      "Configuration 109/162:\n",
      "LR: 1e-06, BS: 10, Optimizer: SGD, Dropout: 0.3, Kernel: 3\n",
      "256\n",
      "Train accuracy is less than 70%. Skipping this configuration.\n",
      "------------------------------------------------------\n",
      "Configuration 110/162:\n",
      "LR: 1e-06, BS: 10, Optimizer: SGD, Dropout: 0.3, Kernel: 4\n",
      "64\n",
      "Train accuracy is less than 70%. Skipping this configuration.\n",
      "------------------------------------------------------\n",
      "Configuration 111/162:\n",
      "LR: 1e-06, BS: 10, Optimizer: SGD, Dropout: 0.3, Kernel: 5\n",
      "64\n",
      "Train accuracy is less than 70%. Skipping this configuration.\n",
      "------------------------------------------------------\n",
      "Configuration 112/162:\n",
      "LR: 1e-06, BS: 10, Optimizer: SGD, Dropout: 0.5, Kernel: 3\n",
      "256\n",
      "Train accuracy is less than 70%. Skipping this configuration.\n",
      "------------------------------------------------------\n",
      "Configuration 113/162:\n",
      "LR: 1e-06, BS: 10, Optimizer: SGD, Dropout: 0.5, Kernel: 4\n",
      "64\n",
      "Train accuracy is less than 70%. Skipping this configuration.\n",
      "------------------------------------------------------\n",
      "Configuration 114/162:\n",
      "LR: 1e-06, BS: 10, Optimizer: SGD, Dropout: 0.5, Kernel: 5\n",
      "64\n",
      "Train accuracy is less than 70%. Skipping this configuration.\n",
      "------------------------------------------------------\n",
      "Configuration 115/162:\n",
      "LR: 1e-06, BS: 10, Optimizer: SGD, Dropout: 0.6, Kernel: 3\n",
      "256\n",
      "Train accuracy is less than 70%. Skipping this configuration.\n",
      "------------------------------------------------------\n",
      "Configuration 116/162:\n",
      "LR: 1e-06, BS: 10, Optimizer: SGD, Dropout: 0.6, Kernel: 4\n",
      "64\n",
      "Train accuracy is less than 70%. Skipping this configuration.\n",
      "------------------------------------------------------\n",
      "Configuration 117/162:\n",
      "LR: 1e-06, BS: 10, Optimizer: SGD, Dropout: 0.6, Kernel: 5\n",
      "64\n",
      "Train accuracy is less than 70%. Skipping this configuration.\n",
      "------------------------------------------------------\n",
      "Configuration 118/162:\n",
      "LR: 1e-06, BS: 10, Optimizer: Adam, Dropout: 0.3, Kernel: 3\n",
      "256\n",
      "Train accuracy is less than 70%. Skipping this configuration.\n",
      "------------------------------------------------------\n",
      "Configuration 119/162:\n",
      "LR: 1e-06, BS: 10, Optimizer: Adam, Dropout: 0.3, Kernel: 4\n",
      "64\n",
      "Train accuracy is less than 70%. Skipping this configuration.\n",
      "------------------------------------------------------\n",
      "Configuration 120/162:\n",
      "LR: 1e-06, BS: 10, Optimizer: Adam, Dropout: 0.3, Kernel: 5\n",
      "64\n",
      "Train accuracy is less than 70%. Skipping this configuration.\n",
      "------------------------------------------------------\n",
      "Configuration 121/162:\n",
      "LR: 1e-06, BS: 10, Optimizer: Adam, Dropout: 0.5, Kernel: 3\n",
      "256\n",
      "Train accuracy is less than 70%. Skipping this configuration.\n",
      "------------------------------------------------------\n",
      "Configuration 122/162:\n",
      "LR: 1e-06, BS: 10, Optimizer: Adam, Dropout: 0.5, Kernel: 4\n",
      "64\n",
      "Train accuracy is less than 70%. Skipping this configuration.\n",
      "------------------------------------------------------\n",
      "Configuration 123/162:\n",
      "LR: 1e-06, BS: 10, Optimizer: Adam, Dropout: 0.5, Kernel: 5\n",
      "64\n",
      "Train accuracy is less than 70%. Skipping this configuration.\n",
      "------------------------------------------------------\n",
      "Configuration 124/162:\n",
      "LR: 1e-06, BS: 10, Optimizer: Adam, Dropout: 0.6, Kernel: 3\n",
      "256\n",
      "Train accuracy is less than 70%. Skipping this configuration.\n",
      "------------------------------------------------------\n",
      "Configuration 125/162:\n",
      "LR: 1e-06, BS: 10, Optimizer: Adam, Dropout: 0.6, Kernel: 4\n",
      "64\n",
      "Train accuracy is less than 70%. Skipping this configuration.\n",
      "------------------------------------------------------\n",
      "Configuration 126/162:\n",
      "LR: 1e-06, BS: 10, Optimizer: Adam, Dropout: 0.6, Kernel: 5\n",
      "64\n",
      "Train accuracy is less than 70%. Skipping this configuration.\n",
      "------------------------------------------------------\n",
      "Configuration 127/162:\n",
      "LR: 1e-06, BS: 32, Optimizer: SGD, Dropout: 0.3, Kernel: 3\n",
      "256\n",
      "Train accuracy is less than 70%. Skipping this configuration.\n",
      "------------------------------------------------------\n",
      "Configuration 128/162:\n",
      "LR: 1e-06, BS: 32, Optimizer: SGD, Dropout: 0.3, Kernel: 4\n",
      "64\n",
      "Train accuracy is less than 70%. Skipping this configuration.\n",
      "------------------------------------------------------\n",
      "Configuration 129/162:\n",
      "LR: 1e-06, BS: 32, Optimizer: SGD, Dropout: 0.3, Kernel: 5\n",
      "64\n",
      "Train accuracy is less than 70%. Skipping this configuration.\n",
      "------------------------------------------------------\n",
      "Configuration 130/162:\n",
      "LR: 1e-06, BS: 32, Optimizer: SGD, Dropout: 0.5, Kernel: 3\n",
      "256\n",
      "Train accuracy is less than 70%. Skipping this configuration.\n",
      "------------------------------------------------------\n",
      "Configuration 131/162:\n",
      "LR: 1e-06, BS: 32, Optimizer: SGD, Dropout: 0.5, Kernel: 4\n",
      "64\n",
      "Train accuracy is less than 70%. Skipping this configuration.\n",
      "------------------------------------------------------\n",
      "Configuration 132/162:\n",
      "LR: 1e-06, BS: 32, Optimizer: SGD, Dropout: 0.5, Kernel: 5\n",
      "64\n",
      "Train accuracy is less than 70%. Skipping this configuration.\n",
      "------------------------------------------------------\n",
      "Configuration 133/162:\n",
      "LR: 1e-06, BS: 32, Optimizer: SGD, Dropout: 0.6, Kernel: 3\n",
      "256\n",
      "Train accuracy is less than 70%. Skipping this configuration.\n",
      "------------------------------------------------------\n",
      "Configuration 134/162:\n",
      "LR: 1e-06, BS: 32, Optimizer: SGD, Dropout: 0.6, Kernel: 4\n",
      "64\n",
      "Train accuracy is less than 70%. Skipping this configuration.\n",
      "------------------------------------------------------\n",
      "Configuration 135/162:\n",
      "LR: 1e-06, BS: 32, Optimizer: SGD, Dropout: 0.6, Kernel: 5\n",
      "64\n",
      "Train accuracy is less than 70%. Skipping this configuration.\n",
      "------------------------------------------------------\n",
      "Configuration 136/162:\n",
      "LR: 1e-06, BS: 32, Optimizer: Adam, Dropout: 0.3, Kernel: 3\n",
      "256\n",
      "Train accuracy is less than 70%. Skipping this configuration.\n",
      "------------------------------------------------------\n",
      "Configuration 137/162:\n",
      "LR: 1e-06, BS: 32, Optimizer: Adam, Dropout: 0.3, Kernel: 4\n",
      "64\n",
      "Train accuracy is less than 70%. Skipping this configuration.\n",
      "------------------------------------------------------\n",
      "Configuration 138/162:\n",
      "LR: 1e-06, BS: 32, Optimizer: Adam, Dropout: 0.3, Kernel: 5\n",
      "64\n",
      "Train accuracy is less than 70%. Skipping this configuration.\n",
      "------------------------------------------------------\n",
      "Configuration 139/162:\n",
      "LR: 1e-06, BS: 32, Optimizer: Adam, Dropout: 0.5, Kernel: 3\n",
      "256\n",
      "Train accuracy is less than 70%. Skipping this configuration.\n",
      "------------------------------------------------------\n",
      "Configuration 140/162:\n",
      "LR: 1e-06, BS: 32, Optimizer: Adam, Dropout: 0.5, Kernel: 4\n",
      "64\n",
      "Train accuracy is less than 70%. Skipping this configuration.\n",
      "------------------------------------------------------\n",
      "Configuration 141/162:\n",
      "LR: 1e-06, BS: 32, Optimizer: Adam, Dropout: 0.5, Kernel: 5\n",
      "64\n",
      "Train accuracy is less than 70%. Skipping this configuration.\n",
      "------------------------------------------------------\n",
      "Configuration 142/162:\n",
      "LR: 1e-06, BS: 32, Optimizer: Adam, Dropout: 0.6, Kernel: 3\n",
      "256\n",
      "Train accuracy is less than 70%. Skipping this configuration.\n",
      "------------------------------------------------------\n",
      "Configuration 143/162:\n",
      "LR: 1e-06, BS: 32, Optimizer: Adam, Dropout: 0.6, Kernel: 4\n",
      "64\n",
      "Train accuracy is less than 70%. Skipping this configuration.\n",
      "------------------------------------------------------\n",
      "Configuration 144/162:\n",
      "LR: 1e-06, BS: 32, Optimizer: Adam, Dropout: 0.6, Kernel: 5\n",
      "64\n",
      "Train accuracy is less than 70%. Skipping this configuration.\n",
      "------------------------------------------------------\n",
      "Configuration 145/162:\n",
      "LR: 1e-06, BS: 64, Optimizer: SGD, Dropout: 0.3, Kernel: 3\n",
      "256\n",
      "Train accuracy is less than 70%. Skipping this configuration.\n",
      "------------------------------------------------------\n",
      "Configuration 146/162:\n",
      "LR: 1e-06, BS: 64, Optimizer: SGD, Dropout: 0.3, Kernel: 4\n",
      "64\n",
      "Train accuracy is less than 70%. Skipping this configuration.\n",
      "------------------------------------------------------\n",
      "Configuration 147/162:\n",
      "LR: 1e-06, BS: 64, Optimizer: SGD, Dropout: 0.3, Kernel: 5\n",
      "64\n",
      "Train accuracy is less than 70%. Skipping this configuration.\n",
      "------------------------------------------------------\n",
      "Configuration 148/162:\n",
      "LR: 1e-06, BS: 64, Optimizer: SGD, Dropout: 0.5, Kernel: 3\n",
      "256\n",
      "Train accuracy is less than 70%. Skipping this configuration.\n",
      "------------------------------------------------------\n",
      "Configuration 149/162:\n",
      "LR: 1e-06, BS: 64, Optimizer: SGD, Dropout: 0.5, Kernel: 4\n",
      "64\n",
      "Train accuracy is less than 70%. Skipping this configuration.\n",
      "------------------------------------------------------\n",
      "Configuration 150/162:\n",
      "LR: 1e-06, BS: 64, Optimizer: SGD, Dropout: 0.5, Kernel: 5\n",
      "64\n",
      "Train accuracy is less than 70%. Skipping this configuration.\n",
      "------------------------------------------------------\n",
      "Configuration 151/162:\n",
      "LR: 1e-06, BS: 64, Optimizer: SGD, Dropout: 0.6, Kernel: 3\n",
      "256\n",
      "Train accuracy is less than 70%. Skipping this configuration.\n",
      "------------------------------------------------------\n",
      "Configuration 152/162:\n",
      "LR: 1e-06, BS: 64, Optimizer: SGD, Dropout: 0.6, Kernel: 4\n",
      "64\n",
      "Train accuracy is less than 70%. Skipping this configuration.\n",
      "------------------------------------------------------\n",
      "Configuration 153/162:\n",
      "LR: 1e-06, BS: 64, Optimizer: SGD, Dropout: 0.6, Kernel: 5\n",
      "64\n",
      "Train accuracy is less than 70%. Skipping this configuration.\n",
      "------------------------------------------------------\n",
      "Configuration 154/162:\n",
      "LR: 1e-06, BS: 64, Optimizer: Adam, Dropout: 0.3, Kernel: 3\n",
      "256\n",
      "Train accuracy is less than 70%. Skipping this configuration.\n",
      "------------------------------------------------------\n",
      "Configuration 155/162:\n",
      "LR: 1e-06, BS: 64, Optimizer: Adam, Dropout: 0.3, Kernel: 4\n",
      "64\n",
      "Train accuracy is less than 70%. Skipping this configuration.\n",
      "------------------------------------------------------\n",
      "Configuration 156/162:\n",
      "LR: 1e-06, BS: 64, Optimizer: Adam, Dropout: 0.3, Kernel: 5\n",
      "64\n",
      "Train accuracy is less than 70%. Skipping this configuration.\n",
      "------------------------------------------------------\n",
      "Configuration 157/162:\n",
      "LR: 1e-06, BS: 64, Optimizer: Adam, Dropout: 0.5, Kernel: 3\n",
      "256\n",
      "Train accuracy is less than 70%. Skipping this configuration.\n",
      "------------------------------------------------------\n",
      "Configuration 158/162:\n",
      "LR: 1e-06, BS: 64, Optimizer: Adam, Dropout: 0.5, Kernel: 4\n",
      "64\n",
      "Train accuracy is less than 70%. Skipping this configuration.\n",
      "------------------------------------------------------\n",
      "Configuration 159/162:\n",
      "LR: 1e-06, BS: 64, Optimizer: Adam, Dropout: 0.5, Kernel: 5\n",
      "64\n",
      "Train accuracy is less than 70%. Skipping this configuration.\n",
      "------------------------------------------------------\n",
      "Configuration 160/162:\n",
      "LR: 1e-06, BS: 64, Optimizer: Adam, Dropout: 0.6, Kernel: 3\n",
      "256\n",
      "Train accuracy is less than 70%. Skipping this configuration.\n",
      "------------------------------------------------------\n",
      "Configuration 161/162:\n",
      "LR: 1e-06, BS: 64, Optimizer: Adam, Dropout: 0.6, Kernel: 4\n",
      "64\n",
      "Train accuracy is less than 70%. Skipping this configuration.\n",
      "------------------------------------------------------\n",
      "Configuration 162/162:\n",
      "LR: 1e-06, BS: 64, Optimizer: Adam, Dropout: 0.6, Kernel: 5\n",
      "64\n",
      "Train accuracy is less than 70%. Skipping this configuration.\n",
      "------------------------------------------------------\n",
      "Best Test Accuracy: 88.91\n",
      "Best Hyperparameters: {'lr': 0.0001, 'batch_size': 10, 'optimizer': <class 'torch.optim.adam.Adam'>, 'dropout_rate': 0.3, 'kernel_size': 3}\n"
     ]
    }
   ],
   "source": [
    "# Define hyperparameters to tune\n",
    "learning_rates = [0.0001, 0.00001, 0.000001]\n",
    "batch_sizes = [10, 32, 64]\n",
    "optimizers = [torch.optim.SGD, torch.optim.Adam]\n",
    "dropout_rates = [0.3, 0.5, 0.6]\n",
    "kernel_sizes = [3, 4, 5]\n",
    "num_epochs = 10\n",
    "\n",
    "best_accuracy = 0.0\n",
    "best_params = {}\n",
    "\n",
    "results = {}  # Dictionary to store results\n",
    "\n",
    "# Counter to keep track of configurations\n",
    "config_count = 1\n",
    "total_configs = (\n",
    "    len(learning_rates)\n",
    "    * len(batch_sizes)\n",
    "    * len(optimizers)\n",
    "    * len(dropout_rates)\n",
    "    * len(kernel_sizes)\n",
    ")\n",
    "# Grid search\n",
    "for lr in learning_rates:\n",
    "    for bs in batch_sizes:\n",
    "        for opt in optimizers:\n",
    "            for dr in dropout_rates:\n",
    "                for ks in kernel_sizes:\n",
    "                    config_key = f\"LR: {lr}, BS: {bs}, Optimizer: {opt.__name__}, Dropout: {dr}, Kernel: {ks}\"\n",
    "                    print(f\"Configuration {config_count}/{total_configs}:\")\n",
    "                    print(config_key)\n",
    "\n",
    "                    train_loader = torch.utils.data.DataLoader(\n",
    "                        train_set, batch_size=bs, shuffle=True\n",
    "                    )\n",
    "                    test_loader = torch.utils.data.DataLoader(test_set, batch_size=bs)\n",
    "\n",
    "                    model = MyCNN(kernel_size=ks, dropout_rate=dr).to(device)\n",
    "                    criterion = nn.CrossEntropyLoss()\n",
    "                    optimizer = opt(model.parameters(), lr=lr)\n",
    "\n",
    "                    # Variables to store cumulative loss across epochs\n",
    "                    total_train_loss = 0.0\n",
    "                    total_test_loss = 0.0\n",
    "                    start_time = time.time()\n",
    "\n",
    "                    # Train and test the model with the current hyperparameters for the specified number of epochs\n",
    "                    for epoch in range(num_epochs):\n",
    "                        train_accuracy, train_loss = train(\n",
    "                            model, train_loader, criterion, optimizer\n",
    "                        )\n",
    "                        total_train_loss += train_loss\n",
    "\n",
    "                        # Skip further training and testing for this configuration if training accuracy is less than 80%\n",
    "                        if train_accuracy < 70.0:\n",
    "                            print(\n",
    "                                \"Train accuracy is less than 70%. Skipping this configuration.\"\n",
    "                            )\n",
    "                            break\n",
    "\n",
    "                        test_accuracy, test_loss = test(model, test_loader)\n",
    "                        total_test_loss += test_loss\n",
    "\n",
    "                        # Printing per-epoch results\n",
    "                        print(f\"Epoch {epoch+1}/{num_epochs}:\")\n",
    "                        print(\n",
    "                            f\"Train Accuracy: {train_accuracy:.2f}% - Train Loss: {train_loss:.4f}\"\n",
    "                        )\n",
    "                        print(\n",
    "                            f\"Test Accuracy: {test_accuracy:.2f}% - Test Loss: {test_loss:.4f}\"\n",
    "                        )\n",
    "                        print(\"----------------------------------------\")\n",
    "\n",
    "                    avg_train_loss = total_train_loss / num_epochs\n",
    "                    avg_test_loss = total_test_loss / num_epochs\n",
    "                    end_time = time.time()\n",
    "                    config_time = end_time - start_time\n",
    "\n",
    "                    # Store the results ONLY if train_accuracy >= 70%\n",
    "                    if train_accuracy >= 70.0:\n",
    "                        results[config_key] = {\n",
    "                            \"Train Accuracy\": train_accuracy,\n",
    "                            \"Test Accuracy\": test_accuracy,\n",
    "                            \"Average Train Loss\": avg_train_loss,\n",
    "                            \"Average Test Loss\": avg_test_loss,\n",
    "                            \"Time (in sec)\": config_time,\n",
    "                        }\n",
    "\n",
    "                    # Update best accuracy and parameters\n",
    "                    if test_accuracy > best_accuracy:\n",
    "                        best_accuracy = test_accuracy\n",
    "                        best_params = {\n",
    "                            \"lr\": lr,\n",
    "                            \"batch_size\": bs,\n",
    "                            \"optimizer\": opt,\n",
    "                            \"dropout_rate\": dr,\n",
    "                            \"kernel_size\": ks,\n",
    "                        }\n",
    "                    print(\"------------------------------------------------------\")\n",
    "                    config_count += 1\n",
    "\n",
    "print(\"Best Test Accuracy:\", best_accuracy)\n",
    "print(\"Best Hyperparameters:\", best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to 'results.csv'\n"
     ]
    }
   ],
   "source": [
    "# Convert the results dictionary to a DataFrame\n",
    "df_results = pd.DataFrame.from_dict(results, orient=\"index\")\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df_results.to_csv(\"./data/results.csv\")\n",
    "\n",
    "print(\"Results saved to 'results.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimized MRI Cnn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration 1/1:\n",
      "LR: 0.0001, BS: 10, Optimizer: Adam, Dropout: 0.3, Kernel: 3\n",
      "36864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/functional.py:1331: UserWarning: dropout2d: Received a 2-D input to dropout2d, which is deprecated and will result in an error in a future release. To retain the behavior and silence this warning, please use dropout instead. Note that dropout2d exists to provide channel-wise dropout on inputs with 2 spatial dimensions, a channel dimension, and an optional batch dimension (i.e. 3D or 4D inputs).\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30], Train Accuracy: 74.56%, Test Accuracy: 82.84%, Train Loss: 0.7967, Test Loss: 0.7185\n",
      "Epoch [2/30], Train Accuracy: 84.64%, Test Accuracy: 87.19%, Train Loss: 0.7041, Test Loss: 0.6780\n",
      "Epoch [3/30], Train Accuracy: 88.43%, Test Accuracy: 89.86%, Train Loss: 0.6656, Test Loss: 0.6545\n",
      "Epoch [4/30], Train Accuracy: 90.61%, Test Accuracy: 91.73%, Train Loss: 0.6475, Test Loss: 0.6360\n",
      "Epoch [5/30], Train Accuracy: 92.48%, Test Accuracy: 93.22%, Train Loss: 0.6294, Test Loss: 0.6195\n",
      "Epoch [6/30], Train Accuracy: 92.79%, Test Accuracy: 93.53%, Train Loss: 0.6241, Test Loss: 0.6144\n",
      "Epoch [7/30], Train Accuracy: 93.84%, Test Accuracy: 94.53%, Train Loss: 0.6143, Test Loss: 0.6056\n",
      "Epoch [8/30], Train Accuracy: 94.34%, Test Accuracy: 95.21%, Train Loss: 0.6079, Test Loss: 0.6007\n",
      "Epoch [9/30], Train Accuracy: 94.22%, Test Accuracy: 93.41%, Train Loss: 0.6079, Test Loss: 0.6191\n",
      "Epoch [10/30], Train Accuracy: 95.21%, Test Accuracy: 93.16%, Train Loss: 0.6008, Test Loss: 0.6192\n",
      "Epoch [11/30], Train Accuracy: 95.90%, Test Accuracy: 96.58%, Train Loss: 0.5932, Test Loss: 0.5863\n",
      "Epoch [12/30], Train Accuracy: 95.96%, Test Accuracy: 96.64%, Train Loss: 0.5929, Test Loss: 0.5853\n",
      "Epoch [13/30], Train Accuracy: 96.21%, Test Accuracy: 96.70%, Train Loss: 0.5898, Test Loss: 0.5849\n",
      "Epoch [14/30], Train Accuracy: 96.64%, Test Accuracy: 97.08%, Train Loss: 0.5858, Test Loss: 0.5813\n",
      "Epoch [15/30], Train Accuracy: 96.08%, Test Accuracy: 96.02%, Train Loss: 0.5912, Test Loss: 0.5910\n",
      "Epoch [16/30], Train Accuracy: 96.95%, Test Accuracy: 97.08%, Train Loss: 0.5828, Test Loss: 0.5811\n",
      "Epoch [17/30], Train Accuracy: 97.14%, Test Accuracy: 97.45%, Train Loss: 0.5812, Test Loss: 0.5781\n",
      "Epoch [18/30], Train Accuracy: 96.58%, Test Accuracy: 96.33%, Train Loss: 0.5857, Test Loss: 0.5888\n",
      "Epoch [19/30], Train Accuracy: 96.77%, Test Accuracy: 97.33%, Train Loss: 0.5848, Test Loss: 0.5792\n",
      "Epoch [20/30], Train Accuracy: 96.27%, Test Accuracy: 97.20%, Train Loss: 0.5866, Test Loss: 0.5797\n",
      "Epoch [21/30], Train Accuracy: 96.77%, Test Accuracy: 97.57%, Train Loss: 0.5828, Test Loss: 0.5757\n",
      "Epoch [22/30], Train Accuracy: 97.70%, Test Accuracy: 97.82%, Train Loss: 0.5760, Test Loss: 0.5737\n",
      "Epoch [23/30], Train Accuracy: 97.51%, Test Accuracy: 97.70%, Train Loss: 0.5770, Test Loss: 0.5737\n",
      "Epoch [24/30], Train Accuracy: 97.26%, Test Accuracy: 97.08%, Train Loss: 0.5794, Test Loss: 0.5809\n",
      "Epoch [25/30], Train Accuracy: 96.83%, Test Accuracy: 97.89%, Train Loss: 0.5821, Test Loss: 0.5741\n",
      "Epoch [26/30], Train Accuracy: 97.57%, Test Accuracy: 96.89%, Train Loss: 0.5752, Test Loss: 0.5831\n",
      "Epoch [27/30], Train Accuracy: 97.64%, Test Accuracy: 98.07%, Train Loss: 0.5756, Test Loss: 0.5708\n",
      "Epoch [28/30], Train Accuracy: 97.76%, Test Accuracy: 97.76%, Train Loss: 0.5741, Test Loss: 0.5732\n",
      "Epoch [29/30], Train Accuracy: 97.76%, Test Accuracy: 98.07%, Train Loss: 0.5749, Test Loss: 0.5708\n",
      "Epoch [30/30], Train Accuracy: 97.89%, Test Accuracy: 97.82%, Train Loss: 0.5735, Test Loss: 0.5729\n",
      "Training and testing completed!\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define hyperparameters to tune\n",
    "learning_rates = [0.0001]\n",
    "batch_sizes = [10]\n",
    "optimizers = [torch.optim.Adam]\n",
    "dropout_rates = [0.3]\n",
    "kernel_sizes = [3]\n",
    "num_epochs = 30\n",
    "\n",
    "# Counter to keep track of configurations\n",
    "config_count = 1\n",
    "total_configs = (\n",
    "    len(learning_rates)\n",
    "    * len(batch_sizes)\n",
    "    * len(optimizers)\n",
    "    * len(dropout_rates)\n",
    "    * len(kernel_sizes)\n",
    ")\n",
    "# Grid search\n",
    "for lr in learning_rates:\n",
    "    for bs in batch_sizes:\n",
    "        for opt in optimizers:\n",
    "            for dr in dropout_rates:\n",
    "                for ks in kernel_sizes:\n",
    "                    config_key = f\"LR: {lr}, BS: {bs}, Optimizer: {opt.__name__}, Dropout: {dr}, Kernel: {ks}\"\n",
    "                    print(f\"Configuration {config_count}/{total_configs}:\")\n",
    "                    print(config_key)\n",
    "\n",
    "                    train_loader_MRI = torch.utils.data.DataLoader(\n",
    "                        train_set_MRI, batch_size=bs, shuffle=True\n",
    "                    )\n",
    "                    test_loader_MRI = torch.utils.data.DataLoader(\n",
    "                        train_set_MRI, batch_size=bs\n",
    "                    )\n",
    "                    model = MyCNN(\n",
    "                        kernel_size=ks, dropout_rate=dr, classes=3, input_size=227\n",
    "                    ).to(device)\n",
    "                    criterion = nn.CrossEntropyLoss()\n",
    "                    optimizer = opt(model.parameters(), lr=lr)\n",
    "                    train_and_evaluate_model(\n",
    "                        model,\n",
    "                        train_loader_MRI,\n",
    "                        test_loader_MRI,\n",
    "                        criterion,\n",
    "                        optimizer,\n",
    "                        num_epochs=30,\n",
    "                        name=\"MRI_Optimized\",\n",
    "                    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
