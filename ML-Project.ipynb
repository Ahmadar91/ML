{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torchsummary import summary\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = torchvision.datasets.FashionMNIST(\n",
    "    \"./data\", download=True, transform=transforms.Compose([transforms.ToTensor()])\n",
    ")\n",
    "test_set = torchvision.datasets.FashionMNIST(\n",
    "    \"./data\",\n",
    "    download=True,\n",
    "    train=False,\n",
    "    transform=transforms.Compose([transforms.ToTensor()]),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=100)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_label(label):\n",
    "    output_mapping = {\n",
    "        0: \"T-shirt/Top\",\n",
    "        1: \"Trouser\",\n",
    "        2: \"Pullover\",\n",
    "        3: \"Dress\",\n",
    "        4: \"Coat\",\n",
    "        5: \"Sandal\",\n",
    "        6: \"Shirt\",\n",
    "        7: \"Sneaker\",\n",
    "        8: \"Bag\",\n",
    "        9: \"Ankle Boot\",\n",
    "    }\n",
    "    input = label.item() if type(label) == torch.Tensor else label\n",
    "    return output_mapping[input]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 1, 28, 28])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = next(iter(train_loader))\n",
    "a[0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image, label = next(iter(train_set))\n",
    "plt.imshow(image.squeeze(), cmap=\"gray\")\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "torch.Size([10, 1, 28, 28]) torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "demo_loader = torch.utils.data.DataLoader(train_set, batch_size=10)\n",
    "\n",
    "batch = next(iter(demo_loader))\n",
    "images, labels = batch\n",
    "print(type(images), type(labels))\n",
    "print(images.shape, labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "torch.Size([10, 1, 28, 28]) torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "demo_loader = torch.utils.data.DataLoader(train_set, batch_size=10)\n",
    "\n",
    "batch = next(iter(demo_loader))\n",
    "images, labels = batch\n",
    "print(type(images), type(labels))\n",
    "print(images.shape, labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdjustedFashionCNN(\n",
       "  (layer1): Sequential(\n",
       "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (fc1): Linear(in_features=1600, out_features=600, bias=True)\n",
       "  (drop): Dropout2d(p=0.25, inplace=False)\n",
       "  (fc2): Linear(in_features=600, out_features=120, bias=True)\n",
       "  (fc3): Linear(in_features=120, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class AdjustedFashionCNN(nn.Module):\n",
    "    def __init__(self, kernel_size=3):\n",
    "        super(AdjustedFashionCNN, self).__init__()\n",
    "\n",
    "        self.kernel_size = kernel_size\n",
    "        input_size = 28  # Assuming the input images are 28x28\n",
    "        output_channels1 = 32\n",
    "        output_channels2 = 64\n",
    "\n",
    "        # Compute output sizes after conv and pool operations\n",
    "        conv1_out_size = input_size - self.kernel_size + 1\n",
    "        pool1_out_size = (\n",
    "            conv1_out_size // 2\n",
    "        )  # Using kernel_size and stride=2 for pooling\n",
    "\n",
    "        conv2_input_size = pool1_out_size\n",
    "        conv2_out_size = conv2_input_size - self.kernel_size + 1\n",
    "        pool2_out_size = conv2_out_size // 2\n",
    "\n",
    "        fc_input_size = output_channels2 * pool2_out_size * pool2_out_size\n",
    "\n",
    "        # Layer 2 to 5\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=1,\n",
    "                out_channels=output_channels1,\n",
    "                kernel_size=self.kernel_size,\n",
    "            ),  # Layer 2: Convolutional layer\n",
    "            nn.BatchNorm2d(output_channels1),  # Layer 3: Batch normalization\n",
    "            nn.ReLU(),  # Layer 4: ReLU activation\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # Layer 5: Max Pooling\n",
    "        )\n",
    "\n",
    "        # Layer 6 to 9\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=output_channels1,\n",
    "                out_channels=output_channels2,\n",
    "                kernel_size=self.kernel_size,\n",
    "            ),  # Layer 6: Convolutional layer\n",
    "            nn.BatchNorm2d(output_channels2),  # Layer 7: Batch normalization\n",
    "            nn.ReLU(),  # Layer 8: ReLU activation\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # Layer 9: Max Pooling\n",
    "        )\n",
    "\n",
    "        # Layer 11\n",
    "        self.fc1 = nn.Linear(in_features=fc_input_size, out_features=600)\n",
    "\n",
    "        # Layer 10: Dropout\n",
    "        self.drop = nn.Dropout2d(0.25)\n",
    "\n",
    "        # Layer 12\n",
    "        self.fc2 = nn.Linear(in_features=600, out_features=120)\n",
    "\n",
    "        # Layer 13: Classification layer\n",
    "        self.fc3 = nn.Linear(in_features=120, out_features=10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc1(out)\n",
    "        out = self.drop(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.fc3(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "# Create an instance of the adjusted model with a kernel size of 3 (for example) and print its architecture\n",
    "adjusted_fashion_model = AdjustedFashionCNN(kernel_size=3)\n",
    "adjusted_fashion_model\n",
    "\n",
    "# Check if GPU is available and move the model to GPU if possible\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "adjusted_fashion_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the train and test functions\n",
    "\n",
    "\n",
    "def train(model, data_loader, criterion, optimizer):\n",
    "    model.train()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for images, labels in data_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    average_loss = total_loss / len(data_loader)\n",
    "    return accuracy, average_loss\n",
    "\n",
    "\n",
    "def test(model, data_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in data_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    average_loss = total_loss / len(data_loader)\n",
    "    return accuracy, average_loss\n",
    "\n",
    "\n",
    "# Model, loss function, optimizer initialization\n",
    "model = AdjustedFashionCNN().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.00001)\n",
    "\n",
    "# Training the model and capturing metrics\n",
    "num_epochs = 5\n",
    "train_accuracies, test_accuracies, train_losses, test_losses = [], [], [], []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_accuracy, train_loss = train(model, train_loader, criterion, optimizer)\n",
    "    test_accuracy, test_loss = test(model, test_loader)\n",
    "\n",
    "    train_accuracies.append(train_accuracy)\n",
    "    test_accuracies.append(test_accuracy)\n",
    "    train_losses.append(train_loss)\n",
    "    test_losses.append(test_loss)\n",
    "\n",
    "    print(\n",
    "        f\"Epoch [{epoch+1}/{num_epochs}], Train Accuracy: {train_accuracy:.2f}%, Test Accuracy: {test_accuracy:.2f}%, Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}\"\n",
    "    )\n",
    "\n",
    "print(\"Training and testing completed!\")\n",
    "\n",
    "# Visualizing the results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_accuracies, \"k\", label=\"Training Accuracy\")\n",
    "plt.plot(test_accuracies, \"r\", label=\"Testing Accuracy\")\n",
    "plt.title(\"Training and Testing Accuracy over Epochs\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy (%)\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_losses, \"k\", label=\"Training Loss\")\n",
    "plt.plot(test_losses, \"r\", label=\"Testing Loss\")\n",
    "plt.title(\"Training and Test Loss over Epochs\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of kernel sizes to experiment with\n",
    "kernelSizes = [2, 3, 4, 5]\n",
    "\n",
    "# Initialize lists to store the overall results for each kernel size\n",
    "all_train_accuracies = {}\n",
    "all_test_accuracies = {}\n",
    "all_train_losses = {}\n",
    "all_test_losses = {}\n",
    "\n",
    "for kernel in kernelSizes:\n",
    "    print(f\"\\nTraining with kernel size: {kernel}\\n\")\n",
    "    # Model, loss function, optimizer initialization for current kernel size\n",
    "    model = AdjustedFashionCNN(kernel_size=kernel).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.00001)\n",
    "\n",
    "    # Lists to store metrics for the current kernel size\n",
    "    train_accuracies, test_accuracies, train_losses, test_losses = [], [], [], []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        train_accuracy, train_loss = train(model, train_loader, criterion, optimizer)\n",
    "        test_accuracy, test_loss = test(model, test_loader)\n",
    "\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        test_accuracies.append(test_accuracy)\n",
    "        train_losses.append(train_loss)\n",
    "        test_losses.append(test_loss)\n",
    "\n",
    "        print(\n",
    "            f\"Epoch [{epoch+1}/{num_epochs}], Train Accuracy: {train_accuracy:.2f}%, Test Accuracy: {test_accuracy:.2f}%, Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}\"\n",
    "        )\n",
    "\n",
    "    # Store the results for the current kernel size\n",
    "    all_train_accuracies[kernel] = train_accuracies\n",
    "    all_test_accuracies[kernel] = test_accuracies\n",
    "    all_train_losses[kernel] = train_losses\n",
    "    all_test_losses[kernel] = test_losses\n",
    "\n",
    "    print(\"Finished Training for kernel size:\", kernel)\n",
    "\n",
    "# Visualizing the results for all kernel sizes\n",
    "plt.figure(figsize=(10, 6))\n",
    "for kernel, acc in all_train_accuracies.items():\n",
    "    plt.plot(acc, label=f\"Training Accuracy (Kernel {kernel})\")\n",
    "for kernel, acc in all_test_accuracies.items():\n",
    "    plt.plot(acc, \"--\", label=f\"Testing Accuracy (Kernel {kernel})\")\n",
    "\n",
    "plt.title(\"Training and Testing Accuracy over Epochs\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy (%)\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "for kernel, loss in all_train_losses.items():\n",
    "    plt.plot(loss, label=f\"Training Loss (Kernel {kernel})\")\n",
    "for kernel, loss in all_test_losses.items():\n",
    "    plt.plot(loss, \"--\", label=f\"Testing Loss (Kernel {kernel})\")\n",
    "\n",
    "plt.title(\"Training and Test Loss over Epochs\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x700 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Function to predict and plot\n",
    "def predict_and_plot(model, data_loader, num_images=5):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    images_so_far = 0\n",
    "    fig = plt.figure(figsize=(15, 7))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (images, labels) in enumerate(data_loader):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            for j in range(images.size()[0]):\n",
    "                images_so_far += 1\n",
    "                ax = plt.subplot(num_images // 5, 5, images_so_far)\n",
    "                ax.axis(\"off\")\n",
    "                ax.set_title(f\"Predicted: {preds[j]}, True: {labels[j]}\")\n",
    "                img = images.cpu().data[j].numpy().transpose((1, 2, 0))\n",
    "                plt.imshow(img.squeeze(), cmap=\"gray\")\n",
    "\n",
    "                if images_so_far == num_images:\n",
    "                    return\n",
    "\n",
    "\n",
    "# Predict and plot using the test data\n",
    "predict_and_plot(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters to tune\n",
    "learning_rates = [0.1, 0.01, 0.001]\n",
    "batch_sizes = [32, 64, 128]\n",
    "optimizers = [torch.optim.SGD, torch.optim.Adam]\n",
    "dropout_rates = [0.2, 0.5]\n",
    "kernel_sizes = [2, 3, 4, 5]\n",
    "\n",
    "best_accuracy = 0.0\n",
    "best_params = {}\n",
    "\n",
    "# Grid search\n",
    "for lr in learning_rates:\n",
    "    for bs in batch_sizes:\n",
    "        for opt in optimizers:\n",
    "            for dr in dropout_rates:\n",
    "                for ks in kernel_sizes:\n",
    "                    train_loader = torch.utils.data.DataLoader(train_set, batch_size=bs)\n",
    "                    test_loader = torch.utils.data.DataLoader(test_set, batch_size=bs)\n",
    "\n",
    "                    model = AdjustedFashionCNN(kernel_size=ks).to(device)\n",
    "                    criterion = nn.CrossEntropyLoss()\n",
    "                    optimizer = opt(model.parameters(), lr=lr)\n",
    "\n",
    "                    # Train and test the model with the current hyperparameters\n",
    "                    for epoch in range(num_epochs):\n",
    "                        train_accuracy, _ = train(\n",
    "                            model, train_loader, criterion, optimizer\n",
    "                        )\n",
    "                        test_accuracy, _ = test(model, test_loader)\n",
    "\n",
    "                    # Update best accuracy and parameters\n",
    "                    if test_accuracy > best_accuracy:\n",
    "                        best_accuracy = test_accuracy\n",
    "                        best_params = {\n",
    "                            \"lr\": lr,\n",
    "                            \"batch_size\": bs,\n",
    "                            \"optimizer\": opt,\n",
    "                            \"dropout_rate\": dr,\n",
    "                            \"kernel_size\": ks,\n",
    "                        }\n",
    "\n",
    "print(\"Best Test Accuracy:\", best_accuracy)\n",
    "print(\"Best Hyperparameters:\", best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdjustedFashionCNN(\n",
      "  (layer1): Sequential(\n",
      "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (fc1): Linear(in_features=1600, out_features=600, bias=True)\n",
      "  (drop): Dropout2d(p=0.25, inplace=False)\n",
      "  (fc2): Linear(in_features=600, out_features=120, bias=True)\n",
      "  (fc3): Linear(in_features=120, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class AdjustedFashionCNN(nn.Module):\n",
    "    def __init__(\n",
    "        self, kernel_size=3, num_filters1=32, num_filters2=64, dropout_rate=0.25\n",
    "    ):\n",
    "        super(AdjustedFashionCNN, self).__init__()\n",
    "\n",
    "        self.kernel_size = kernel_size\n",
    "        input_size = 28  # Assuming the input images are 28x28\n",
    "\n",
    "        # Compute output sizes after conv and pool operations\n",
    "        conv1_out_size = input_size - self.kernel_size + 1\n",
    "        pool1_out_size = (\n",
    "            conv1_out_size // 2\n",
    "        )  # Using kernel_size=2 and stride=2 for pooling\n",
    "\n",
    "        conv2_input_size = pool1_out_size\n",
    "        conv2_out_size = conv2_input_size - self.kernel_size + 1\n",
    "        pool2_out_size = conv2_out_size // 2\n",
    "\n",
    "        fc_input_size = num_filters2 * pool2_out_size * pool2_out_size\n",
    "\n",
    "        # Layer 2 to 5\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=1,\n",
    "                out_channels=num_filters1,\n",
    "                kernel_size=self.kernel_size,\n",
    "            ),  # Layer 2: Convolutional layer\n",
    "            nn.BatchNorm2d(num_filters1),  # Layer 3: Batch normalization\n",
    "            nn.ReLU(),  # Layer 4: ReLU activation\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # Layer 5: Max Pooling\n",
    "        )\n",
    "\n",
    "        # Layer 6 to 9\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=num_filters1,\n",
    "                out_channels=num_filters2,\n",
    "                kernel_size=self.kernel_size,\n",
    "            ),  # Layer 6: Convolutional layer\n",
    "            nn.BatchNorm2d(num_filters2),  # Layer 7: Batch normalization\n",
    "            nn.ReLU(),  # Layer 8: ReLU activation\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # Layer 9: Max Pooling\n",
    "        )\n",
    "\n",
    "        # Layer 11\n",
    "        self.fc1 = nn.Linear(in_features=fc_input_size, out_features=600)\n",
    "\n",
    "        # Layer 10: Dropout\n",
    "        self.drop = nn.Dropout2d(dropout_rate)\n",
    "\n",
    "        # Layer 12\n",
    "        self.fc2 = nn.Linear(in_features=600, out_features=120)\n",
    "\n",
    "        # Layer 13: Classification layer\n",
    "        self.fc3 = nn.Linear(in_features=120, out_features=10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc1(out)\n",
    "        out = self.drop(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.fc3(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "# For testing purposes:\n",
    "# Create an instance of the adjusted model with a kernel size of 3 and filter numbers of 32 and 64 respectively.\n",
    "# Print its architecture.\n",
    "adjusted_fashion_model = AdjustedFashionCNN(\n",
    "    kernel_size=3, num_filters1=32, num_filters2=64\n",
    ")\n",
    "print(adjusted_fashion_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration 1/1296:\n",
      "LR: 0.1, BS: 32, Optimizer: SGD, Dropout: 0.2, Kernel: 2, Filters1: 16, Filters2: 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/functional.py:1331: UserWarning: dropout2d: Received a 2-D input to dropout2d, which is deprecated and will result in an error in a future release. To retain the behavior and silence this warning, please use dropout instead. Note that dropout2d exists to provide channel-wise dropout on inputs with 2 spatial dimensions, a channel dimension, and an optional batch dimension (i.e. 3D or 4D inputs).\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy for this configuration: 89.88%\n",
      "------------------------------------------------------\n",
      "Configuration 2/1296:\n",
      "LR: 0.1, BS: 32, Optimizer: SGD, Dropout: 0.2, Kernel: 2, Filters1: 16, Filters2: 64\n",
      "Test Accuracy for this configuration: 10.00%\n",
      "------------------------------------------------------\n",
      "Configuration 3/1296:\n",
      "LR: 0.1, BS: 32, Optimizer: SGD, Dropout: 0.2, Kernel: 2, Filters1: 16, Filters2: 128\n",
      "Test Accuracy for this configuration: 10.00%\n",
      "------------------------------------------------------\n",
      "Configuration 4/1296:\n",
      "LR: 0.1, BS: 32, Optimizer: SGD, Dropout: 0.2, Kernel: 2, Filters1: 32, Filters2: 32\n",
      "Test Accuracy for this configuration: 89.94%\n",
      "------------------------------------------------------\n",
      "Configuration 5/1296:\n",
      "LR: 0.1, BS: 32, Optimizer: SGD, Dropout: 0.2, Kernel: 2, Filters1: 32, Filters2: 64\n",
      "Test Accuracy for this configuration: 10.00%\n",
      "------------------------------------------------------\n",
      "Configuration 6/1296:\n",
      "LR: 0.1, BS: 32, Optimizer: SGD, Dropout: 0.2, Kernel: 2, Filters1: 32, Filters2: 128\n",
      "Test Accuracy for this configuration: 10.00%\n",
      "------------------------------------------------------\n",
      "Configuration 7/1296:\n",
      "LR: 0.1, BS: 32, Optimizer: SGD, Dropout: 0.2, Kernel: 2, Filters1: 64, Filters2: 32\n",
      "Test Accuracy for this configuration: 89.52%\n",
      "------------------------------------------------------\n",
      "Configuration 8/1296:\n",
      "LR: 0.1, BS: 32, Optimizer: SGD, Dropout: 0.2, Kernel: 2, Filters1: 64, Filters2: 64\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/ahmadar/Downloads/ML/ML-Project.ipynb Cell 16\u001b[0m line \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ahmadar/Downloads/ML/ML-Project.ipynb#Y102sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m \u001b[39m# Train and test the model with the current hyperparameters\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ahmadar/Downloads/ML/ML-Project.ipynb#Y102sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_epochs):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/ahmadar/Downloads/ML/ML-Project.ipynb#Y102sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m     train_accuracy, _ \u001b[39m=\u001b[39m train(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ahmadar/Downloads/ML/ML-Project.ipynb#Y102sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m         model, train_loader, criterion, optimizer\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ahmadar/Downloads/ML/ML-Project.ipynb#Y102sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m     )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ahmadar/Downloads/ML/ML-Project.ipynb#Y102sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m     test_accuracy, _ \u001b[39m=\u001b[39m test(model, test_loader)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ahmadar/Downloads/ML/ML-Project.ipynb#Y102sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m \u001b[39m# Update best accuracy and parameters\u001b[39;00m\n",
      "\u001b[1;32m/Users/ahmadar/Downloads/ML/ML-Project.ipynb Cell 16\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ahmadar/Downloads/ML/ML-Project.ipynb#Y102sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m images, labels \u001b[39m=\u001b[39m images\u001b[39m.\u001b[39mto(device), labels\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ahmadar/Downloads/ML/ML-Project.ipynb#Y102sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/ahmadar/Downloads/ML/ML-Project.ipynb#Y102sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m outputs \u001b[39m=\u001b[39m model(images)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ahmadar/Downloads/ML/ML-Project.ipynb#Y102sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ahmadar/Downloads/ML/ML-Project.ipynb#Y102sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/Users/ahmadar/Downloads/ML/ML-Project.ipynb Cell 16\u001b[0m line \u001b[0;36m6\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ahmadar/Downloads/ML/ML-Project.ipynb#Y102sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/ahmadar/Downloads/ML/ML-Project.ipynb#Y102sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlayer1(x)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ahmadar/Downloads/ML/ML-Project.ipynb#Y102sZmlsZQ%3D%3D?line=62'>63</a>\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer2(out)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ahmadar/Downloads/ML/ML-Project.ipynb#Y102sZmlsZQ%3D%3D?line=63'>64</a>\u001b[0m     out \u001b[39m=\u001b[39m out\u001b[39m.\u001b[39mview(out\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m), \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 463\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    456\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    457\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    458\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 459\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    460\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Define hyperparameters to tune\n",
    "learning_rates = [0.1, 0.01, 0.001]\n",
    "batch_sizes = [32, 64, 128]\n",
    "optimizers = [torch.optim.SGD, torch.optim.Adam]\n",
    "dropout_rates = [0.2, 0.5]\n",
    "kernel_sizes = [2, 3, 4, 5]\n",
    "num_filters1 = [16, 32, 64]\n",
    "num_filters2 = [32, 64, 128]\n",
    "\n",
    "best_accuracy = 0.0\n",
    "best_params = {}\n",
    "\n",
    "# Counter to keep track of configurations\n",
    "config_count = 1\n",
    "total_configs = (\n",
    "    len(learning_rates)\n",
    "    * len(batch_sizes)\n",
    "    * len(optimizers)\n",
    "    * len(dropout_rates)\n",
    "    * len(kernel_sizes)\n",
    "    * len(num_filters1)\n",
    "    * len(num_filters2)\n",
    ")\n",
    "\n",
    "# Grid search\n",
    "for lr in learning_rates:\n",
    "    for bs in batch_sizes:\n",
    "        for opt in optimizers:\n",
    "            for dr in dropout_rates:\n",
    "                for ks in kernel_sizes:\n",
    "                    for nf1 in num_filters1:\n",
    "                        for nf2 in num_filters2:\n",
    "                            print(f\"Configuration {config_count}/{total_configs}:\")\n",
    "                            print(\n",
    "                                f\"LR: {lr}, BS: {bs}, Optimizer: {opt.__name__}, Dropout: {dr}, Kernel: {ks}, Filters1: {nf1}, Filters2: {nf2}\"\n",
    "                            )\n",
    "\n",
    "                            train_loader = torch.utils.data.DataLoader(\n",
    "                                train_set, batch_size=bs\n",
    "                            )\n",
    "                            test_loader = torch.utils.data.DataLoader(\n",
    "                                test_set, batch_size=bs\n",
    "                            )\n",
    "\n",
    "                            model = AdjustedFashionCNN(\n",
    "                                kernel_size=ks,\n",
    "                                num_filters1=nf1,\n",
    "                                num_filters2=nf2,\n",
    "                                dropout_rate=dr,\n",
    "                            ).to(device)\n",
    "                            criterion = nn.CrossEntropyLoss()\n",
    "                            optimizer = opt(model.parameters(), lr=lr)\n",
    "\n",
    "                            # Train and test the model with the current hyperparameters\n",
    "                            for epoch in range(num_epochs):\n",
    "                                train_accuracy, _ = train(\n",
    "                                    model, train_loader, criterion, optimizer\n",
    "                                )\n",
    "                                test_accuracy, _ = test(model, test_loader)\n",
    "\n",
    "                            # Update best accuracy and parameters\n",
    "                            if test_accuracy > best_accuracy:\n",
    "                                best_accuracy = test_accuracy\n",
    "                                best_params = {\n",
    "                                    \"lr\": lr,\n",
    "                                    \"batch_size\": bs,\n",
    "                                    \"optimizer\": opt,\n",
    "                                    \"dropout_rate\": dr,\n",
    "                                    \"kernel_size\": ks,\n",
    "                                    \"num_filters1\": nf1,\n",
    "                                    \"num_filters2\": nf2,\n",
    "                                }\n",
    "                            print(\n",
    "                                f\"Test Accuracy for this configuration: {test_accuracy:.2f}%\"\n",
    "                            )\n",
    "                            print(\n",
    "                                \"------------------------------------------------------\"\n",
    "                            )\n",
    "                            config_count += 1\n",
    "\n",
    "print(\"Best Test Accuracy:\", best_accuracy)\n",
    "print(\"Best Hyperparameters:\", best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration 2/144:\n",
      "LR: 0.1, BS: 32, Optimizer: SGD, Dropout: 0.2, Kernel: 2\n",
      "Train accuracy is less than 80%. Skipping this configuration.\n",
      "------------------------------------------------------\n",
      "Configuration 3/144:\n",
      "LR: 0.1, BS: 32, Optimizer: SGD, Dropout: 0.2, Kernel: 3\n",
      "Epoch 1/5:\n",
      "Train Accuracy: 83.11% - Train Loss: 0.4698\n",
      "Test Accuracy: 83.76% - Test Loss: 0.4330\n",
      "----------------------------------------\n",
      "Epoch 2/5:\n",
      "Train Accuracy: 88.39% - Train Loss: 0.3214\n",
      "Test Accuracy: 86.92% - Test Loss: 0.3571\n",
      "----------------------------------------\n",
      "Epoch 3/5:\n",
      "Train Accuracy: 89.98% - Train Loss: 0.2770\n",
      "Test Accuracy: 87.87% - Test Loss: 0.3241\n",
      "----------------------------------------\n",
      "Epoch 4/5:\n",
      "Train Accuracy: 90.91% - Train Loss: 0.2510\n",
      "Test Accuracy: 89.06% - Test Loss: 0.3042\n",
      "----------------------------------------\n",
      "Epoch 5/5:\n",
      "Train Accuracy: 91.54% - Train Loss: 0.2323\n",
      "Test Accuracy: 88.86% - Test Loss: 0.3067\n",
      "----------------------------------------\n",
      "------------------------------------------------------\n",
      "Configuration 4/144:\n",
      "LR: 0.1, BS: 32, Optimizer: SGD, Dropout: 0.2, Kernel: 4\n",
      "Epoch 1/5:\n",
      "Train Accuracy: 82.91% - Train Loss: 0.4728\n",
      "Test Accuracy: 84.32% - Test Loss: 0.4292\n",
      "----------------------------------------\n",
      "Epoch 2/5:\n",
      "Train Accuracy: 87.94% - Train Loss: 0.3342\n",
      "Test Accuracy: 86.77% - Test Loss: 0.3602\n",
      "----------------------------------------\n",
      "Epoch 3/5:\n",
      "Train Accuracy: 89.50% - Train Loss: 0.2898\n",
      "Test Accuracy: 88.31% - Test Loss: 0.3222\n",
      "----------------------------------------\n",
      "Epoch 4/5:\n",
      "Train Accuracy: 90.44% - Train Loss: 0.2615\n",
      "Test Accuracy: 88.59% - Test Loss: 0.3139\n",
      "----------------------------------------\n",
      "Epoch 5/5:\n",
      "Train Accuracy: 91.35% - Train Loss: 0.2409\n",
      "Test Accuracy: 88.44% - Test Loss: 0.3153\n",
      "----------------------------------------\n",
      "------------------------------------------------------\n",
      "Configuration 5/144:\n",
      "LR: 0.1, BS: 32, Optimizer: SGD, Dropout: 0.2, Kernel: 5\n",
      "Epoch 1/5:\n",
      "Train Accuracy: 83.40% - Train Loss: 0.4599\n",
      "Test Accuracy: 85.37% - Test Loss: 0.4003\n",
      "----------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/ahmadar/Downloads/ML/ML-Project.ipynb Cell 17\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ahmadar/Downloads/ML/ML-Project.ipynb#Y103sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m \u001b[39m# Train and test the model with the current hyperparameters for the specified number of epochs\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ahmadar/Downloads/ML/ML-Project.ipynb#Y103sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_epochs):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/ahmadar/Downloads/ML/ML-Project.ipynb#Y103sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m     train_accuracy, train_loss \u001b[39m=\u001b[39m train(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ahmadar/Downloads/ML/ML-Project.ipynb#Y103sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m         model, train_loader, criterion, optimizer\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ahmadar/Downloads/ML/ML-Project.ipynb#Y103sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m     )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ahmadar/Downloads/ML/ML-Project.ipynb#Y103sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m     total_train_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m train_loss\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ahmadar/Downloads/ML/ML-Project.ipynb#Y103sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m     \u001b[39m# Skip further training and testing for this configuration if training accuracy is less than 80%\u001b[39;00m\n",
      "\u001b[1;32m/Users/ahmadar/Downloads/ML/ML-Project.ipynb Cell 17\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ahmadar/Downloads/ML/ML-Project.ipynb#Y103sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m images, labels \u001b[39m=\u001b[39m images\u001b[39m.\u001b[39mto(device), labels\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ahmadar/Downloads/ML/ML-Project.ipynb#Y103sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/ahmadar/Downloads/ML/ML-Project.ipynb#Y103sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m outputs \u001b[39m=\u001b[39m model(images)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ahmadar/Downloads/ML/ML-Project.ipynb#Y103sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ahmadar/Downloads/ML/ML-Project.ipynb#Y103sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/Users/ahmadar/Downloads/ML/ML-Project.ipynb Cell 17\u001b[0m line \u001b[0;36m6\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ahmadar/Downloads/ML/ML-Project.ipynb#Y103sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ahmadar/Downloads/ML/ML-Project.ipynb#Y103sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer1(x)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/ahmadar/Downloads/ML/ML-Project.ipynb#Y103sZmlsZQ%3D%3D?line=62'>63</a>\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlayer2(out)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ahmadar/Downloads/ML/ML-Project.ipynb#Y103sZmlsZQ%3D%3D?line=63'>64</a>\u001b[0m     out \u001b[39m=\u001b[39m out\u001b[39m.\u001b[39mview(out\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m), \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ahmadar/Downloads/ML/ML-Project.ipynb#Y103sZmlsZQ%3D%3D?line=64'>65</a>\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc1(out)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 463\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    456\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    457\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    458\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 459\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    460\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ... [rest of the code remains unchanged]\n",
    "\n",
    "# Grid search\n",
    "for lr in learning_rates:\n",
    "    for bs in batch_sizes:\n",
    "        for opt in optimizers:\n",
    "            for dr in dropout_rates:\n",
    "                for ks in kernel_sizes:\n",
    "                    config_key = f\"LR: {lr}, BS: {bs}, Optimizer: {opt.__name__}, Dropout: {dr}, Kernel: {ks}\"\n",
    "                    print(f\"Configuration {config_count}/{total_configs}:\")\n",
    "                    print(config_key)\n",
    "\n",
    "                    train_loader = torch.utils.data.DataLoader(train_set, batch_size=bs)\n",
    "                    test_loader = torch.utils.data.DataLoader(test_set, batch_size=bs)\n",
    "\n",
    "                    model = AdjustedFashionCNN(kernel_size=ks, dropout_rate=dr).to(\n",
    "                        device\n",
    "                    )\n",
    "                    criterion = nn.CrossEntropyLoss()\n",
    "                    optimizer = opt(model.parameters(), lr=lr)\n",
    "\n",
    "                    # Variables to store cumulative loss across epochs\n",
    "                    total_train_loss = 0.0\n",
    "                    total_test_loss = 0.0\n",
    "\n",
    "                    # Train and test the model with the current hyperparameters for the specified number of epochs\n",
    "                    for epoch in range(num_epochs):\n",
    "                        train_accuracy, train_loss = train(\n",
    "                            model, train_loader, criterion, optimizer\n",
    "                        )\n",
    "                        total_train_loss += train_loss\n",
    "\n",
    "                        # Skip further training and testing for this configuration if training accuracy is less than 80%\n",
    "                        if train_accuracy < 80.0:\n",
    "                            print(\n",
    "                                \"Train accuracy is less than 80%. Skipping this configuration.\"\n",
    "                            )\n",
    "                            break\n",
    "\n",
    "                        test_accuracy, test_loss = test(model, test_loader)\n",
    "                        total_test_loss += test_loss\n",
    "\n",
    "                        # Printing per-epoch results\n",
    "                        print(f\"Epoch {epoch+1}/{num_epochs}:\")\n",
    "                        print(\n",
    "                            f\"Train Accuracy: {train_accuracy:.2f}% - Train Loss: {train_loss:.4f}\"\n",
    "                        )\n",
    "                        print(\n",
    "                            f\"Test Accuracy: {test_accuracy:.2f}% - Test Loss: {test_loss:.4f}\"\n",
    "                        )\n",
    "                        print(\"----------------------------------------\")\n",
    "\n",
    "                    avg_train_loss = total_train_loss / num_epochs\n",
    "                    avg_test_loss = total_test_loss / num_epochs\n",
    "\n",
    "                    # Store the results\n",
    "                    results[config_key] = {\n",
    "                        \"Train Accuracy\": train_accuracy,\n",
    "                        \"Test Accuracy\": test_accuracy,\n",
    "                        \"Average Train Loss\": avg_train_loss,\n",
    "                        \"Average Test Loss\": avg_test_loss,\n",
    "                    }\n",
    "\n",
    "                    # Update best accuracy and parameters\n",
    "                    if test_accuracy > best_accuracy:\n",
    "                        best_accuracy = test_accuracy\n",
    "                        best_params = {\n",
    "                            \"lr\": lr,\n",
    "                            \"batch_size\": bs,\n",
    "                            \"optimizer\": opt,\n",
    "                            \"dropout_rate\": dr,\n",
    "                            \"kernel_size\": ks,\n",
    "                        }\n",
    "                    print(\"------------------------------------------------------\")\n",
    "                    config_count += 1\n",
    "\n",
    "print(\"Best Test Accuracy:\", best_accuracy)\n",
    "print(\"Best Hyperparameters:\", best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Convert the results dictionary to a DataFrame\n",
    "df_results = pd.DataFrame.from_dict(results, orient=\"index\")\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df_results.to_csv(\"./data/results.csv\")\n",
    "\n",
    "print(\"Results saved to 'results.csv'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
