{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torchsummary import summary\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import time\n",
    "import pathlib\n",
    "import os\n",
    "import scipy.io\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "import glob\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 1, 227, 227])\n",
      "tensor([2, 1, 2, 0, 1, 0, 2, 0, 0, 2])\n"
     ]
    }
   ],
   "source": [
    "MAX_LENGTH = 100  # or another appropriate value based on your data\n",
    "\n",
    "\n",
    "class BrainTumorDataset(Dataset):\n",
    "    def __init__(self, mat_files, transform=None):\n",
    "        self.mat_files = mat_files\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.mat_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        with h5py.File(self.mat_files[idx], \"r\") as f:\n",
    "            image = f[\"cjdata\"][\"image\"][()]\n",
    "\n",
    "            label = int(\n",
    "                f[\"cjdata\"][\"label\"][0][0] - 1\n",
    "            )  # Convert labels from 1-3 to 0-2\n",
    "\n",
    "            # Convert to grayscale ('L' mode) before applying transformations\n",
    "            image = Image.fromarray(image).convert(\"L\")\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "\n",
    "# Sample use:\n",
    "# Assuming path to .mat files is './data/New folder/'\n",
    "mat_files = [\n",
    "    os.path.join(\"./data/New folder/\", f)\n",
    "    for f in os.listdir(\"./data/New folder/\")\n",
    "    if f.endswith(\".mat\")\n",
    "]\n",
    "\n",
    "# Split the data into train and test sets (e.g., 80% train, 20% test)\n",
    "indices = list(range(len(mat_files)))\n",
    "split = int(np.floor(0.7 * len(mat_files)))\n",
    "np.random.shuffle(indices)\n",
    "train_indices, test_indices = indices[:split], indices[split:]\n",
    "\n",
    "\n",
    "# Transforms\n",
    "transforming_img = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((227, 227)),\n",
    "        transforms.ToTensor(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "train_set = Subset(\n",
    "    BrainTumorDataset(mat_files, transform=transforming_img), train_indices\n",
    ")\n",
    "test_set = Subset(\n",
    "    BrainTumorDataset(mat_files, transform=transforming_img), test_indices\n",
    ")\n",
    "train_loader = DataLoader(train_set, batch_size=10, shuffle=True)\n",
    "test_loader = DataLoader(test_set, batch_size=10, shuffle=False)\n",
    "\n",
    "# Sample training loop:\n",
    "for image, label in train_loader:\n",
    "    print(image.shape)\n",
    "    print(label)\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 0.562102735042572\n",
      "Standard Deviation: 0.42450031638145447\n"
     ]
    }
   ],
   "source": [
    "# Initializing variables to store sum and squared sum\n",
    "total_sum = 0\n",
    "total_squared_sum = 0\n",
    "num_pixels = 0\n",
    "\n",
    "# Calculating mean and standard deviation\n",
    "for data, _ in train_set:\n",
    "    total_sum += data.sum()\n",
    "    total_squared_sum += (data**2).sum()\n",
    "    num_pixels += data.numel()\n",
    "\n",
    "mean = total_sum / num_pixels\n",
    "std_dev = (total_squared_sum / num_pixels - mean**2) ** 0.5\n",
    "\n",
    "print(f\"Mean: {mean.item()}\")\n",
    "print(f\"Standard Deviation: {std_dev.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 1, 227, 227])\n",
      "tensor([0, 0, 1, 1, 0, 1, 1, 1, 1, 2])\n"
     ]
    }
   ],
   "source": [
    "# Transforms\n",
    "transforming_img = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((227, 227)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((mean.item()), (std_dev.item())),\n",
    "    ]\n",
    ")\n",
    "\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "train_set = Subset(\n",
    "    BrainTumorDataset(mat_files, transform=transforming_img), train_indices\n",
    ")\n",
    "test_set = Subset(\n",
    "    BrainTumorDataset(mat_files, transform=transforming_img), test_indices\n",
    ")\n",
    "train_loader = DataLoader(train_set, batch_size=10, shuffle=True)\n",
    "test_loader = DataLoader(test_set, batch_size=10, shuffle=False)\n",
    "\n",
    "# Sample training loop:\n",
    "for image, label in train_loader:\n",
    "    print(image.shape)\n",
    "    print(label)\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class AdjustedFashionCNN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size=28,\n",
    "        kernel_size=5,\n",
    "        num_filters1=32,\n",
    "        num_filters2=64,\n",
    "        dropout_rate=0.5,\n",
    "        in_channels=1,\n",
    "        classes=10,\n",
    "    ):\n",
    "        super(AdjustedFashionCNN, self).__init__()\n",
    "\n",
    "        self.kernel_size = kernel_size\n",
    "        self.input_size = input_size\n",
    "        self.pool_kernel_size = 3\n",
    "        self.pool_stride = 3\n",
    "\n",
    "        # Compute output sizes after conv and pool operations\n",
    "        conv1_out_size = self.input_size - self.kernel_size + 1\n",
    "        pool1_out_size = (\n",
    "            conv1_out_size - self.pool_kernel_size\n",
    "        ) // self.pool_stride + 1\n",
    "\n",
    "        conv2_input_size = pool1_out_size\n",
    "        conv2_out_size = conv2_input_size - self.kernel_size + 1\n",
    "        pool2_out_size = (\n",
    "            conv2_out_size - self.pool_kernel_size\n",
    "        ) // self.pool_stride + 1\n",
    "\n",
    "        fc_input_size = num_filters2 * pool2_out_size * pool2_out_size\n",
    "        print(fc_input_size)\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=in_channels,\n",
    "                out_channels=num_filters1,\n",
    "                kernel_size=self.kernel_size,\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(num_filters1),\n",
    "            nn.MaxPool2d(kernel_size=self.pool_kernel_size, stride=self.pool_stride),\n",
    "        )\n",
    "\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=num_filters1,\n",
    "                out_channels=num_filters2,\n",
    "                kernel_size=self.kernel_size,\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(num_filters2),\n",
    "            nn.MaxPool2d(kernel_size=self.pool_kernel_size, stride=self.pool_stride),\n",
    "        )\n",
    "        self.drop = nn.Dropout2d(dropout_rate)\n",
    "        self.fc1 = nn.Linear(fc_input_size, classes)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.drop(out)\n",
    "        out = self.fc1(out)\n",
    "        out = self.softmax(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Adjusted train function\n",
    "def train(model, data_loader, criterion, optimizer):\n",
    "    model.train()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for images, labels in data_loader:  # Unpacking tumor_border and tumor_mask\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    average_loss = total_loss / len(data_loader)\n",
    "    return accuracy, average_loss\n",
    "\n",
    "\n",
    "# Adjusted test function\n",
    "def test(model, data_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in data_loader:  # Unpacking tumor_border and tumor_mask\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    average_loss = total_loss / len(data_loader)\n",
    "    return accuracy, average_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/functional.py:1331: UserWarning: dropout2d: Received a 2-D input to dropout2d, which is deprecated and will result in an error in a future release. To retain the behavior and silence this warning, please use dropout instead. Note that dropout2d exists to provide channel-wise dropout on inputs with 2 spatial dimensions, a channel dimension, and an optional batch dimension (i.e. 3D or 4D inputs).\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30], Train Accuracy: 63.18%, Test Accuracy: 70.58%, Train Loss: 0.9094, Test Loss: 0.8416\n",
      "Epoch [2/30], Train Accuracy: 71.39%, Test Accuracy: 72.46%, Train Loss: 0.8346, Test Loss: 0.8229\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[E thread_pool.cpp:109] Exception in thread pool task: mutex lock failed: Invalid argument\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/ahmadar/Downloads/ML/ML-Project.ipynb Cell 8\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ahmadar/Downloads/ML/ML-Project.ipynb#X32sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m train_accuracies, test_accuracies, train_losses, test_losses \u001b[39m=\u001b[39m [], [], [], []\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ahmadar/Downloads/ML/ML-Project.ipynb#X32sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_epochs):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/ahmadar/Downloads/ML/ML-Project.ipynb#X32sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     train_accuracy, train_loss \u001b[39m=\u001b[39m train(model, train_loader, criterion, optimizer)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ahmadar/Downloads/ML/ML-Project.ipynb#X32sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     test_accuracy, test_loss \u001b[39m=\u001b[39m test(model, test_loader)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ahmadar/Downloads/ML/ML-Project.ipynb#X32sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     train_accuracies\u001b[39m.\u001b[39mappend(train_accuracy)\n",
      "\u001b[1;32m/Users/ahmadar/Downloads/ML/ML-Project.ipynb Cell 8\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ahmadar/Downloads/ML/ML-Project.ipynb#X32sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m images, labels \u001b[39m=\u001b[39m images\u001b[39m.\u001b[39mto(device), labels\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ahmadar/Downloads/ML/ML-Project.ipynb#X32sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/ahmadar/Downloads/ML/ML-Project.ipynb#X32sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m outputs \u001b[39m=\u001b[39m model(images)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ahmadar/Downloads/ML/ML-Project.ipynb#X32sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ahmadar/Downloads/ML/ML-Project.ipynb#X32sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/Users/ahmadar/Downloads/ML/ML-Project.ipynb Cell 8\u001b[0m line \u001b[0;36m6\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ahmadar/Downloads/ML/ML-Project.ipynb#X32sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ahmadar/Downloads/ML/ML-Project.ipynb#X32sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer1(x)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/ahmadar/Downloads/ML/ML-Project.ipynb#X32sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlayer2(out)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ahmadar/Downloads/ML/ML-Project.ipynb#X32sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m     out \u001b[39m=\u001b[39m out\u001b[39m.\u001b[39mview(out\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m), \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ahmadar/Downloads/ML/ML-Project.ipynb#X32sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdrop(out)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py:171\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    164\u001b[0m     bn_training \u001b[39m=\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrunning_mean \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m) \u001b[39mand\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrunning_var \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m    166\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[39mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[1;32m    168\u001b[0m \u001b[39mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \u001b[39mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 171\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mbatch_norm(\n\u001b[1;32m    172\u001b[0m     \u001b[39minput\u001b[39;49m,\n\u001b[1;32m    173\u001b[0m     \u001b[39m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrunning_mean\n\u001b[1;32m    175\u001b[0m     \u001b[39mif\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining \u001b[39mor\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrack_running_stats\n\u001b[1;32m    176\u001b[0m     \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    177\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrunning_var \u001b[39mif\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining \u001b[39mor\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrack_running_stats \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    178\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight,\n\u001b[1;32m    179\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias,\n\u001b[1;32m    180\u001b[0m     bn_training,\n\u001b[1;32m    181\u001b[0m     exponential_average_factor,\n\u001b[1;32m    182\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49meps,\n\u001b[1;32m    183\u001b[0m )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/functional.py:2450\u001b[0m, in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2447\u001b[0m \u001b[39mif\u001b[39;00m training:\n\u001b[1;32m   2448\u001b[0m     _verify_batch_size(\u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize())\n\u001b[0;32m-> 2450\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mbatch_norm(\n\u001b[1;32m   2451\u001b[0m     \u001b[39minput\u001b[39;49m, weight, bias, running_mean, running_var, training, momentum, eps, torch\u001b[39m.\u001b[39;49mbackends\u001b[39m.\u001b[39;49mcudnn\u001b[39m.\u001b[39;49menabled\n\u001b[1;32m   2452\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# MAT SETUP\n",
    "model = AdjustedFashionCNN(input_size=227, classes=3, in_channels=1).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0003)\n",
    "\n",
    "# Training the model and capturing metrics\n",
    "num_epochs = 30\n",
    "train_accuracies, test_accuracies, train_losses, test_losses = [], [], [], []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_accuracy, train_loss = train(model, train_loader, criterion, optimizer)\n",
    "    test_accuracy, test_loss = test(model, test_loader)\n",
    "\n",
    "    train_accuracies.append(train_accuracy)\n",
    "    test_accuracies.append(test_accuracy)\n",
    "    train_losses.append(train_loss)\n",
    "    test_losses.append(test_loss)\n",
    "\n",
    "    print(\n",
    "        f\"Epoch [{epoch+1}/{num_epochs}], Train Accuracy: {train_accuracy:.2f}%, Test Accuracy: {test_accuracy:.2f}%, Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}\"\n",
    "    )\n",
    "\n",
    "print(\"Training and testing completed!\")\n",
    "\n",
    "# Visualizing the results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_accuracies, \"k\", label=\"Training Accuracy\")\n",
    "plt.plot(test_accuracies, \"r\", label=\"Testing Accuracy\")\n",
    "plt.title(\"Training and Testing Accuracy over Epochs\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy (%)\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_losses, \"k\", label=\"Training Loss\")\n",
    "plt.plot(test_losses, \"r\", label=\"Testing Loss\")\n",
    "plt.title(\"Training and Test Loss over Epochs\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_set = torchvision.datasets.FashionMNIST(\n",
    "    \"./data\", download=True, transform=transforms.Compose([transforms.ToTensor()])\n",
    ")\n",
    "\n",
    "test_set = torchvision.datasets.FashionMNIST(\n",
    "    \"./data\",\n",
    "    download=True,\n",
    "    train=False,\n",
    "    transform=transforms.Compose([transforms.ToTensor()]),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 0.2860410809516907\n",
      "Standard Deviation: 0.3530237078666687\n"
     ]
    }
   ],
   "source": [
    "# Initializing variables to store sum and squared sum\n",
    "total_sum = 0\n",
    "total_squared_sum = 0\n",
    "num_pixels = 0\n",
    "\n",
    "# Calculating mean and standard deviation\n",
    "for data, _ in train_set:\n",
    "    total_sum += data.sum()\n",
    "    total_squared_sum += (data**2).sum()\n",
    "    num_pixels += data.numel()\n",
    "\n",
    "mean = total_sum / num_pixels\n",
    "std_dev = (total_squared_sum / num_pixels - mean**2) ** 0.5\n",
    "\n",
    "print(f\"Mean: {mean.item()}\")\n",
    "print(f\"Standard Deviation: {std_dev.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Assuming these are the calculated mean and standard deviation values\n",
    "mean_value = mean.item()  # replace with the calculated mean value\n",
    "std_value = std_dev.item()  # replace with the calculated std deviation value\n",
    "\n",
    "# Updated transformations with normalization\n",
    "transforming_img = transforms.Compose(\n",
    "    [transforms.ToTensor(), transforms.Normalize((mean_value,), (std_value,))]\n",
    ")\n",
    "\n",
    "# Loading the FashionMNIST datasets with the updated transformations\n",
    "train_set = torchvision.datasets.FashionMNIST(\n",
    "    \"./data\", download=True, transform=transforming_img\n",
    ")\n",
    "test_set = torchvision.datasets.FashionMNIST(\n",
    "    \"./data\", download=True, train=False, transform=transforming_img\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=10, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def output_label(label):\n",
    "    output_mapping = {\n",
    "        0: \"T-shirt/Top\",\n",
    "        1: \"Trouser\",\n",
    "        2: \"Pullover\",\n",
    "        3: \"Dress\",\n",
    "        4: \"Coat\",\n",
    "        5: \"Sandal\",\n",
    "        6: \"Shirt\",\n",
    "        7: \"Sneaker\",\n",
    "        8: \"Bag\",\n",
    "        9: \"Ankle Boot\",\n",
    "    }\n",
    "    input = label.item() if type(label) == torch.Tensor else label\n",
    "    return output_mapping[input]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 1, 28, 28])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = next(iter(train_loader))\n",
    "a[0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image, label = next(iter(train_set))\n",
    "plt.imshow(image.squeeze(), cmap=\"gray\")\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "torch.Size([10, 1, 28, 28]) torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "demo_loader = torch.utils.data.DataLoader(train_set, batch_size=10)\n",
    "\n",
    "batch = next(iter(demo_loader))\n",
    "images, labels = batch\n",
    "print(type(images), type(labels))\n",
    "print(images.shape, labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "torch.Size([10, 1, 28, 28]) torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "demo_loader = torch.utils.data.DataLoader(train_set, batch_size=10)\n",
    "\n",
    "batch = next(iter(demo_loader))\n",
    "images, labels = batch\n",
    "print(type(images), type(labels))\n",
    "print(images.shape, labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AdjustedFashionCNN(\n",
       "  (layer1): Sequential(\n",
       "    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): MaxPool2d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): MaxPool2d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (drop): Dropout2d(p=0.5, inplace=False)\n",
       "  (fc1): Linear(in_features=64, out_features=10, bias=True)\n",
       "  (softmax): Softmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create an instance of the adjusted model with a kernel size of 3 (for example) and print its architecture\n",
    "adjusted_fashion_model = AdjustedFashionCNN(kernel_size=5)\n",
    "adjusted_fashion_model\n",
    "\n",
    "# Check if GPU is available and move the model to GPU if possible\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "adjusted_fashion_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n",
      "Epoch [1/30], Train Accuracy: 77.17%, Test Accuracy: 82.72%, Train Loss: 1.7083, Test Loss: 1.6382\n",
      "Epoch [2/30], Train Accuracy: 81.85%, Test Accuracy: 84.60%, Train Loss: 1.6492, Test Loss: 1.6183\n",
      "Epoch [3/30], Train Accuracy: 83.59%, Test Accuracy: 85.03%, Train Loss: 1.6297, Test Loss: 1.6125\n",
      "Epoch [4/30], Train Accuracy: 84.68%, Test Accuracy: 86.25%, Train Loss: 1.6182, Test Loss: 1.6006\n",
      "Epoch [5/30], Train Accuracy: 85.31%, Test Accuracy: 86.65%, Train Loss: 1.6110, Test Loss: 1.5950\n",
      "Epoch [6/30], Train Accuracy: 85.83%, Test Accuracy: 86.84%, Train Loss: 1.6055, Test Loss: 1.5942\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/ahmadar/Downloads/ML/ML-Project.ipynb Cell 20\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ahmadar/Downloads/ML/ML-Project.ipynb#X22sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m train_accuracies, test_accuracies, train_losses, test_losses \u001b[39m=\u001b[39m [], [], [], []\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ahmadar/Downloads/ML/ML-Project.ipynb#X22sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_epochs):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/ahmadar/Downloads/ML/ML-Project.ipynb#X22sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     train_accuracy, train_loss \u001b[39m=\u001b[39m train(model, train_loader, criterion, optimizer)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ahmadar/Downloads/ML/ML-Project.ipynb#X22sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     test_accuracy, test_loss \u001b[39m=\u001b[39m test(model, test_loader)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ahmadar/Downloads/ML/ML-Project.ipynb#X22sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     train_accuracies\u001b[39m.\u001b[39mappend(train_accuracy)\n",
      "\u001b[1;32m/Users/ahmadar/Downloads/ML/ML-Project.ipynb Cell 20\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ahmadar/Downloads/ML/ML-Project.ipynb#X22sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m total \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ahmadar/Downloads/ML/ML-Project.ipynb#X22sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m total_loss \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/ahmadar/Downloads/ML/ML-Project.ipynb#X22sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mfor\u001b[39;00m images, labels \u001b[39min\u001b[39;00m data_loader:  \u001b[39m# Unpacking tumor_border and tumor_mask\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ahmadar/Downloads/ML/ML-Project.ipynb#X22sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     images, labels \u001b[39m=\u001b[39m images\u001b[39m.\u001b[39mto(device), labels\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ahmadar/Downloads/ML/ML-Project.ipynb#X22sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/utils/data/dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    634\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    635\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    636\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/utils/data/dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    676\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 677\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    678\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    679\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torchvision/datasets/mnist.py:145\u001b[0m, in \u001b[0;36mMNIST.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    142\u001b[0m img \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39mfromarray(img\u001b[39m.\u001b[39mnumpy(), mode\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mL\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    144\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 145\u001b[0m     img \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransform(img)\n\u001b[1;32m    147\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_transform \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    148\u001b[0m     target \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_transform(target)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torchvision/transforms/transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, img):\n\u001b[1;32m     94\u001b[0m     \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransforms:\n\u001b[0;32m---> 95\u001b[0m         img \u001b[39m=\u001b[39m t(img)\n\u001b[1;32m     96\u001b[0m     \u001b[39mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torchvision/transforms/transforms.py:137\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, pic):\n\u001b[1;32m    130\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m    132\u001b[0m \u001b[39m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[39m        Tensor: Converted image.\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 137\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mto_tensor(pic)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torchvision/transforms/functional.py:166\u001b[0m, in \u001b[0;36mto_tensor\u001b[0;34m(pic)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[39m# handle PIL Image\u001b[39;00m\n\u001b[1;32m    165\u001b[0m mode_to_nptype \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mI\u001b[39m\u001b[39m\"\u001b[39m: np\u001b[39m.\u001b[39mint32, \u001b[39m\"\u001b[39m\u001b[39mI;16\u001b[39m\u001b[39m\"\u001b[39m: np\u001b[39m.\u001b[39mint16, \u001b[39m\"\u001b[39m\u001b[39mF\u001b[39m\u001b[39m\"\u001b[39m: np\u001b[39m.\u001b[39mfloat32}\n\u001b[0;32m--> 166\u001b[0m img \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mfrom_numpy(np\u001b[39m.\u001b[39;49marray(pic, mode_to_nptype\u001b[39m.\u001b[39;49mget(pic\u001b[39m.\u001b[39;49mmode, np\u001b[39m.\u001b[39;49muint8), copy\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m))\n\u001b[1;32m    168\u001b[0m \u001b[39mif\u001b[39;00m pic\u001b[39m.\u001b[39mmode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m1\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    169\u001b[0m     img \u001b[39m=\u001b[39m \u001b[39m255\u001b[39m \u001b[39m*\u001b[39m img\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Model, loss function, optimizer initialization\n",
    "model = AdjustedFashionCNN().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0003)\n",
    "\n",
    "# Training the model and capturing metrics\n",
    "num_epochs = 30\n",
    "train_accuracies, test_accuracies, train_losses, test_losses = [], [], [], []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_accuracy, train_loss = train(model, train_loader, criterion, optimizer)\n",
    "    test_accuracy, test_loss = test(model, test_loader)\n",
    "\n",
    "    train_accuracies.append(train_accuracy)\n",
    "    test_accuracies.append(test_accuracy)\n",
    "    train_losses.append(train_loss)\n",
    "    test_losses.append(test_loss)\n",
    "\n",
    "    print(\n",
    "        f\"Epoch [{epoch+1}/{num_epochs}], Train Accuracy: {train_accuracy:.2f}%, Test Accuracy: {test_accuracy:.2f}%, Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}\"\n",
    "    )\n",
    "\n",
    "print(\"Training and testing completed!\")\n",
    "\n",
    "# Visualizing the results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_accuracies, \"k\", label=\"Training Accuracy\")\n",
    "plt.plot(test_accuracies, \"r\", label=\"Testing Accuracy\")\n",
    "plt.title(\"Training and Testing Accuracy over Epochs\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy (%)\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_losses, \"k\", label=\"Training Loss\")\n",
    "plt.plot(test_losses, \"r\", label=\"Testing Loss\")\n",
    "plt.title(\"Training and Test Loss over Epochs\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x700 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Function to predict and plot\n",
    "def predict_and_plot(model, data_loader, num_images=5):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    images_so_far = 0\n",
    "    fig = plt.figure(figsize=(15, 7))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (images, labels) in enumerate(data_loader):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            for j in range(images.size()[0]):\n",
    "                images_so_far += 1\n",
    "                ax = plt.subplot(num_images // 5, 5, images_so_far)\n",
    "                ax.axis(\"off\")\n",
    "                ax.set_title(f\"Predicted: {preds[j]}, True: {labels[j]}\")\n",
    "                img = images.cpu().data[j].numpy().transpose((1, 2, 0))\n",
    "                plt.imshow(img.squeeze(), cmap=\"gray\")\n",
    "\n",
    "                if images_so_far == num_images:\n",
    "                    return\n",
    "\n",
    "\n",
    "# Predict and plot using the test data\n",
    "predict_and_plot(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration 1/162:\n",
      "LR: 0.0001, BS: 10, Optimizer: SGD, Dropout: 0.3, Kernel: 3\n",
      "256\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/ahmadar/Downloads/ML/ML-Project.ipynb Cell 22\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ahmadar/Downloads/ML/ML-Project.ipynb#X24sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m \u001b[39m# Train and test the model with the current hyperparameters for the specified number of epochs\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ahmadar/Downloads/ML/ML-Project.ipynb#X24sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_epochs):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/ahmadar/Downloads/ML/ML-Project.ipynb#X24sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m     train_accuracy, train_loss \u001b[39m=\u001b[39m train(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ahmadar/Downloads/ML/ML-Project.ipynb#X24sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m         model, train_loader, criterion, optimizer\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ahmadar/Downloads/ML/ML-Project.ipynb#X24sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m     )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ahmadar/Downloads/ML/ML-Project.ipynb#X24sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m     total_train_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m train_loss\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ahmadar/Downloads/ML/ML-Project.ipynb#X24sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m     \u001b[39m# Skip further training and testing for this configuration if training accuracy is less than 80%\u001b[39;00m\n",
      "\u001b[1;32m/Users/ahmadar/Downloads/ML/ML-Project.ipynb Cell 22\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ahmadar/Downloads/ML/ML-Project.ipynb#X24sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m images, labels \u001b[39m=\u001b[39m images\u001b[39m.\u001b[39mto(device), labels\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ahmadar/Downloads/ML/ML-Project.ipynb#X24sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/ahmadar/Downloads/ML/ML-Project.ipynb#X24sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m outputs \u001b[39m=\u001b[39m model(images)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ahmadar/Downloads/ML/ML-Project.ipynb#X24sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ahmadar/Downloads/ML/ML-Project.ipynb#X24sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/Users/ahmadar/Downloads/ML/ML-Project.ipynb Cell 22\u001b[0m line \u001b[0;36m6\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ahmadar/Downloads/ML/ML-Project.ipynb#X24sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ahmadar/Downloads/ML/ML-Project.ipynb#X24sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer1(x)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/ahmadar/Downloads/ML/ML-Project.ipynb#X24sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlayer2(out)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ahmadar/Downloads/ML/ML-Project.ipynb#X24sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m     out \u001b[39m=\u001b[39m out\u001b[39m.\u001b[39mview(out\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m), \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ahmadar/Downloads/ML/ML-Project.ipynb#X24sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdrop(out)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py:171\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    164\u001b[0m     bn_training \u001b[39m=\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrunning_mean \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m) \u001b[39mand\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrunning_var \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m    166\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[39mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[1;32m    168\u001b[0m \u001b[39mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \u001b[39mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 171\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mbatch_norm(\n\u001b[1;32m    172\u001b[0m     \u001b[39minput\u001b[39;49m,\n\u001b[1;32m    173\u001b[0m     \u001b[39m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrunning_mean\n\u001b[1;32m    175\u001b[0m     \u001b[39mif\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining \u001b[39mor\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrack_running_stats\n\u001b[1;32m    176\u001b[0m     \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    177\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrunning_var \u001b[39mif\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining \u001b[39mor\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrack_running_stats \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    178\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight,\n\u001b[1;32m    179\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias,\n\u001b[1;32m    180\u001b[0m     bn_training,\n\u001b[1;32m    181\u001b[0m     exponential_average_factor,\n\u001b[1;32m    182\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49meps,\n\u001b[1;32m    183\u001b[0m )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/functional.py:2450\u001b[0m, in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2447\u001b[0m \u001b[39mif\u001b[39;00m training:\n\u001b[1;32m   2448\u001b[0m     _verify_batch_size(\u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize())\n\u001b[0;32m-> 2450\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mbatch_norm(\n\u001b[1;32m   2451\u001b[0m     \u001b[39minput\u001b[39;49m, weight, bias, running_mean, running_var, training, momentum, eps, torch\u001b[39m.\u001b[39;49mbackends\u001b[39m.\u001b[39;49mcudnn\u001b[39m.\u001b[39;49menabled\n\u001b[1;32m   2452\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Define hyperparameters to tune\n",
    "learning_rates = [0.0001, 0.00001, 0.000001]\n",
    "batch_sizes = [10, 32, 64]\n",
    "optimizers = [torch.optim.SGD, torch.optim.Adam]\n",
    "dropout_rates = [0.3, 0.5, 0.6]\n",
    "kernel_sizes = [3, 4, 5]\n",
    "num_epochs = 10\n",
    "\n",
    "best_accuracy = 0.0\n",
    "best_params = {}\n",
    "\n",
    "results = {}  # Dictionary to store results\n",
    "\n",
    "# Counter to keep track of configurations\n",
    "config_count = 1\n",
    "total_configs = (\n",
    "    len(learning_rates)\n",
    "    * len(batch_sizes)\n",
    "    * len(optimizers)\n",
    "    * len(dropout_rates)\n",
    "    * len(kernel_sizes)\n",
    ")\n",
    "# Grid search\n",
    "for lr in learning_rates:\n",
    "    for bs in batch_sizes:\n",
    "        for opt in optimizers:\n",
    "            for dr in dropout_rates:\n",
    "                for ks in kernel_sizes:\n",
    "                    config_key = f\"LR: {lr}, BS: {bs}, Optimizer: {opt.__name__}, Dropout: {dr}, Kernel: {ks}\"\n",
    "                    print(f\"Configuration {config_count}/{total_configs}:\")\n",
    "                    print(config_key)\n",
    "\n",
    "                    train_loader = torch.utils.data.DataLoader(train_set, batch_size=bs)\n",
    "                    test_loader = torch.utils.data.DataLoader(test_set, batch_size=bs)\n",
    "\n",
    "                    model = AdjustedFashionCNN(kernel_size=ks, dropout_rate=dr).to(\n",
    "                        device\n",
    "                    )\n",
    "                    criterion = nn.CrossEntropyLoss()\n",
    "                    optimizer = opt(model.parameters(), lr=lr)\n",
    "\n",
    "                    # Variables to store cumulative loss across epochs\n",
    "                    total_train_loss = 0.0\n",
    "                    total_test_loss = 0.0\n",
    "                    start_time = time.time()\n",
    "\n",
    "                    # Train and test the model with the current hyperparameters for the specified number of epochs\n",
    "                    for epoch in range(num_epochs):\n",
    "                        train_accuracy, train_loss = train(\n",
    "                            model, train_loader, criterion, optimizer\n",
    "                        )\n",
    "                        total_train_loss += train_loss\n",
    "\n",
    "                        # Skip further training and testing for this configuration if training accuracy is less than 80%\n",
    "                        if train_accuracy < 80.0:\n",
    "                            print(\n",
    "                                \"Train accuracy is less than 80%. Skipping this configuration.\"\n",
    "                            )\n",
    "                            break\n",
    "\n",
    "                        test_accuracy, test_loss = test(model, test_loader)\n",
    "                        total_test_loss += test_loss\n",
    "\n",
    "                        # Printing per-epoch results\n",
    "                        print(f\"Epoch {epoch+1}/{num_epochs}:\")\n",
    "                        print(\n",
    "                            f\"Train Accuracy: {train_accuracy:.2f}% - Train Loss: {train_loss:.4f}\"\n",
    "                        )\n",
    "                        print(\n",
    "                            f\"Test Accuracy: {test_accuracy:.2f}% - Test Loss: {test_loss:.4f}\"\n",
    "                        )\n",
    "                        print(\"----------------------------------------\")\n",
    "\n",
    "                    avg_train_loss = total_train_loss / num_epochs\n",
    "                    avg_test_loss = total_test_loss / num_epochs\n",
    "                    end_time = time.time()\n",
    "                    config_time = end_time - start_time\n",
    "\n",
    "                    # Store the results ONLY if train_accuracy >= 80%\n",
    "                    if train_accuracy >= 80.0:\n",
    "                        results[config_key] = {\n",
    "                            \"Train Accuracy\": train_accuracy,\n",
    "                            \"Test Accuracy\": test_accuracy,\n",
    "                            \"Average Train Loss\": avg_train_loss,\n",
    "                            \"Average Test Loss\": avg_test_loss,\n",
    "                            \"Time (in sec)\": config_time,\n",
    "                        }\n",
    "\n",
    "                    # Update best accuracy and parameters\n",
    "                    if test_accuracy > best_accuracy:\n",
    "                        best_accuracy = test_accuracy\n",
    "                        best_params = {\n",
    "                            \"lr\": lr,\n",
    "                            \"batch_size\": bs,\n",
    "                            \"optimizer\": opt,\n",
    "                            \"dropout_rate\": dr,\n",
    "                            \"kernel_size\": ks,\n",
    "                        }\n",
    "                    print(\"------------------------------------------------------\")\n",
    "                    config_count += 1\n",
    "\n",
    "print(\"Best Test Accuracy:\", best_accuracy)\n",
    "print(\"Best Hyperparameters:\", best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to 'results.csv'\n"
     ]
    }
   ],
   "source": [
    "# Convert the results dictionary to a DataFrame\n",
    "df_results = pd.DataFrame.from_dict(results, orient=\"index\")\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df_results.to_csv(\"./data/results.csv\")\n",
    "\n",
    "print(\"Results saved to 'results.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# run their parameters in the fashion and get the results -- 97.2%\n",
    "# run the different parameters in the fashion data and get the best combinations .... Arend claimed low Kernel size will be better\n",
    "# take the best hyperparameter and run it on the MRI data set and then compare\n",
    "# check if hyper-tuning on small dataset can be faster/better/works and if our claim would be correct and generalize it to larger dataset\n",
    "# then check if we get better results than the MRI paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters to tune\n",
    "learning_rates = [0.1, 0.01, 0.001, 0.0001, 0.00001]\n",
    "batch_sizes = [32, 64, 128]\n",
    "optimizers = [torch.optim.SGD, torch.optim.Adam]\n",
    "dropout_rates = [0.2, 0.5]\n",
    "kernel_sizes = [2, 3, 4, 5, 6]\n",
    "num_epochs = 30\n",
    "\n",
    "best_accuracy = 0.0\n",
    "best_params = {}\n",
    "\n",
    "results = {}  # Dictionary to store results\n",
    "\n",
    "# Counter to keep track of configurations\n",
    "config_count = 1\n",
    "total_configs = (\n",
    "    len(learning_rates)\n",
    "    * len(batch_sizes)\n",
    "    * len(optimizers)\n",
    "    * len(dropout_rates)\n",
    "    * len(kernel_sizes)\n",
    ")\n",
    "# Grid search\n",
    "for lr in learning_rates:\n",
    "    for bs in batch_sizes:\n",
    "        for opt in optimizers:\n",
    "            for dr in dropout_rates:\n",
    "                for ks in kernel_sizes:\n",
    "                    config_key = f\"LR: {lr}, BS: {bs}, Optimizer: {opt.__name__}, Dropout: {dr}, Kernel: {ks}\"\n",
    "                    print(f\"Configuration {config_count}/{total_configs}:\")\n",
    "                    print(config_key)\n",
    "\n",
    "                    train_loader = torch.utils.data.DataLoader(train_set, batch_size=bs)\n",
    "                    test_loader = torch.utils.data.DataLoader(test_set, batch_size=bs)\n",
    "\n",
    "                    model = AdjustedFashionCNN(kernel_size=ks, dropout_rate=dr).to(\n",
    "                        device\n",
    "                    )\n",
    "                    criterion = nn.CrossEntropyLoss()\n",
    "                    optimizer = opt(model.parameters(), lr=lr)\n",
    "\n",
    "                    # Variables to store cumulative loss across epochs\n",
    "                    total_train_loss = 0.0\n",
    "                    total_test_loss = 0.0\n",
    "\n",
    "                    # Train and test the model with the current hyperparameters for the specified number of epochs\n",
    "                    for epoch in range(num_epochs):\n",
    "                        train_accuracy, train_loss = train(\n",
    "                            model, train_loader, criterion, optimizer\n",
    "                        )\n",
    "                        total_train_loss += train_loss\n",
    "\n",
    "                        # Skip further training and testing for this configuration if training accuracy is less than 80%\n",
    "                        if train_accuracy < 80.0:\n",
    "                            print(\n",
    "                                \"Train accuracy is less than 80%. Skipping this configuration.\"\n",
    "                            )\n",
    "                            break\n",
    "\n",
    "                        test_accuracy, test_loss = test(model, test_loader)\n",
    "                        total_test_loss += test_loss\n",
    "\n",
    "                        # Printing per-epoch results\n",
    "                        print(f\"Epoch {epoch+1}/{num_epochs}:\")\n",
    "                        print(\n",
    "                            f\"Train Accuracy: {train_accuracy:.2f}% - Train Loss: {train_loss:.4f}\"\n",
    "                        )\n",
    "                        print(\n",
    "                            f\"Test Accuracy: {test_accuracy:.2f}% - Test Loss: {test_loss:.4f}\"\n",
    "                        )\n",
    "                        print(\"----------------------------------------\")\n",
    "\n",
    "                    avg_train_loss = total_train_loss / num_epochs\n",
    "                    avg_test_loss = total_test_loss / num_epochs\n",
    "\n",
    "                    # Store the results ONLY if train_accuracy >= 80%\n",
    "                    if train_accuracy >= 80.0:\n",
    "                        results[config_key] = {\n",
    "                            \"Train Accuracy\": train_accuracy,\n",
    "                            \"Test Accuracy\": test_accuracy,\n",
    "                            \"Average Train Loss\": avg_train_loss,\n",
    "                            \"Average Test Loss\": avg_test_loss,\n",
    "                        }\n",
    "\n",
    "                    # Update best accuracy and parameters\n",
    "                    if test_accuracy > best_accuracy:\n",
    "                        best_accuracy = test_accuracy\n",
    "                        best_params = {\n",
    "                            \"lr\": lr,\n",
    "                            \"batch_size\": bs,\n",
    "                            \"optimizer\": opt,\n",
    "                            \"dropout_rate\": dr,\n",
    "                            \"kernel_size\": ks,\n",
    "                        }\n",
    "                    print(\"------------------------------------------------------\")\n",
    "                    config_count += 1\n",
    "\n",
    "print(\"Best Test Accuracy:\", best_accuracy)\n",
    "print(\"Best Hyperparameters:\", best_params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
